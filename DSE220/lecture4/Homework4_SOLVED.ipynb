{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 - Raul G. Martinez (PID: A12461871)\n",
    "## DSE 220: Machine Learning \n",
    "## Due Date: May 28, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Instructions\n",
    "A report for this Homework should be submitted on Gradescope by May 28. The code will only be evaluated, so make sure that you include your code. To secure full marks both the report and the code should be in sync and logically correct. Please only submit relevant and legible code. Even if your method does not run well, you will gain marks for comprehensiveness of your analysis as and where applicable. Please complete this homework individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Overview\n",
    "The large number of English words can make language-based applications daunting. To cope with this, it is helpful to have a clustering or embedding of these words, so that words with similar meanings are clustered together, or have embedding that are close to one another.\n",
    "\n",
    "But how can we get at the meanings of words? John Firth (1957) put it thus:\n",
    "You shall know a word by the company it keeps.\n",
    "\n",
    "That is, words that tend to appear in similar contexts are likely to be related. In this assignment, you will investigate this idea by coming up with an embedding of words that is based on co-occurrence statistics.\n",
    "\n",
    "The description here assumes you are using Python with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. First, download the Brown corpus (using nltk.corpus). This is a collection of text samples from a wide range of sources, with a total of over a million words. Calling brown.words() returns this text in one long list, which is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/gio/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# get brown corpus words\n",
    "brown_words = brown.words()\n",
    "\n",
    "# print number of words\n",
    "len(brown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Remove stopwords and punctuation, make everything lowercase, and count how often each word occurs. Use this to come up with two lists:\n",
    "\n",
    "- A vocabulary V , consisting of a few thousand (e.g., 5000) of the most commonly-occurring words.\n",
    "- A shorter list C of at most 1000 of the most commonly-occurring words, which we shall call context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013319"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# make lowercase\n",
    "brown_words = [i.lower() for i in brown_words]\n",
    "\n",
    "# remove punctuations from each word\n",
    "brown_words = [i.translate(str.maketrans('','',string.punctuation)) for i in brown_words]\n",
    "\n",
    "# exclude spaces and punctuations\n",
    "brown_words = [i for i in brown_words if i not in set([''] + list(string.punctuation))] \n",
    "\n",
    "# print number of words\n",
    "len(brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "539921"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# get stop words from nltk and remove them\n",
    "stopwords = set(stopwords.words('english'))\n",
    "brown_words = [i for i in brown_words if i not in stopwords]\n",
    "\n",
    "# print number of words\n",
    "len(brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word frequency\n",
    "word_count = {i:0 for i in set(brown_words)}\n",
    "for i in brown_words:\n",
    "    word_count[i]+=1\n",
    "\n",
    "# convert to tuple and sort by count\n",
    "word_count = sorted([(k,v) for k,v in word_count.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print first ten elements of sorted tuple\n",
    "word_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary V, get 5,000 most commonly-occurring words\n",
    "V = [i[0] for i in word_count[:5000]]\n",
    "\n",
    "# define list C, get 1,000 most commonly-occurring words\n",
    "C = [i[0] for i in word_count[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For each word w ∈ V , and each occurrence of it in the text stream, look at the surrounding window of four words (two before, two after):\n",
    "<br />\n",
    "<div align=\"center\">w1 &nbsp; &nbsp; w2 &nbsp; &nbsp; w &nbsp; &nbsp; w3 &nbsp; &nbsp; w4.</div>\n",
    "\n",
    "Keep count of how often context words from C appear in these positions around word w. That is, for w ∈ V, c ∈ C,  define\n",
    "\n",
    "<br />\n",
    "<div align=\"center\">n(w, c) = # of times c occurs in a window around w.</div>\n",
    "\n",
    "Using these counts, construct the probability distribution Pr(c|w) of context words around w (for each w ∈ V ), as well as the overall distribution Pr(c) of context words. These are distributions over C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.9 s, sys: 286 ms, total: 32.2 s\n",
      "Wall time: 32.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# find n(w,c)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# define empty array with 5,000 x 1,000 elements\n",
    "WordMatrix = np.zeros((5000, 1000))\n",
    "\n",
    "# convert to sets for superior speed\n",
    "set_V = set(V)\n",
    "set_C = set(C)\n",
    "\n",
    "# loop over all words from brown corpus (stream text)\n",
    "for n, stream_word in enumerate(brown_words):\n",
    "    \n",
    "    # check if word is in vocabulary \n",
    "    if stream_word in set_V:\n",
    "        \n",
    "        # find indices for window -2 to +2 (exclude indices from corner cases at the beginning and end)\n",
    "        textStream_indices = [i for i in range(n-2, n+3) if i>=0 and i<len(brown_words) and i!=n]\n",
    "        \n",
    "        # loop over the window of four words (less than four words for some corner cases)\n",
    "        for idx in textStream_indices:\n",
    "            \n",
    "            # use index to get word from stream text\n",
    "            window_word = brown_words[idx]\n",
    "            \n",
    "            # update WordMatrix count if word is present on context words\n",
    "            if window_word in set_C:\n",
    "                WordMatrix[V.index(stream_word), C.index(window_word)]+=1\n",
    "                \n",
    "# display shape of matrix\n",
    "WordMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# find the probability distribution Pr(c|w)\n",
    "Pr_c_w = WordMatrix/np.sum(WordMatrix, axis=1)[:, None]\n",
    "\n",
    "Pr_c_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the overall distribution Pr(c)\n",
    "Pr_c = np.sum(WordMatrix/np.sum(WordMatrix), axis=0)\n",
    "\n",
    "Pr_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Represent each vocabulary item w by a |C|-dimensional vector φ(w), whose cth coordinate is:\n",
    "<br />\n",
    "<div align=\"center\">φ(w) = max(0, log Pr(c|w) / Pr(c))</div>\n",
    "\n",
    "This is known as the (positive) pointwise mutual information, and has been quite successful in work on word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as geek\n",
    "\n",
    "# find element-wise maximum value \n",
    "Phi_w = geek.maximum(np.zeros((5000, 1000)),np.log(Pr_c_w/Pr_c))\n",
    "\n",
    "Phi_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Suppose we want a 100-dimensional representation. How would you achieve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# use PCA to reduce to a 100-dimensional representation\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(Phi_w)\n",
    "\n",
    "# reduced data\n",
    "Phi_w_transformed_PCA = pca.fit_transform(Phi_w)\n",
    "\n",
    "Phi_w_transformed_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA total % variance explained: 22.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a29f9a710>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3xW9fn/8dfF3nsYIOwtU6IoOHFPsOIqKo6Ktn6rVlt/Xd/WVtv6tVardRUXYHFL3QuZKoiAsjchzBBWCGGErOv3x33SpikJN5A7d3Kf9/PxuB+5z8kZ1/HgO+f+3J/zOebuiIhIeFSLdwEiIlKxFPwiIiGj4BcRCRkFv4hIyCj4RURCRsEvIhIyCn6RODCzNDM75xi3sdfMOpdXTRIeCn6pMoKwPBAEXoaZvWRmDYr9/nwzm2lm2Wa23cxmmNllJbZxppm5md0X5T47mVmhmT1d3sdzrNy9gbunxrsOqXoU/FLVXOruDYATgBOBXwOY2UjgTWAC0A5oDfwGuLTE+qOBXcHPaNwAZALXmFntY65epBJQ8EuV5O6bgY+BPmZmwKPAA+7+vLtnuXuhu89w91uL1jGzesBI4A6gm5mlRLGrG4j8ccmjxB+R4JPD7Wa22swyzeypoBbMrIuZTTWznWa2w8wmmlmTkhs3s+PMbL+ZNS82b1DwiaWmmXUNPrlkBdt5vcT+uwbvLzKzZcGnnc1m9tOo/2NK6Cj4pUoys2TgIuA7oAeQDLx1mNWuAPYS+WTwKZFQL2sfpxH59PAa8EYpy19C5JNHf+Aq4Pyi1YE/AW2AXkF995dc2d23AtODdYtcB7zm7nnAA8BnQNOglr+VUu4LwG3u3hDoA0wt69gk3BT8UtW8Y2a7gS+BGcAfgaKr5fTDrDsaeN3dC4BXgGvNrOZhlv/Y3TOD5S80s1YllnnI3Xe7+wZgGjAAwN3XuPtkdz/o7tuJfCI5o5T9jCcS9phZdeBa4OXgd3lAB6CNu+e4+5elbCMP6G1mjdw9092/LeO4JOQU/FLVjHD3Ju7ewd1/5O4HgJ3B75JKWyn4hHAWMDGY9S5QB7i4lOXrAlcWLe/us4ENwPdLLLq12Pv9QINg/VZm9lrQ7LIH+AfQopTy3iUS2p2Bc4Esd/8m+N19RD49fGNmS83s5lK2cQWRT0Drg6ahU0pZTkTBLwlhJbCRSPiV5noi/97fN7OtQCqR4C+tuedyoBHwtJltDdZpW8byJf0JcKCfuzcickVvh1rQ3XOINCWNCup8udjvtrr7re7eBrgtqKfrIbYx192HA62Ad4LtiRySgl+qPI+MLX4P8L9mdpOZNTKzamZ2qpmNDRa7AfgdkaaYotcVwMXFv1gtZjTwItC32PJDgQFm1jeKshoS+T5ht5m1BX52mOUnADcClxH5dACAmV1pZu2CyUwif0wKiq9oZrXMbJSZNQ6+F9hTchmR4hT8khDc/S3gauBmYAuQATwIvGtmJwMdgaeCK+ii13vAGiJt6v8SBPXZwF9LLD8f+ITouoL+jkiX0yzgQ2DSYer/CigEvnX3tGK/OhGYY2Z7gfeAu9x93SE2cT2QFjQr3U7wnYHIoZgexCJSOZjZVOAVd38+3rVIYlPwi1QCZnYiMBlIdvfseNcjiS1mTT1mlmxm08xsedAb4a5g/v1BT4cFweuiWNUgUhWY2Xjgc+Buhb5UhJhd8ZtZEpDk7t+aWUNgPjCCyI0qe939kZjsWEREylQjVht293SCG2rcPdvMlhPpDiciInFUIW38ZtYRmEnkVvJ7iHRb2wPMA+4N7owsuc4YYAxA/fr1B/Xs2TPmdYqIJJL58+fvcPeWJefHPPiDYXNnAH9w90lm1hrYQaQ/8gNEmoNKuxsRgJSUFJ83b15M6xQRSTRmNt/d/2swwpj24w/GQXkbmOjukwDcPcPdC9y9EHgOOCmWNYiIyH+KZa8eIzJi4HJ3f7TY/OLjqVwOLIlVDSIi8t9i9uUukdvbrwcWm9mCYN4viYyIOIBIU08akfFHRESkgsSyV8+XHHpQqo9itU8RETk8jdUjIhIyCn4RkZBR8IuIhIyCX0QkZBT8IiIho+AXEQkZBb+ISMgo+EVEQkbBLyISMgp+EZGQUfCLiISMgl9EJGQU/CIiIaPgFxEJGQW/iEgllZNXEJPtKvhFRCqZjbv286ePlnPyn6awaNPuct9+LJ/AJSIiUXJ3vlqzk3Gz0piyIoNqZpzXuzV1alYv930p+EVE4mjvwXz++e0mxs9ez5pte2lWvxY/OrMLowZ3oE2TujHZp4JfRCQOUrfvZcLs9bw9fxPZB/Pp27Yxj1zZn0v6JcXkKr84Bb+ISAUpLHSmr9rGuFnrmblqOzWrGxf3TeKGIR0ZmNwEs0M9prz8KfhFRGIsa38eb87fyITZ69mwaz+tGtbmJ+d059rBybRqWKfC61Hwi4jEyMqt2YyblcY7323mQF4BKR2a8rPze3BBn+OoWT1+nSoV/CIi5Si/oJDJyzIYPzuNr1N3UbtGNYYPaMMNp3SkT9vG8S4PUPCLiJSLnXsP8trcjUz8ej1bsnJo26QuP7+wJ1enJNO0fq14l/cfFPwiIsdg0abdjJ+1nvcXbSE3v5BTu7bg/suO5+xeralerWK+rD1SCn4RkSOUm1/Ix0vSGTcrje827KZerepcnZLM6CEd6NqqYbzLOywFv4hIlDL25DBxzgZembOBHXsP0qlFfX57aW+uGNSORnVqxru8qCn4RUTK4O7MX5/JuFlpfLJkKwXuDOvRihuGdOS0ri2oVkmbc8qi4BcROYScvALeW7CF8bPTWLplD43q1ODGIR25/pQOdGheP97lHRMFv4hIMZsy9/Py1+t5fe5Gdu/Po0frhvzh8j5cPrAt9WolRmQmxlGIiBwDd2f22sjImJ8vz8DMOLdXa0YP6cjJnZtV2FAKFUXBLyKhte9gPpO+28yEWWmsDkbGvP2MLlx3cuxGxqwMFPwiEjrrduxjwuw03ppX8SNjVgYKfhEJBXfnyzU7eP6LdcwIRsa8qG8Soyt4ZMzKQMEvIgktN7+QDxZtYezMVFZszaZFg9rcfU43vj+4fVxGxqwMYhb8ZpYMTACOAwqBse7+uJk1A14HOgJpwFXunhmrOkQknLIO5PHqNxt46at1ZOw5SLdWDXh4ZD+GD2hD7RqJ35xTllhe8ecD97r7t2bWEJhvZpOBG4Ep7v6Qmf0c+Dnw/2JYh4iEyKbM/bz0VRqvfbOBfbkFDO3anIeu6MeZ3VuGqjmnLDELfndPB9KD99lmthxoCwwHzgwWGw9MR8EvIsdoyeYsxs5M5cPF6QBc0i+JW0/rXGmGQq5MKqSN38w6AgOBOUDr4I8C7p5uZq1KWWcMMAagffv2FVGmiFQxhYXOjFXbGTszldmpO2lQuwY3D+3IjUM70TaBu2Meq5gHv5k1AN4G7nb3PdF+1HL3scBYgJSUFI9dhSJS1eTkFfDugs0898U61mzbS1LjOvzqol5cfVJylRosLV5iGvxmVpNI6E9090nB7AwzSwqu9pOAbbGsQUQSR+a+XCbOWc+4WevZsfcgvZIa8ehV/bm0f5u4Psqwqollrx4DXgCWu/ujxX71HjAaeCj4+W6sahCRxLB+5z5e/HIdb8zbxIG8Ak7v3pLbTu/MkC7N9YXtUYjlFf9Q4HpgsZktCOb9kkjgv2FmtwAbgCtjWIOIVGHfbcjkuS9S+WTJVqpXM4YPaMsPTutEz+Maxbu0Ki2WvXq+BEr7U3x2rPYrIlVbYaHz+fIMnvsilblpmTSqU4Pbz+jC6CEdad0onDdclTfduSsilUJOXgFvzd/EC1+uY92OfbRrWpffXNKbq09Mpn5tRVV50n9NEYmrnXsPMmH2el7+ej279uXSr11jnvz+QC44/jhq6AvbmFDwi0hcrN2+l+e/WMekbzdxML+Qc3q14tbTOnNSp8Qb/76yUfCLSIVxd+amZTJ2ZiqfL8+gVo1qXHFCO245tRNdWzWId3mhoeAXkZjLLyjk06UZjP0ilYUbd9O0Xk3uHNaVG4Z0pEWD2vEuL3QU/CISM/sO5vPmvI288NU6Nu46QMfm9XhgRB9GntCOurXCPUJmPCn4RaTcbduTw/jZafzj6w1kHcgjpUNTfnVRb87t3Zrq1dR+H2+lBr+ZZQOljpHj7rqDQkT+w6qMbJ6bmcq7C7aQV1jI+b2P49bTOzOoQ9N4lybFlBr87t4QwMx+D2wFXiZyQ9YooGGFVCcilZ67M3vtTsZ+kcr0ldupU7Ma15yUzM1DO9GxRf14lyeHEE1Tz/nuPrjY9DNmNgd4OEY1iUgVkFdQyEeL0xk7M5WlW/bQokFt7j23O9ed3IGm9WvFuzwpQzTBX2Bmo4DXiDT9XAsUxLQqEam0snPyeH3uRl78ch1bsnLo2qoBD32vLyMGtqVOTX1hWxVEE/zfBx4PXg58FcwTkRBJzzrAuK/SeGXOBrIP5jO4UzMevLwPZ3ZvRTV9YVulHDb43T2NyOMSRSSElm7J4vkv1vH+wi04cFHfJG49rRP92jWJd2lylA4b/GbWHXiGyCMT+5hZP+Ayd38w5tWJSNzMSd3JU9PXMnPVdurVqs4Np3TkpqEdSW5WL96lyTGKpqnnOeBnwN8B3H2Rmb0CKPhFEox75Bm2T01bw9y0TFo0qMV9F/Rg1EkdaFxPjzRMFNEEfz13/6bEoEn5MapHROKgsND5bFkGT01bw+LNWSQ1rsNvL+3NNSe21x22CSia4N9hZl0IbuYys5FAekyrEpEKkV9QyAeL0nl6+hpWZeylQ/N6PPS9vnzvhHbUqqEhkRNVNMF/BzAW6Glmm4F1wHUxrUpEYupgfgGTvt3MM9PXsmHXfrq3bsDj1wzg4r5JGgM/BKLp1ZMKnGNm9YFq7p4d+7JEJBYO5Bbw2twN/H1GKlv35NC3bWP+fv0gzu3VWl0yQySaXj21gSuAjkCNorZ+d/99TCsTkXKTnZPHy1+v54Uv1rFzXy4ndWzG/43sx+ndWuihJyEUTVPPu0AWMB84GNtyRKQ8Ze7L5aWv1jFuVhp7cvI5o3tL7jirKyd1ahbv0iSOogn+du5+QcwrEZFys21PDs99kcrEORvYn1vA+ce35o6zuuqmKwGiC/5ZZtbX3RfHvBoROSYbd+3n7zPX8sa8TeQXFHJZ/zb86KyudG+tAXXl36IJ/lOBG81sHZGmHgPc3fvFtDIRidra7Xt5Zvpa3vluM2YwclA7bju9i4ZFlkOKJvgvjHkVInJUlm3Zw1PT1/DR4nRq16jGdSd3YMzpnWnTpG68S5NKrKwncDVy9z2Aum+KVDLfbsjkqalrmLJiGw1q1+D2M7pwy6md9OByiUpZV/yvAJcQ6c3jRJp4ijjQOYZ1iUgJ7s7s1J08OXUNs9bupEm9mtxzbndGn9JR4+jIESnr0YuXBD87VVw5IlKSuzNt5TaenLqGbzfspmXD2vzqol58f3B76teOprVW5D9F9a/GzJoC3YA6RfPcfWasihIRKCh0Pl26lSenrmFZ+h7aNqnLA8OP58qUZD3pSo5JNHfu/gC4C2gHLABOBmYDw2Jbmkg45RUU8t6CLTw9fQ1rt++jc4v6/HlkP0YMbEtNjaMj5SCaK/67gBOBr939LDPrCfwutmWJhE9OXgFvzd/EszPWsinzAD2Pa8jfrh3IRX2TqK5xdKQcRRP8Oe6eY2aYWW13X2FmPWJemUhI7M/N55U5Gxg7M5Vt2QcZkNyE+y89nrN7tdI4OhIT0QT/JjNrArwDTDazTGBLbMsSSXxZB/KYMCuNF79aR+b+PE7p3JzHrh7AkC7NFfgSU9EMy3x58PZ+M5sGNAY+iWlVIgls596DvPDlOl6evZ7sg/mc1aMl/zOsG4M6NI13aRISZd3Adajh+4rG62kA7Cprw2b2IpH7ALa5e59g3v3ArcD2YLFfuvtHR1izSJWUnnWAsTNTefWbDRzML+SiPkn86KwuHN+mcbxLk5Ap64r/UDduFYnmBq5xwJPAhBLzH3P3R6ItUKSq27BzP8/MWMtb8zdS6DBiQFt+eGYXurZqEO/SJKTKuoHrmG7ccveZZtbxWLYhUpWt2ZbNU9PW8t7CLVSvZlx9YjK3nd6F5Gb14l2ahFy0N3B9j8gonQ584e7vHMM+/8fMbgDmAfe6e2Yp+xwDjAFo3779MexOpGKlbt/LE1NW8+7CLdStWZ2bh3bk1tM606pRncOvLFIBzN3LXsDsaaAr8Gow62pgrbvfcdiNR674PyjWxt8a2EHkD8gDQJK733y47aSkpPi8efMOt5hIXG3YuZ8npq5m0rebqF2jOjcM6cBtp3ehWf1a8S5NQsrM5rt7Ssn50VzxnwH08eAvhJmN599f8h4Rd88oVtBzwAdHsx2RymTz7gM8OXU1b87bRPVqxk1DO3H7GV1o2VAjZUrlFE3wrwTaA+uD6WRg0dHszMyS3D09mLwcWHI02xGpDLZm5fDUtDW8NncDhnHdyR344ZldaK0mHankogn+5sByM/smmD4R+NrM3gNw98sOtZKZvQqcCbQws03Ab4EzzWwAkaaeNOC2Y6peJA62ZefwzPS1TJyzAXfnqpRk7jirqx5+IlVGNMH/m6PZsLtfe4jZLxzNtkQqgx17D/L3GWt5+ev15BU4V5zQlh8P66ZeOlLlRBP82919WfEZZnamu0+PTUkilUvmvlz+PjOV8bPSOJhfwIiBbblzWDc9z1aqrGiC/w0zmwD8mch4/A8DKcApsSxMJN6y9ufx/JepvPjlOvbnFXBpvzbceXY33XglVV40wT8Y+D9gFtAQmAgMjWVRIvG0JyePl75M4/kvU8nOyeeivsdx9znd6d66YbxLEykX0QR/HnAAqEvkin+duxfGtCqRONh3MJ9xs9IYOzOVrAN5nNe7NT85tzu9khrFuzSRchVN8M8F3iXSm6c58HczG+nuI2NamUgFOZBbwMtfp/HsjFR27ctlWM9W/OSc7vRtp8HTJDFFE/y3uHvRbbNbgeFmdn0MaxKpEDl5BUycs4Fnpq9lx96DnN69JT85pxsD22t4ZElsZQ3LPMzdp7r7PDPr5O7riv16XwXUJhITB/MLeH3uRp6atoaMPQcZ0qU5z1x3Aid2PNRI5CKJp6wr/keAE4L3bxd7D/BrYFKsihKJhdz8Qt6cv5Enp64hPSuHEzs25a9XD+SULs3jXZpIhSor+K2U94eaFqm08gsKmfTtZp6YuppNmQcY2L4Jfx7Zn6Fd9YhDCaeygt9LeX+oaZFKp6DQeXfBZp6Yspq0nfvp164xD4zow5ndWyrwJdTKCv7OwXg8Vuw9wfQxPaRFJJYKC50PFqfz189Xkbp9H72TGvH8DSmc3auVAl+EsoN/eLH3JR+VqEcnSqVTWOh8unQrj32+ilUZe+nRuiHPXncC5/U+jmrVFPgiRcp69OKMiixE5Gi5O1NXbOORz1axPH0PXVrW54lrB3JJ3yQFvsghRPXoRZHKav76XTz08QrmpmXSsXk9Hru6P5f1b0t1Bb5IqRT8UiWtzsjm4U9XMnlZBi0b1ubBEX24+sRkalavFu/SRCq9qIPfzOq7u27ckrhKzzrAXyev5s35G6lXqwY/Pa87N5/aiXq1dA0jEq3D/t9iZkOA54EGQHsz6w/c5u4/inVxIkWy9ufx9Iw1jPsqDXe4aWgn7jirqx5kLnIUorlMegw4Hyh61OJCMzs9plWJBHLyChg3K42np60h+2A+lw9oy0/O7a6nXokcg6g+H7v7xhL9nwtiU45IRNHdto9OXsXWPTkM69mKn53fQ0Mki5SDaIJ/Y9Dc42ZWC7gTWB7bsiSs3J3JyzJ4+NOVrNm2lwHJTfjrNQM4ubPG0xEpL9EE/+3A40BbYBPwGXBHLIuScJqbFumaOX99Jp1b1ufZ6wZx/vGtdbetSDk7bPC7+w5gVAXUIiG1cms2D3+ygikrttG6UW0e+l5fRg5qRw11zRSJiWh69YwH7nL33cF0U+Av7n5zrIuTxLZ59wEem7yKt7/dRIPaNbjvgh7cNKQTdWtVj3dpIgktmqaefkWhD+DumWY2MIY1SYLL3JfL09PXMH72egDGnNaZH57ZhSb11DVTpCJEE/zVzKypu2cCmFmzKNcT+Q8Hcgt48at1PDt9Lfty8/neCe2459zutGlSN96liYRKNAH+F2CWmb0VTF8J/CF2JUmiyS8o5I15m/jr56vYln2Qc3q15r4LetC9dcN4lyYSStF8uTvBzOYDZxEZi/977r4s5pVJlefufLJkK3/+dCWpO/aR0qEpT43Ss21F4i3aJpsVQGbR8mbW3t03xKwqqfJmr93JQ5+sYOHG3XRr1YCx1w/i3N7qmilSGUTTq+fHwG+BDCJ37BqRRy/2i21pUhUt27KHhz9dwfSV20lqXIeHR/bjihPaaZhkkUokmiv+u4Ae7r4z1sVI1bVx134em7yKfy7YTMPaNfj5hT25cUhH6tRU10yRyiaqIRuArFgXIlXTrn25PDl1Df/4ej1mcNvpXfjhGV1oXK9mvEsTkVJEE/ypwHQz+xA4WDTT3R+NWVVS6eXmFzJhdhqPT1nNvoP5XJWSzF3ndCOpsbpmilR20QT/huBVK3hJiLk7ny3L4E8fLSdt537O6N6SX13cS10zRaqQaLpz/q4iCpHKb8nmLB78cBlfp+6ia6sGjLvpRM7s0SreZYnIEYqmV09L4D7geKBO0Xx3HxbDuqQS2bYnhz9/upK3vt1E03q1eGBEH649MVmDqIlUUdE09UwEXgcuITJE82hg++FWMrMXg3W2uXufYF6zYFsdgTTgqqKhIKTyyckr4LmZqTwzYy15BYXcelpn7jirK43r6otbkaosmku25u7+ApDn7jOCUTlPjmK9ccAFJeb9HJji7t2AKcG0VDLuzrsLNjPsken8ZfIqTu/Wks/vOYNfXtRLoS+SAKK54s8Lfqab2cXAFqDd4VZy95lm1rHE7OHAmcH78cB04P9FUYNUkPnrM3ngg2Us2LibPm0b8ejVevqVSKKJJvgfNLPGwL3A34BGwE+Ocn+t3T0dwN3TzazUbwbNbAwwBqB9+/ZHuTuJ1qbM/Tz08Qo+WJROq4a1+XNwx2013XErknCi6dXzQfA2i8hAbRXC3ccCYwFSUlK8ovYbNvtz83l62lrGfpFKNYM7z+7Gbad3pn5tjbwtkqhK/b/bzO5z94fN7G9Exub5D+5+51HsL8PMkoKr/SRg21FsQ8qBu/PR4q08+OEy0rNyGDGgDfdd0FNj44uEQFmXdcuDn/PKcX/vEekV9FDw891y3LZEaXVGNr99bymz1u6kd1Ij/nbtQFI0VLJIaJQa/O7+vplVB/q4+8+OdMNm9iqRL3JbmNkmIiN8PgS8YWa3ELkb+MqjqlqOSnZOHo9/vppxs9KoV6s6vx9+PKMGd9DImSIhU2ZDrrsXmNmgo9mwu19byq/OPprtydFzd/753Wb+9PEKduw9yDUnJvPT83rQvEHteJcmInEQzTd435nZe8CbwL6ime4+KWZVSblZtmUPv3l3CfPWZ9I/uQnP35BC/+Qm8S5LROIomuBvBuwEig/R4ICCvxLbvT+XRyev4h9fr6dJvVo8fEU/Rg5S90wRia47500VUYiUj8JC5415G3n405Xs3p/L9Sd34J5ze2h8fBH5l2gGaasD3MJ/D9J2cwzrkqOwYusefvXPJcxfn8lJHZtx/2XH07tNo3iXJSKVTDRNPS8Tedj6+cDvgVH8u6unVAIHcgt4fMpqnv8ilYZ1avDIlf254oS2erC5iBxSNMHf1d2vNLPh7j7ezF4BPo11YRKdr1N38vO3F5G2cz9XpbTjFxf2oml9PS9HREp3JIO07TazPsBWIsMqSxxl5+Tx0McrmDhnA+2b1eOVHwxmSNcW8S5LRKqAaIJ/rJk1BX5N5M7bBsD/xrQqKdO0Fdv45T8Xs3VPDrec2ol7z+tOvVoaW0dEolPWWD2t3T3D3Z8PZs0EOldMWXIou/bl8vv3l/LOgi10a9WAST8cwsD2TeNdlohUMWVdJi40s8XAq8Db7p5VQTVJCe7Oh4vT+e27S8k6kMedZ3fjjrO6ULtG9XiXJiJVUFnB3xY4B7gG+JOZzSbyR+A9dz9QEcVJ5Hm3v3pnCZOXZdCvXWMm3jqYnsepi6aIHL2yBmkrINJ751MzqwVcSOSPwONmNsXdR1VQjaH13sIt/O87S8jJK+AXF/bkllM76QHnInLMovpG0N1zzWwZkf77g4DeMa0q5Hbty+V/313Ch4vSGZDchL9c1Z8uLRvEuywRSRBlBr+ZtQeuBq4F6gOvAcPdXTdwxchnS7fyy38uJutAHj87vwe3nd5ZV/kiUq7K6tUzi0g7/5vAGHcvzweySAlZB/L43ftLmfTtZnolNeLlWwbTK0lt+SJS/sq64v8FMNPd9bzbGJuTupN73ljI1j05/HhYV348rBu1augqX0Rio6wvd2dUZCFhlJtfyGOfr+LZGWvp0Kweb91+ivrli0jM6XbPOFm7fS93v7aAxZuzuDolmd9c2pv6tXU6RCT2lDQVzN2ZOGcDD364jDo1q/PsdSdwQZ+keJclIiESdfCb2cnAH4HawJ/d/Z2YVZWgMvflct/bi5i8LIPTurXgkSv707pRncOvKCJSjsrq1XOcu28tNuse4DLAgFmAgv8IfLNuF3e99h079h7k1xf34uahnfQYRBGJi7Ku+J81s/lEru5zgN3A94FCYE9FFJcICgqdp6et4bHPV5HcrB6TfjiUvu0ax7ssEQmxsnr1jDCzS4EPzGw8cDeR4K8HjKig+qq0jD053P3aAman7mT4gDY8OKIPDevo2bciEl9ltvG7+/tm9hHwI2AS8Ad3/6JCKqvipq/cxr1vLGR/bgEPj+zHlYPa6VGIIlIplHqXkJldZmZfAlOBJUQGaLvczF41sy4VVWBVU1joPDFlNTeNm0vLhrV5/8dDuSolWaEvIpVGWVf8DwKnAHWBj9z9JOAeM+sG/IHIHwIpZk9OHve8vpDPl2dw+cC2/PHyvtStpTHzRaRyKSv4s4iEe11gW9FMd1+NQv+/rM7I5raX57Nh137uv7Q3o4d01FW+iFRKZS8DhIoAAAueSURBVAX/5URG5cwj8qWulOKjxen89M2F1KtVnYk/GMzgzs3jXZKISKnK6tWzA/hbBdZS5RQWOo98tpKnp69lYPsmPDNqEMc11g1ZIlK5aciGo7Q/N5+7X1vAZ8syuPak9tx/WW89A1dEqgQF/1HI2JPDLePnsmzLHn57aW9uVHu+iFQhCv4jtDojm+tf+IbsnDyeH53CsJ6t412SiMgRUfAfgYUbdzP6pW+oWb0ab94+hN5t9IQsEal6FPxRmrV2B7eOn0ezBrX4xy2D6dC8frxLEhE5KnEJfjNLA7KBAiDf3VPiUUe0Zq/dyY0vzaVDs3r84weDNZSyiFRp8bziPyvoMlqpLd2SxZgJ82jfrB6v33YKzerXindJIiLHRE/0LsOGnfu58aW5NKhTgwk3n6TQF5GEEK/gd+AzM5tvZmMOtYCZjTGzeWY2b/v27RVcHuzal8sNL84hN7+QCTefRJsmdSu8BhGRWIhX8A919xOAC4E7zOz0kgu4+1h3T3H3lJYtW1ZocQWFzp2vfseW3Tm8eGMK3Vo3rND9i4jEUlyC3923BD+3Af8ETopHHaX5y2cr+XLNDn4//HgGdWgW73JERMpVhQe/mdU3s4ZF74HziIz3Xyl8unQrT09fy7UnJXPNSe3jXY6ISLmLR6+e1sA/gyEOagCvuPsncajjv6zbsY9731hI/3aNuf+y4+NdjohITFR48Lt7KtC/ovd7OHkFhdz92ndUr2Y8fd0gDbgmIglLd+4GnpiymoWbsnh61Am0VQ8eEUlg6scPzE3bxVPT1jByUDsu6psU73JERGIq9MGfnZPHT15fQLum9dSuLyKhEPqmnsc/X83m3Qd46/ZTaFA79P85RCQEQn3Fv37nPsbPTuOqQcnqry8ioRHq4H/o4xXUrF6Ne8/rHu9SREQqTGiD/5t1u/h4yVZuP6MLrTTMsoiESCiDv7DQ+cOHyziuUR1uPa1zvMsREalQoQz+Dxans3BTFvdd0IO6tXSjloiES+iC3915etoaurVqwIgBbeNdjohIhQtd8E9fuZ0VW7O57YwuVKtm8S5HRKTChS74n5mxlqTGdbisf5t4lyIiEhehCv5vN2Tyzbpd3HJqJ2rVCNWhi4j8S6jS79npa2lctybXapx9EQmx0AT/mm17mbw8gxtO6UB9Dc0gIiEWmuD/x9frqVm9GqOHdIx3KSIicRWK4C8odD5anM5ZPVrSokHteJcjIhJXoQj+eWm72JZ9kIv7qSePiEgogv/DxenUqVmNs3u2incpIiJxl/DBH2nm2cqwnq30pa6ICCEI/jnrdrJj70Eu7qtmHhERCEHwf7gonbo1q3NWz5bxLkVEpFJI6ODPLyjkkyVbGdarFfVqqZlHRAQSPPjnrNvFzn25XNI3Kd6liIhUGgkd/B8sSqdereqcpd48IiL/ktDtH2NO78wZ3VtQp6YetiIiUiShg79Ti/p0alE/3mWIiFQqCd3UIyIi/03BLyISMgp+EZGQUfCLiISMgl9EJGQU/CIiIaPgFxEJGQW/iEjIxCX4zewCM1tpZmvM7OfxqEFEJKwqPPjNrDrwFHAh0Bu41sx6V3QdIiJhFY8r/pOANe6e6u65wGvA8DjUISISSvEYq6ctsLHY9CZgcMmFzGwMMCaY3GtmK49yfy2AHUe5blUWxuMO4zFDOI87jMcMR37cHQ41Mx7Bb4eY5/81w30sMPaYd2Y2z91TjnU7VU0YjzuMxwzhPO4wHjOU33HHo6lnE5BcbLodsCUOdYiIhFI8gn8u0M3MOplZLeAa4L041CEiEkoV3tTj7vlm9j/Ap0B14EV3XxrDXR5zc1EVFcbjDuMxQziPO4zHDOV03Ob+X83rIiKSwHTnrohIyCj4RURCJqGDPwxDQ5hZsplNM7PlZrbUzO4K5jczs8lmtjr42TTetZY3M6tuZt+Z2QfBdCczmxMc8+tB54GEYmZNzOwtM1sRnPNTEv1cm9lPgn/bS8zsVTOrk4jn2sxeNLNtZrak2LxDnluLeCLItkVmdsKR7Cthgz9EQ0PkA/e6ey/gZOCO4Dh/Dkxx927AlGA60dwFLC82/X/AY8ExZwK3xKWq2Hoc+MTdewL9iRx/wp5rM2sL3AmkuHsfIh1CriExz/U44IIS80o7txcC3YLXGOCZI9lRwgY/IRkawt3T3f3b4H02kSBoS+RYxweLjQdGxKfC2DCzdsDFwPPBtAHDgLeCRRLxmBsBpwMvALh7rrvvJsHPNZHeh3XNrAZQD0gnAc+1u88EdpWYXdq5HQ5M8IivgSZmlhTtvhI5+A81NETbONVSIcysIzAQmAO0dvd0iPxxAFrFr7KY+CtwH1AYTDcHdrt7fjCdiOe7M7AdeClo4nrezOqTwOfa3TcDjwAbiAR+FjCfxD/XRUo7t8eUb4kc/FENDZEozKwB8DZwt7vviXc9sWRmlwDb3H1+8dmHWDTRzncN4ATgGXcfCOwjgZp1DiVo0x4OdALaAPWJNHOUlGjn+nCO6d97Igd/aIaGMLOaREJ/ortPCmZnFH30C35ui1d9MTAUuMzM0og04Q0j8gmgSdAcAIl5vjcBm9x9TjD9FpE/BIl8rs8B1rn7dnfPAyYBQ0j8c12ktHN7TPmWyMEfiqEhgrbtF4Dl7v5osV+9B4wO3o8G3q3o2mLF3X/h7u3cvSOR8zrV3UcB04CRwWIJdcwA7r4V2GhmPYJZZwPLSOBzTaSJ52Qzqxf8Wy865oQ+18WUdm7fA24IevecDGQVNQlFxd0T9gVcBKwC1gK/inc9MTrGU4l8xFsELAheFxFp854CrA5+Not3rTE6/jOBD4L3nYFvgDXAm0DteNcXg+MdAMwLzvc7QNNEP9fA74AVwBLgZaB2Ip5r4FUi32PkEbmiv6W0c0ukqeepINsWE+n1FPW+NGSDiEjIJHJTj4iIHIKCX0QkZBT8IiIho+AXEQkZBb+ISMgo+KXKM7M/mdmZZjbiSEdhNbOWwSiP35nZaSV+Nz0Y3XVB8BpZ2nYOs4+7zaze0awrEgsKfkkEg4mMT3QG8MURrns2sMLdB7r7odYd5e4Dgtdbh/h9NO4mMrhY1IrdlSpS7hT8UmWZ2Z/NbBFwIjAb+AHwjJn95hDLdjCzKcHY5VPMrL2ZDQAeBi4KrujrRrnf68zsm2CdvwdDgGNmz5jZvGDs+N8F8+4kMsbMNDObFszbW2xbI81sXPB+nJk9Giz3f2ZWPxijfW7wiWR4sNzxxfa/yMy6He1/QwmpeN+tppdex/IiMvz234CawFdlLPc+MDp4fzPwTvD+RuDJUtaZDqzk33dENwd6BduqGSzzNHBD8L7orsrqwbr9guk0oEWx7e4t9n4kMC54Pw74AKgeTP8RuC5434TIXej1g+MdFcyvBdSN93nQq2q99HFSqrqBREK5J5ExXEpzCvC94P3LRK70ozHK3ecVTZjZtcAgYG5k6Bjq8u+Bs64yszFERtFMIvIAoEVR7qfIm+5eELw/j8hgdD8NpusA7Yl8uvlV8EyCSe6++gj3ISGn4JcqKWimGUdkVMIdRNrQzcwWAKe4+4HDbOJoxyoxYLy7/6JEPZ2AnwInuntm0HxTJ4p9l1xmX4l9XeHuK0sss9zM5hB5EM2nZvYDd596hMchIaY2fqmS3H2Buw8g0vzRG5gKnO+RL2EPFfqziIzkCTAK+PIodz0FGGlmreBfz0TtADQiEtpZZtaa/xwzPhtoWGw6w8x6mVk14PIy9vUp8ONgVErMbGDwszOQ6u5PEBmlsd9RHouElIJfqiwzawlkunsh0NPdy2rquRO4Kfgy+Hoiz+s9YsE+fg18FmxrMpDk7guB74ClwIvAV8VWGwt8XPTlLpGHp3xA5I9VWUPpPkDku4tFFnkA9wPB/KuBJcGnm57AhKM5Fgkvjc4pIhIyuuIXEQkZBb+ISMgo+EVEQkbBLyISMgp+EZGQUfCLiISMgl9EJGT+P9UYFa2S74fGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# find percentage of variance explained using PCA\n",
    "perc_var = np.round(pca.explained_variance_ratio_, decimals=4)*100\n",
    "\n",
    "# plot cummulative distribution\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(0,25)\n",
    "\n",
    "print('PCA total % variance explained: {:.2f}'.format(np.sum(perc_var)))\n",
    "plt.plot(np.cumsum(perc_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# use SVD to reduce to a 100-dimensional representation\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "svd.fit(Phi_w)\n",
    "\n",
    "# reduced data\n",
    "Phi_w_transformed_SVD = svd.fit_transform(Phi_w)\n",
    "\n",
    "Phi_w_transformed_SVD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD total % variance explained: 22.70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a28d00250>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find percentage of variance explained using PCA\n",
    "perc_var = np.round(svd.explained_variance_ratio_, decimals=4)*100\n",
    "\n",
    "# plot cummulative distribution\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('SVD Analysis')\n",
    "plt.ylim(0,25)\n",
    "\n",
    "print('SVD total % variance explained: {:.2f}'.format(np.sum(perc_var)))\n",
    "plt.plot(np.cumsum(perc_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Investigate the resulting embedding in two ways:\n",
    "\n",
    "- Cluster the vocabulary into 100 clusters. Look them over; do they seem completely random, or is there some sense       to them?\n",
    "- Try finding the nearest neighbor of selected words. Do the answers make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 1.79 s, total: 25.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# initialize k-means\n",
    "kmeans_PCA = KMeans(n_clusters=100, random_state=0, init='k-means++')\n",
    "kmeans_SVD = KMeans(n_clusters=100, random_state=0, init='k-means++')\n",
    "\n",
    "# git the k-means by passing it the input\n",
    "kmeans_PCA.fit(Phi_w_transformed_PCA) # method 1: PCA\n",
    "kmeans_SVD.fit(Phi_w_transformed_SVD) # method 2: SVD\n",
    "\n",
    "# retrieve labels (cluster ids) for all the inputs\n",
    "labels_PCA = kmeans_PCA.labels_\n",
    "labels_SVD = kmeans_SVD.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# initialize dictionaries\n",
    "V_clusters_PCA = defaultdict(list)\n",
    "V_clusters_SVD = defaultdict(list)\n",
    "\n",
    "# form clusters\n",
    "for c in range(100):\n",
    "    \n",
    "    # add clusters inside dict as lists\n",
    "    V_clusters_PCA[c] = [V[i] for i in np.where(labels_PCA == c)[0]]\n",
    "    V_clusters_SVD[c] = [V[i] for i in np.where(labels_SVD == c)[0]]\n",
    "    \n",
    "# display number of clusters\n",
    "len(V_clusters_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: PCA\n",
      "-----> word 1: magnitude\n",
      "['radio', 'mass', 'energy', 'source', 'produced', 'sources', 'loss', 'speed', 'plus', 'impact', 'resolution', 'associated', 'scale', 'intensity', 'measurements', 'foods', 'efficiency', 'comparison', 'concentration', 'essentially', 'models', 'wave', 'relative', 'atomic', 'meat', 'thickness', 'resulting', 'possibilities', 'sufficiently', 'particles', 'comparable', 'presumably', 'thyroid', 'output', '1000', 'visible', 'receiving', 'chlorine', 'missiles', 'emission', 'density', 'flux', 'proportion', 'magnitude', 'electron', 'velocity', 'estimates', 'heating', 'diffusion', 'planets', 'lengths', 'availability', 'particle', 'planetary', 'input', 'biological']\n",
      "-----> word 2: miles\n",
      "['two', 'years', 'three', 'days', 'later', 'several', 'four', 'times', 'five', 'ago', 'six', 'minutes', 'months', 'hours', 'miles', 'hundred', 'ten', 'weeks', 'nearly', 'persons', 'couple', 'seven', 'eight', 'spent', 'dollars', 'thousand', 'nine', 'spend', 'twelve', 'oclock', 'eleven', 'fourteen']\n",
      "\n",
      "Method 2: SVD\n",
      "-----> word 1: magnitude\n",
      "['mass', 'produce', 'impact', 'cutting', 'resolution', 'associated', 'tension', 'intensity', 'conventional', 'foods', 'efficiency', 'extremely', 'concentration', 'models', 'periods', 'wave', 'electrical', 'load', 'considerably', 'resulting', 'occur', 'possibilities', 'sufficiently', 'particles', 'presumably', 'introduction', 'pressures', 'thyroid', 'organic', 'contains', 'devices', 'output', 'visible', 'receiving', 'chlorine', 'emission', 'variation', 'skywave', 'density', 'fallout', 'strain', 'dimensions', 'flux', 'magnitude', 'electron', 'roots', 'velocity', 'reflect', 'exposure', 'composition', 'estimates', 'heating', 'diffusion', 'subjected', 'phases', 'planets', 'crystal', 'sphere', 'frequency', 'availability', 'loop', 'particle', 'fluid', 'protein', 'planetary', 'planet', 'input', 'fats', 'binding', 'hypothalamus', 'reactivity', 'optical', 'nonspecific']\n",
      "-----> word 2: miles\n",
      "['two', 'years', 'three', 'several', 'four', 'five', 'ago', 'six', 'minutes', 'miles', 'hundred', 'ten', 'couple', 'seven', 'eight', 'dollars', 'thousand', 'nine', 'twenty', 'fifty', 'thirty', 'fifteen', 'twelve', 'eleven', 'forty', 'fourteen', 'twentyfive']\n",
      "\n",
      "Similar words\n",
      "-----> word 1: magnitude\n",
      "{'visible', 'sufficiently', 'intensity', 'availability', 'flux', 'resulting', 'concentration', 'wave', 'resolution', 'receiving', 'velocity', 'planetary', 'possibilities', 'estimates', 'impact', 'particle', 'mass', 'associated', 'foods', 'heating', 'magnitude', 'density', 'electron', 'diffusion', 'input', 'emission', 'presumably', 'models', 'planets', 'thyroid', 'efficiency', 'output', 'chlorine', 'particles'}\n",
      "-----> word 2: miles\n",
      "{'years', 'minutes', 'three', 'fourteen', 'two', 'ten', 'five', 'several', 'hundred', 'seven', 'nine', 'eleven', 'couple', 'four', 'dollars', 'twelve', 'thousand', 'ago', 'six', 'eight', 'miles'}\n"
     ]
    }
   ],
   "source": [
    "# display clusters for selected words \n",
    "\n",
    "# define vocabulary word indices\n",
    "idx1, idx2 = 3563, 455\n",
    "\n",
    "# predict using PCA\n",
    "print('Embedding Method 1: PCA')\n",
    "print('-----> word 1: {}'.format(V[idx1]))\n",
    "Clust_word1_PCA = V_clusters_PCA[kmeans_PCA.predict([Phi_w_transformed_PCA[idx1]])[0]]\n",
    "print(Clust_word1_PCA)\n",
    "print('-----> word 2: {}'.format(V[idx2]))\n",
    "Clust_word2_PCA = V_clusters_PCA[kmeans_PCA.predict([Phi_w_transformed_PCA[idx2]])[0]]\n",
    "print(Clust_word2_PCA)\n",
    "\n",
    "# predict using SVD\n",
    "print('\\nEmbedding Method 2: SVD')\n",
    "print('-----> word 1: {}'.format(V[idx1]))\n",
    "Clust_word1_SVD = V_clusters_SVD[kmeans_SVD.predict([Phi_w_transformed_SVD[idx1]])[0]]\n",
    "print(Clust_word1_SVD)\n",
    "print('-----> word 2: {}'.format(V[idx2]))\n",
    "Clust_word2_SVD = V_clusters_SVD[kmeans_SVD.predict([Phi_w_transformed_SVD[idx2]])[0]]\n",
    "print(Clust_word2_SVD)\n",
    "\n",
    "# print similar words \n",
    "print('\\nSimilar words')\n",
    "print('-----> word 1: {}'.format(V[idx1]))\n",
    "print(set(Clust_word1_PCA).intersection(set(Clust_word1_SVD)))\n",
    "print('-----> word 2: {}'.format(V[idx2]))\n",
    "print(set(Clust_word2_PCA).intersection(set(Clust_word2_SVD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer (Q6) - Clustering the vocabulary to 100 clusters with K-Means\n",
    "\n",
    "The clusters generated after dimensionality reduction, using PCA and SVD methods, seem to make sense and are not completely at random. To illustrate this idea two words have been chosen at random (see cell above) and their respective clusters are displayed using each method. Looking at the first word \"magnitude\" it seems that both methods are clustering very similarly with about 34 words overlapping between the both, and the context of the words are related mostly to science such as chemistry, biology, and physics. Likewise, the second word \"miles\" seems to cluster similarly with 21 words overlapping, and the context of the cluster seems to be related to time, numbers, and currency units. \n",
    "\n",
    "In general, the 100 dimensional representation using both methods (PCA and SVD) seems to be very similar in variance metrics with a total percentage of variance explained of about 23%. When K-means is applied to the vocabulary they both create clusters with words that have similar meaning, but the clusters are not exactly the same. The variation introduced when finding the clusters doesn't really mean that one is performing better over the other, but instead that they are capturing similar information differently. As a consequence, their applications would have to be considered based on the cluster labels defined and the context behind the words being used to process the stream of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Method 1: PCA\n",
      "Vocabulary, NN Prediction\n",
      "\n",
      "personnel , expenditures\n",
      "milligrams , beef\n",
      "window , door\n",
      "correct , measurement\n",
      "provided , provide\n",
      "mathematics , science\n",
      "studio , orange\n",
      "agencies , public\n",
      "jew , lonely\n",
      "reorganization , payments\n",
      "intensity , thermal\n",
      "folks , dare\n",
      "military , increased\n",
      "corps , peace\n",
      "originally , publication\n",
      "plenty , theres\n",
      "making , made\n",
      "hated , cold\n",
      "romantic , lucy\n",
      "damned , maggie\n",
      "answer , yes\n",
      "remainder , benefits\n",
      "christian , religious\n",
      "standard , materials\n",
      "17 , 30\n",
      "\n",
      "Embedding Method 2: SVD\n",
      "Vocabulary, NN Prediction\n",
      "\n",
      "personnel , service\n",
      "milligrams , beef\n",
      "window , stood\n",
      "correct , must\n",
      "provided , available\n",
      "mathematics , science\n",
      "studio , knows\n",
      "agencies , public\n",
      "jew , lonely\n",
      "reorganization , payments\n",
      "intensity , thermal\n",
      "folks , nice\n",
      "military , economic\n",
      "corps , peace\n",
      "originally , expert\n",
      "plenty , theres\n",
      "making , made\n",
      "hated , panic\n",
      "romantic , mercy\n",
      "damned , maggie\n",
      "answer , something\n",
      "remainder , revenues\n",
      "christian , religious\n",
      "standard , value\n",
      "17 , oct\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# try finding the nearest neighbor for 25 random words from the vocabulary\n",
    "\n",
    "# Embedding Method 1: PCA\n",
    "\n",
    "# find neighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2, metric='cosine').fit(Phi_w_transformed_PCA)\n",
    "word_idx = [randrange(len(V)) for i in range(25)] # get random list of indices\n",
    "distances, indices = neigh.kneighbors(Phi_w_transformed_PCA[word_idx])\n",
    "\n",
    "# print words and NN\n",
    "print('Embedding Method 1: PCA\\nVocabulary, NN Prediction\\n')\n",
    "NN_idx = [k[1] for k in indices] # first element is the word itself, select second\n",
    "for i,j in zip(word_idx, NN_idx):\n",
    "    print(V[i],',',V[j])\n",
    "\n",
    "\n",
    "# Embedding Method 2: SVD\n",
    "\n",
    "# find neighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2, metric='cosine').fit(Phi_w_transformed_SVD)\n",
    "distances, indices = neigh.kneighbors(Phi_w_transformed_SVD[word_idx])\n",
    "\n",
    "# print words and NN\n",
    "print('\\nEmbedding Method 2: SVD\\nVocabulary, NN Prediction\\n')\n",
    "NN_idx = [k[1] for k in indices] # first element is the word itself, select second\n",
    "for i,j in zip(word_idx, NN_idx):\n",
    "    print(V[i],',',V[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer (Q6) - Finding the nearest neighbor of selected words\n",
    "\n",
    "A total of 25 random words were selected from the vocabulary in order to apply a Nearest Neighbors (NN) algorithm using embedded representations with PCA and SVD from a Positive Pointwise Mutual Information words matrix. The NN is then found for each word and printed side-by-side with the vocabulary word (as shown in the cell above). Looking at the list of pairs is very clear that many of the words selected make sense, meaning that is likely to find them next to each other on a text stream or they have similar context. And when comparing the performance between PCA and SVD they both seem to be performing somewhat similarly; in the example above there are about 11 words out of 25 where the NN is exactly the same which equals to ~44% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The Brown corpus is very small. Current work on word embedding uses data sets that are several orders of magnitude larger, but the methodology is along the same lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 What to turn in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the due date, turn in a typewritten report containing the following elements (each labeled clearly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A description of your 100-dimensional embedding.\n",
    "\n",
    "    The description should be concise and clear, and should make it obvious exactly what steps you took to obtain your word embeddings. Below, we will denote these as Ψ(w) ∈ R100, for w ∈ V . Also clarify exactly how you selected the vocabulary V and the context words C. (35 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSteps used for this section are described in the implementation above, and also in the report below. \\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Steps used for this section are described in the implementation above, and also in the report below. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Nearest neighbor results.\n",
    "\n",
    "    Pick a collection of 25 words w ∈ V . For each w, return its nearest neighbor w′ ̸= w in V . A popular distance measure to use for this is cosine distance:\n",
    "<br />\n",
    "<div align=\"center\">1 − ( Ψ(w).Ψ(w′) / ||Ψ(w)|| ||Ψ(w′)|| )</div>\n",
    "\n",
    "    Here are some suggestions for words you might choose:\n",
    "\n",
    "    communism, autumn, cigarette, pulmonary, mankind, africa, chicago, revolution, september, chemical, detergent, dictionary, storm, worship\n",
    "\n",
    "    Do the results make any sense? You can use other distance measures apart from cosine distance to improve the results. (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>PCA_cosine</th>\n",
       "      <th>SVD_cosine</th>\n",
       "      <th>PCA_euclidean</th>\n",
       "      <th>SVD_euclidean</th>\n",
       "      <th>PCA_manhattan</th>\n",
       "      <th>SVD_manhattan</th>\n",
       "      <th>PCA_chebyshev</th>\n",
       "      <th>SVD_chebyshev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>communism</td>\n",
       "      <td>era</td>\n",
       "      <td>war</td>\n",
       "      <td>era</td>\n",
       "      <td>utopian</td>\n",
       "      <td>utopian</td>\n",
       "      <td>utopian</td>\n",
       "      <td>giant</td>\n",
       "      <td>exploration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autumn</td>\n",
       "      <td>storm</td>\n",
       "      <td>summer</td>\n",
       "      <td>fogg</td>\n",
       "      <td>journey</td>\n",
       "      <td>storm</td>\n",
       "      <td>time</td>\n",
       "      <td>fogg</td>\n",
       "      <td>orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cigarette</td>\n",
       "      <td>swung</td>\n",
       "      <td>suddenly</td>\n",
       "      <td>bullet</td>\n",
       "      <td>ben</td>\n",
       "      <td>haney</td>\n",
       "      <td>time</td>\n",
       "      <td>bullet</td>\n",
       "      <td>whisky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pulmonary</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mankind</td>\n",
       "      <td>divine</td>\n",
       "      <td>world</td>\n",
       "      <td>divine</td>\n",
       "      <td>fatal</td>\n",
       "      <td>twentieth</td>\n",
       "      <td>fatal</td>\n",
       "      <td>inevitably</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>africa</td>\n",
       "      <td>asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>markets</td>\n",
       "      <td>germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chicago</td>\n",
       "      <td>portland</td>\n",
       "      <td>club</td>\n",
       "      <td>portland</td>\n",
       "      <td>york</td>\n",
       "      <td>portland</td>\n",
       "      <td>portland</td>\n",
       "      <td>reports</td>\n",
       "      <td>club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>revolution</td>\n",
       "      <td>modern</td>\n",
       "      <td>modern</td>\n",
       "      <td>time</td>\n",
       "      <td>oral</td>\n",
       "      <td>suspected</td>\n",
       "      <td>oral</td>\n",
       "      <td>legislative</td>\n",
       "      <td>killing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>september</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>december</td>\n",
       "      <td>june</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chemical</td>\n",
       "      <td>drugs</td>\n",
       "      <td>drugs</td>\n",
       "      <td>drugs</td>\n",
       "      <td>drugs</td>\n",
       "      <td>instances</td>\n",
       "      <td>drugs</td>\n",
       "      <td>drugs</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>detergent</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>illustration</td>\n",
       "      <td>folklore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>occurrence</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>storm</td>\n",
       "      <td>noon</td>\n",
       "      <td>summer</td>\n",
       "      <td>noon</td>\n",
       "      <td>fogg</td>\n",
       "      <td>noon</td>\n",
       "      <td>fogg</td>\n",
       "      <td>circuit</td>\n",
       "      <td>journey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>worship</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>organized</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>conception</td>\n",
       "      <td>life</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>entering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>received</td>\n",
       "      <td>receive</td>\n",
       "      <td>washington</td>\n",
       "      <td>first</td>\n",
       "      <td>washington</td>\n",
       "      <td>first</td>\n",
       "      <td>perhaps</td>\n",
       "      <td>special</td>\n",
       "      <td>washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>threw</td>\n",
       "      <td>door</td>\n",
       "      <td>front</td>\n",
       "      <td>barn</td>\n",
       "      <td>foot</td>\n",
       "      <td>barn</td>\n",
       "      <td>turned</td>\n",
       "      <td>stairs</td>\n",
       "      <td>gardens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>screw</td>\n",
       "      <td>ben</td>\n",
       "      <td>one</td>\n",
       "      <td>ben</td>\n",
       "      <td>one</td>\n",
       "      <td>ben</td>\n",
       "      <td>skyros</td>\n",
       "      <td>victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>empirical</td>\n",
       "      <td>solutions</td>\n",
       "      <td>structure</td>\n",
       "      <td>hypothalamic</td>\n",
       "      <td>hypothalamic</td>\n",
       "      <td>hypothalamic</td>\n",
       "      <td>melody</td>\n",
       "      <td>hypothalamic</td>\n",
       "      <td>continuity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>relatives</td>\n",
       "      <td>intimate</td>\n",
       "      <td>family</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>bet</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>emperor</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lack</td>\n",
       "      <td>interest</td>\n",
       "      <td>need</td>\n",
       "      <td>work</td>\n",
       "      <td>need</td>\n",
       "      <td>work</td>\n",
       "      <td>personal</td>\n",
       "      <td>view</td>\n",
       "      <td>making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>religion</td>\n",
       "      <td>political</td>\n",
       "      <td>personal</td>\n",
       "      <td>great</td>\n",
       "      <td>personal</td>\n",
       "      <td>experience</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>human</td>\n",
       "      <td>rather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>accepted</td>\n",
       "      <td>merely</td>\n",
       "      <td>beginning</td>\n",
       "      <td>one</td>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>spencer</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>knees</td>\n",
       "      <td>shoulders</td>\n",
       "      <td>shoulders</td>\n",
       "      <td>shaking</td>\n",
       "      <td>shaking</td>\n",
       "      <td>shaking</td>\n",
       "      <td>shaking</td>\n",
       "      <td>wiped</td>\n",
       "      <td>suitcase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>city</td>\n",
       "      <td>york</td>\n",
       "      <td>york</td>\n",
       "      <td>york</td>\n",
       "      <td>york</td>\n",
       "      <td>york</td>\n",
       "      <td>york</td>\n",
       "      <td>central</td>\n",
       "      <td>located</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>billion</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>january</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vocabulary PCA_cosine  SVD_cosine PCA_euclidean SVD_euclidean  \\\n",
       "0    communism        era         war           era       utopian   \n",
       "1       autumn      storm      summer          fogg       journey   \n",
       "2    cigarette      swung    suddenly        bullet           ben   \n",
       "3    pulmonary     artery      artery        artery        artery   \n",
       "4      mankind     divine       world        divine         fatal   \n",
       "5       africa       asia        asia          asia          asia   \n",
       "6      chicago   portland        club      portland          york   \n",
       "7   revolution     modern      modern          time          oral   \n",
       "8    september   december    december      december      december   \n",
       "9     chemical      drugs       drugs         drugs         drugs   \n",
       "10   detergent    fabrics     fabrics       fabrics       fabrics   \n",
       "11  dictionary       text        text          text          text   \n",
       "12       storm       noon      summer          noon          fogg   \n",
       "13     worship    beliefs   organized       beliefs    conception   \n",
       "14    received    receive  washington         first    washington   \n",
       "15       threw       door       front          barn          foot   \n",
       "16  ridiculous      screw         ben           one           ben   \n",
       "17   empirical  solutions   structure  hypothalamic  hypothalamic   \n",
       "18   relatives   intimate      family      coolidge           bet   \n",
       "19        lack   interest        need          work          need   \n",
       "20    religion  political    personal         great      personal   \n",
       "21    accepted     merely   beginning           one         known   \n",
       "22       knees  shoulders   shoulders       shaking       shaking   \n",
       "23        city       york        york          york          york   \n",
       "24     billion    million     million       million       million   \n",
       "\n",
       "   PCA_manhattan SVD_manhattan PCA_chebyshev SVD_chebyshev  \n",
       "0        utopian       utopian         giant   exploration  \n",
       "1          storm          time          fogg       orleans  \n",
       "2          haney          time        bullet        whisky  \n",
       "3         artery        artery        artery        artery  \n",
       "4      twentieth         fatal    inevitably         world  \n",
       "5           asia          asia       markets       germany  \n",
       "6       portland      portland       reports          club  \n",
       "7      suspected          oral   legislative       killing  \n",
       "8       december      december      december          june  \n",
       "9      instances         drugs         drugs         drugs  \n",
       "10       fabrics       fabrics  illustration      folklore  \n",
       "11          text          text    occurrence          text  \n",
       "12          noon          fogg       circuit       journey  \n",
       "13          life       beliefs       beliefs      entering  \n",
       "14         first       perhaps       special    washington  \n",
       "15          barn        turned        stairs       gardens  \n",
       "16           one           ben        skyros        victim  \n",
       "17  hypothalamic        melody  hypothalamic    continuity  \n",
       "18      coolidge      coolidge       emperor           one  \n",
       "19          work      personal          view        making  \n",
       "20    experience    philosophy         human        rather  \n",
       "21         known       spencer     sometimes       history  \n",
       "22       shaking       shaking         wiped      suitcase  \n",
       "23          york          york       central       located  \n",
       "24       million       million       january            11  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# try finding the nearest neighbor for a collection of 25 words\n",
    "\n",
    "# define sample words and get their indices\n",
    "sample_words = ['communism', 'autumn', 'cigarette', 'pulmonary', 'mankind', 'africa', 'chicago', \n",
    "                'revolution', 'september', 'chemical', 'detergent', 'dictionary', 'storm', 'worship']\n",
    "sample_words_idx = [V.index(i) for i in sample_words]\n",
    "\n",
    "# fill the collection of 25 with 11 more random words\n",
    "word_idx = sample_words_idx + [randrange(len(V)) for i in range(11)] \n",
    "\n",
    "# create dataframe with vocabulary words\n",
    "df = pd.DataFrame([V[i] for i in word_idx], columns=['vocabulary'])\n",
    "\n",
    "# find NN word using different distance metrics\n",
    "for d in ['cosine', 'euclidean', 'manhattan', 'chebyshev']:\n",
    "\n",
    "    # Embedding Method 1: PCA\n",
    "\n",
    "    # find neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=2, metric=d).fit(Phi_w_transformed_PCA)\n",
    "    distances, indices = neigh.kneighbors(Phi_w_transformed_PCA[word_idx])\n",
    "\n",
    "    # add NN to dataframe\n",
    "    col_name = 'PCA' + '_' + d\n",
    "    df[col_name] = [V[i] for i in [k[1] for k in indices]] # second element is NN\n",
    "\n",
    "    # Embedding Method 2: SVD\n",
    "\n",
    "    # find neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=2, metric=d).fit(Phi_w_transformed_SVD)\n",
    "    distances, indices = neigh.kneighbors(Phi_w_transformed_SVD[word_idx])\n",
    "\n",
    "    # add NN to dataframe\n",
    "    col_name = 'SVD' + '_' + d\n",
    "    df[col_name] = [V[i] for i in [k[1] for k in indices]] # second element is NN\n",
    "\n",
    "# display dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Clustering.\n",
    "\n",
    "    Using the vectorial representation Ψ(.), cluster the words in V into 100 groups. Clearly specify what algorithm and distance function you using for this, and the reasons for your choices.\n",
    "\n",
    "    Look over the resulting 100 clusters. Do any of them seem even moderately coherent? Pick out a few of the best clusters and list the words in them. (35 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSteps used for this section are described in the implementation above, and also in the report below. \\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Steps used for this section are described in the implementation above, and also in the report below. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare the similarities and differences when using the embedding and full matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>Full-Matrix_cosine</th>\n",
       "      <th>Full-Matrix_euclidean</th>\n",
       "      <th>Full-Matrix_manhattan</th>\n",
       "      <th>Full-Matrix_chebyshev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>communism</td>\n",
       "      <td>utopian</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autumn</td>\n",
       "      <td>summer</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cigarette</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pulmonary</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>artery</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mankind</td>\n",
       "      <td>christ</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>africa</td>\n",
       "      <td>asia</td>\n",
       "      <td>united</td>\n",
       "      <td>notte</td>\n",
       "      <td>among</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chicago</td>\n",
       "      <td>portland</td>\n",
       "      <td>new</td>\n",
       "      <td>kansas</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>revolution</td>\n",
       "      <td>music</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>september</td>\n",
       "      <td>july</td>\n",
       "      <td>year</td>\n",
       "      <td>rico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chemical</td>\n",
       "      <td>drugs</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>given</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>detergent</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>text</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>storm</td>\n",
       "      <td>weekend</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>worship</td>\n",
       "      <td>degree</td>\n",
       "      <td>one</td>\n",
       "      <td>amen</td>\n",
       "      <td>program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>depression</td>\n",
       "      <td>mount</td>\n",
       "      <td>one</td>\n",
       "      <td>puerto</td>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>collective</td>\n",
       "      <td>involves</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>especially</td>\n",
       "      <td>many</td>\n",
       "      <td>many</td>\n",
       "      <td>notte</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>often</td>\n",
       "      <td>different</td>\n",
       "      <td>one</td>\n",
       "      <td>notte</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>involved</td>\n",
       "      <td>fact</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>among</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>paula</td>\n",
       "      <td>lover</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jobs</td>\n",
       "      <td>job</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>although</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dartmouth</td>\n",
       "      <td>college</td>\n",
       "      <td>one</td>\n",
       "      <td>rico</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>present</td>\n",
       "      <td>important</td>\n",
       "      <td>may</td>\n",
       "      <td>notte</td>\n",
       "      <td>perhaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>observations</td>\n",
       "      <td>measurements</td>\n",
       "      <td>one</td>\n",
       "      <td>tetrachloride</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>demand</td>\n",
       "      <td>area</td>\n",
       "      <td>one</td>\n",
       "      <td>dairy</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vocabulary Full-Matrix_cosine Full-Matrix_euclidean  \\\n",
       "0      communism            utopian                   one   \n",
       "1         autumn             summer                   one   \n",
       "2      cigarette           sleeping                   one   \n",
       "3      pulmonary             artery                artery   \n",
       "4        mankind             christ                   one   \n",
       "5         africa               asia                united   \n",
       "6        chicago           portland                   new   \n",
       "7     revolution              music                   one   \n",
       "8      september               july                  year   \n",
       "9       chemical              drugs                   one   \n",
       "10     detergent            fabrics                   one   \n",
       "11    dictionary               text                   one   \n",
       "12         storm            weekend                   one   \n",
       "13       worship             degree                   one   \n",
       "14    depression              mount                   one   \n",
       "15    collective           involves                   one   \n",
       "16    especially               many                  many   \n",
       "17         often          different                   one   \n",
       "18      involved               fact                   one   \n",
       "19         paula              lover                   one   \n",
       "20          jobs                job                   one   \n",
       "21     dartmouth            college                   one   \n",
       "22       present          important                   may   \n",
       "23  observations       measurements                   one   \n",
       "24        demand               area                   one   \n",
       "\n",
       "   Full-Matrix_manhattan Full-Matrix_chebyshev  \n",
       "0                   rico                  must  \n",
       "1                  notte                  made  \n",
       "2                  notte                  back  \n",
       "3                 artery            population  \n",
       "4                   rico                 every  \n",
       "5                  notte                 among  \n",
       "6                 kansas               provide  \n",
       "7                  notte                  even  \n",
       "8                   rico                     1  \n",
       "9                  notte                 given  \n",
       "10                  rico                 found  \n",
       "11                  rico                  form  \n",
       "12                  rico                  went  \n",
       "13                  amen               program  \n",
       "14                puerto                 every  \n",
       "15                 notte                 trade  \n",
       "16                 notte                  work  \n",
       "17                 notte                 would  \n",
       "18                  rico                 among  \n",
       "19                  rico                  last  \n",
       "20                  rico              although  \n",
       "21                  rico                  also  \n",
       "22                 notte               perhaps  \n",
       "23         tetrachloride                 study  \n",
       "24                 dairy                  many  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# try finding the nearest neighbor for a collection of 25 words\n",
    "\n",
    "# define sample words and get their indices\n",
    "sample_words = ['communism', 'autumn', 'cigarette', 'pulmonary', 'mankind', 'africa', 'chicago', \n",
    "                'revolution', 'september', 'chemical', 'detergent', 'dictionary', 'storm', 'worship']\n",
    "sample_words_idx = [V.index(i) for i in sample_words]\n",
    "\n",
    "# fill the collection of 25 with 11 more random words\n",
    "word_idx = sample_words_idx + [randrange(len(V)) for i in range(11)] \n",
    "\n",
    "# create dataframe with vocabulary words\n",
    "df = pd.DataFrame([V[i] for i in word_idx], columns=['vocabulary'])\n",
    "\n",
    "# compute neighbors without embedding\n",
    "for d in ['cosine', 'euclidean', 'manhattan', 'chebyshev']:\n",
    "    \n",
    "    # find neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=2, metric=d).fit(Phi_w)\n",
    "    distances, indices = neigh.kneighbors(Phi_w[word_idx])\n",
    "\n",
    "    # add NN to dataframe\n",
    "    col_name = 'Full-Matrix' + '_' + d\n",
    "    df[col_name] = [V[i] for i in [k[1] for k in indices]]\n",
    "\n",
    "# display dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you are expected to come up with three strategies (one for each task). You should to turn-in a concise report (2-3 pages) with the results of the above tasks along with a description of your final approach. You should clearly describe your approach along with all the parameters you use. The description should be enough for someone else to recreate your model/results. Along with the description, you are expected to provide the performance analysis of your model including the shortcomings (if any) and any ideas to further improve it. You should also include a brief description of all the major approaches you tried (with code on Gradescope), and the reason why you selected your current model over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won’t be graded on the performance of your model but you’ll be graded (30%) on the comprehensiveness of your analysis. You are not expected to come up with a new algorithm (though that would be great!), but your report should justify that the approach you suggest is equivalent/better than some of the standard models discussed in the class. In essence, to secure full marks in this section, your analysis of this problem should be in-depth and thorough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for section 3 - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the 100-dimensional embedding\n",
    "\n",
    "This analysis contains over 1 million words from the NLTK Brown corpus, which is a collection of text samples from a variety of sources. To start, the corpus was read into Python as a list of raw words coming from all the combined text streams. Then, data processing and preparation was achieved by first converting all letters for each word to lower case and removing any spaces or punctuations such as commas and question marks, following by removing list elements that appear in the stopwords list from the NLTK corpus library. After removing stopwords the remaining list of words had over half a million elements, from here two more lists were generated named as Vocabulary (C) and Context words (C) containing the most commonly-occuring words, 5000 and 1000 respectively; therefore the second is a subset of the first. And the last step for data preparation was to create the co-occurance matrix with both lists described above, whose dimension is 5000 x 1000. The matrix is basically created by scanning through the listed text stream (with processed words) and finding the number of times each element in C occurs in a defined window of two elements before and after around each element in V. \n",
    "\n",
    "Now, some natural language processing was performed by finding the positive pointwise mutual information (PPMI) on the co-ocurrance matrix. The first step was to find the probabilities for each element in C given elements in V, where the matrix dimension is the same as the original matrix with 5000 x 1000. Secondly, the probability distribution for each element in C was found, whose dimension is 1000. With this in mind, the PPMI was then be calculated by taking the element-wise maximum value between zero and the ratio of logs between the first and second probabilities calculated above. This was able to set any negative values to zero as the PPMI is only interested in understanding the mutual information between words and not how they occur individually, for example, it kept the relationship for the words \"new york\" despite the word \"new\" can appear many times individually in context with other words. \n",
    "\n",
    "Lastly, the PPMI result found was highly dimensional with 1000 dimensions, therefore, using a lower dimensional representation was used with the intent to help model training efficiency, or in theory it could also possibly help with issues related to the curse of dimensionality. To achieve this, the analysis considered a 100-dimensional embedding where two methods are compared, namely Singular Value Decomposition (SVD) and Principal Component Analysis (PCA). After applying each of them, the transformed PPMI expressed about 23% total variance explained from the original data. The embedding was essentially expressing about a fifth of the initial variance with 10-fold decrease in features, these kind of trade-offs are decided according to the practicality of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest neighbor results\n",
    "\n",
    "Nearest neighbors (NN) is implemented above (section 3.2) by defining a list of suggested words and adding an additional 11 random words to form a collection of 25 words total, where every word is an element of the vocabulary. The model used comes from the libray sklearn.neighbors and it was processed using both methods tested for embedding as mentioned above, PCA and SVD. For each, the embedding (or transformed data) of dimension 100 was processed with the NearestNeighbors model for each of the 5000 words in the vocabulary. The parameters used are n_neighbors=2, since the first NN is the word itself the second one is taken. In addition to different distance metrics including cosine, eucledian, manhattan, and chebyshev. The results are summarized in the dataframe from section 3.2, and basically the results appear to make sense as the majority of words have similar context despite which distance metric or embedding method was used. However, is worth highlighting that there is a distinguishable pattern for this collection of words where all methods predicted the same word with the exception of \"chebyshev\", and this happens on rows 5, 11, 23, and 24 for the words asia, fabrics, york, and million. All in all, Is probable that some methods are better at prediction a particular context or cluster of words by finding specific hidden patterns, for this reason the parameter selection should depend on the type of application and the ability to practically apply restrictions that would make the model predict more accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "\n",
    "The embedding for each method, PCA and SVD, was used to cluster the vocabulary (with 5000 words) into 100 clusters. The algorithm used comes from sklearn.cluster library and is called Kmeans, where the parameters used are n_clusters = 100 and init = 'k-means++'. Section 2.6 from the notebook above shows an example where two words were picked at random, \"magnitude\" and \"miles\". In general, the clusters generated for such words seem to be coherent across both embeddings tested, the first word \"magnitude\" clustered with other words similar in the context of science such as chemistry, biology, and physics. While the second word \"miles\" clustered with other words related with time, numbers, and currency units. Then, when comparing the clusters where each of the words belongs PCA clustered a total of 56 and 32 words including \"magnitude\" and \"miles\" respectively, and SVD a total of 73 and 27 in the same order. And when comparing the overlapping words pertaining to the clusters of \"magnitude\" and \"miles\", both embeddings agreed on 34 and 21 respectively. This is a very decent result considering that both embeddings have about 23% as the total percentage of variance explained. The amount of overlap between embeddings and higher relationship within the cluster might also well depend on the word chosen, in this case the word \"magnitude\" seems to be part of a larger cluster as it is likely to be found in a variety of places and disciplines as compared to the word \"miles\" that is probably only found when refering to distances or measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison using full matrix and reduced matrix\n",
    "\n",
    "To keep exploring the importance of the embeddings generated, the full matrix for PPMI was used to replicate the analysis for nearest neighbors from above. The results are summarized on a dataframe in section 3.4 where it basically shows how the performance is significantly lower as compared to the embedding results in section 3.2 with PCA and SVD. For instance, the full matrix resuls when using the cosine metric for distance seem to be decent as the context between words makes sense and is comparable to the embedding results, however, the other distance metrics are completely incorrect. There is an interesting repetition of errors where words are repeated when predicting the NN, for example the words \"one\" and \"rico\" appear multiple times for the distance metrics 'eucledian' and 'manhattan'. In summary, when comparing the NN prediction between full matrix and reduced matrix the winner seems to be the reduced matrix in every scenario tested in this analysis. Is possible that methods like clustering and nearest neighbor break down at higher dimensions as the distances might become highly concentrated or the values get lost in space, these observations could definitely be tied to the curse of dimensionality. And with this in mind, the need to create an embeding before clustering is of absolute importance in order to produce a useful model with accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future steps\n",
    "\n",
    "Lastly, to further explore this analysis there are a few things to consider around data preparation, domain knowledge, and statistics to bring better insight on the results. First, some restrictions in the vocabulary might be useful in order to elimiate added noise to the predictions. For example some words from the vocabulary appear as numerical values or text numbers (i.e. one), and in the context of words these can possibly appear in many places without any specific mutual information of interest between words. Another example would be to reduce the number of words by considering linguistics, words that have similar meaning, or known noise words that might induce negative interactions. Secondly, after data preparation is considered and clustering is performed, it would be useful to talk to a domain expert about each of the clusters learned in order to identify the nature of each group and whether they make sense to belong together or not. In addition, to keep exploring the clusters it would be fundamental to count and quantify que occurence of each word as multiple embedding methods (i.e. PCA) and parameters (i.e. cosine distance) are tested and implemented. And lastly, more work can be done in understanding the words that are clustered near the boundaries among other clusters as they might provide useful insights when trying to understand relationships, rank importance, or being able to discriminate with higher confidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
