{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Topic Modeling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is an unsupervised machine learning technique thatâ€™s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Why Do It? </font>\n",
    "To extract information from a huge amount of text\n",
    "* You can use this information to create indexes and then provide a query interface\n",
    "* Your mini Google search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> What is Gensim? </font>\n",
    "An open-source library that lets one work with topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Steps involved </font>\n",
    "* Get a corpus\n",
    "* Find words in each document of the corupus that are most likely to represent the document (find topics) - store these in a dictionary\n",
    "* Use this dictionary to convert each document into a vector that represents how relevant each topic is to the document\n",
    "* This is our model\n",
    "* Now, you can use it to query topics and find relevant documents (this is one use case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> An Example </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> A Game of Corpora (Step 1) </font>\n",
    "Get a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to\n",
      "[nltk_data]     C:\\Users\\prgzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Let's use the abc corpus\n",
    "import pprint\n",
    "import nltk\n",
    "nltk.download('abc')\n",
    "corpus = nltk.corpus.abc.sents()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> A Clash Of Words (Step 2) </font>\n",
    "Prune the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awb', 'kickbacks', 'prime', 'minister', 'has', 'he', 'knew', 'awb', 'was', 'paying', 'kickbacks', 'iraq', 'despite', 'wheat', 'on', 'iraq', 'wheat', 'sales']\n"
     ]
    }
   ],
   "source": [
    "# Remove some common words and lower case everything\n",
    "stoplist = set(\"\"\"for a of the and to in . , $ \" ' \"\"\".split(' '))\n",
    "pruned_corpus = [[word.lower() for word in document if word.lower() not in stoplist]\n",
    "         for document in corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in pruned_corpus:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "pruned_corpus = [[token for token in text if frequency[token] > 1] for text in pruned_corpus]\n",
    "print(pruned_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='indigo'> A Storm Of Vectors (Step 3) </font>\n",
    "Create a dictionary and convert the corpus to a list of vectors. Gensim will be used now. Install it using: `pip install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',\"': 31,\n",
      " '20': 63,\n",
      " '2002': 27,\n",
      " 'about': 36,\n",
      " 'actually': 55,\n",
      " 'an': 42,\n",
      " 'another': 57,\n",
      " 'at': 32,\n",
      " 'average': 58,\n",
      " 'awb': 0,\n",
      " 'been': 14,\n",
      " 'but': 37,\n",
      " 'by': 15,\n",
      " 'cole': 16,\n",
      " 'despite': 1,\n",
      " 'do': 38,\n",
      " 'email': 43,\n",
      " 'from': 17,\n",
      " 'geary': 44,\n",
      " 'get': 49,\n",
      " 'government': 23,\n",
      " 'grain': 47,\n",
      " 'had': 45,\n",
      " 'has': 2,\n",
      " 'have': 18,\n",
      " 'he': 3,\n",
      " 'howard': 19,\n",
      " 'i': 40,\n",
      " 'inquiry': 20,\n",
      " 'into': 21,\n",
      " 'iraq': 4,\n",
      " 'it': 41,\n",
      " 'kickbacks': 5,\n",
      " 'knew': 6,\n",
      " 'letters': 22,\n",
      " 'may': 33,\n",
      " 'minister': 7,\n",
      " 'mr': 24,\n",
      " 'much': 59,\n",
      " 'not': 39,\n",
      " 'on': 8,\n",
      " 'one': 25,\n",
      " 'over': 60,\n",
      " 'paying': 9,\n",
      " 'payments': 30,\n",
      " 'pretty': 56,\n",
      " 'prices': 50,\n",
      " 'prime': 10,\n",
      " 'producer': 54,\n",
      " 's': 28,\n",
      " 'said': 34,\n",
      " 'sales': 11,\n",
      " 'says': 29,\n",
      " 'support': 48,\n",
      " 'that': 46,\n",
      " 'their': 51,\n",
      " 'they': 52,\n",
      " 'think': 53,\n",
      " 'this': 35,\n",
      " 'tonne': 64,\n",
      " 'too': 61,\n",
      " 'was': 12,\n",
      " 'wheat': 13,\n",
      " 'will': 62,\n",
      " 'with': 26}\n"
     ]
    }
   ],
   "source": [
    "# gensim imports\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "# Create dictionary for most popular words (this will be our list of topics)\n",
    "dictionary = corpora.Dictionary(pruned_corpus)\n",
    "# There are 65 of these\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 1), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Use this dictionary to convert our pruned corpus into a list of vectors\n",
    "# Our list of vectors correspond to a bag-of-words representation\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in pruned_corpus]\n",
    "# First element of the tuple is the topic while the second is no. of times it occurred in this document\n",
    "# Doesn't exactly look like a vector. Think of each document as a 65-dimensional vector, where each \n",
    "# dimension corresponds to the count of the associated word in the document.\n",
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'> A Feast For Modeling (Step 4) </font>\n",
    "Create a model (TF-IDF here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a TF-IDF model out of these vectors\n",
    "\n",
    "# Model training\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'> A Dance With Queries (Step 5) </font>\n",
    "We're done. Let's query with a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.38083693\n",
      "1: 0.14157072\n",
      "2: 0.19464979\n",
      "3: 0.19936569\n",
      "4: 0.0\n",
      "5: 0.19307509\n",
      "6: 0.28589374\n",
      "7: 0.15641421\n",
      "8: 0.0\n",
      "9: 0.0\n",
      "10: 0.0\n",
      "11: 0.0\n",
      "12: 0.0\n",
      "13: 0.0\n",
      "14: 0.0\n",
      "15: 0.0\n",
      "16: 0.0\n",
      "17: 0.0\n",
      "18: 0.0\n",
      "19: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create a similarity matrix (an index)\n",
    "from gensim import similarities\n",
    "# This one uses cosine similarity to compare two vectors\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=65)\n",
    "\n",
    "# Our query - think of this as a Google search\n",
    "query = 'iraq minister'.split()\n",
    "query_bow = dictionary.doc2bow(query)\n",
    "sims = index[tfidf[query_bow]]\n",
    "\n",
    "# Print how relevant is each document to the query\n",
    "for doc_ind, sim in enumerate(sims):\n",
    "    print(\"%i: %s\" % (doc_ind, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.']\n",
      "['But', 'the', 'Prime', 'Minister', 'says', 'letters', 'show', 'he', 'was', 'inquiring', 'about', 'the', 'future', 'of', 'wheat', 'sales', 'in', 'Iraq', 'and', 'do', 'not', 'prove', 'the', 'Government', 'knew', 'of', 'the', 'payments', '.']\n"
     ]
    }
   ],
   "source": [
    "# Let's see 2 relevant documents\n",
    "print(corpus[0])\n",
    "print(corpus[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Other Ways To Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We just looked at the **TF-IDF** model.\n",
    "* Other popular approaches: **LSA, LDA**\n",
    "* Link: https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html (**Section: Available transformations**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> References </font>\n",
    "* Based on: https://radimrehurek.com/gensim/auto_examples/ (**This is the official one. Has 4 good core guides**)\n",
    "* A nice reference in general: https://monkeylearn.com/blog/introduction-to-topic-modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
