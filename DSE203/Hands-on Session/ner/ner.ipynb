{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Named-Entity Recognition </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. One can consider it as a form of chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> What is chunking? </font>\n",
    "The process of extracting phrases from sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='brown'> Example </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "\n",
    "# Let's chunk this sentence\n",
    "sentence = \"By Election Day, more than 60% of U.S. K-12 public school students will be attending \"\\\n",
    "           \"schools that offer in-person learning at least a few days a week, an updated tracker finds.\"\n",
    "# We will use a Noun Phrase Chunker - this is a chunker that looks for Noun Phrases in a sentence\n",
    "# For example 'United States' is a chunk, even though there are two words here.\n",
    "# So, the below regex looks for noun phrases.\n",
    "# The rule states that whenever the chunk finds an optional determiner (DT) followed \n",
    "# by any number of adjectives (JJ) and then a noun (NN), then the Noun Phrase(NP) chunk should be formed.\n",
    "regex = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')\n",
    "# Create the parser\n",
    "chunk_parser = nltk.RegexpParser(regex)\n",
    "\n",
    "# Tokenize and tag the sentence\n",
    "tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "# Form the chunks and create a tree out of it\n",
    "chunked_tree = chunk_parser.parse(tagged_words)\n",
    "\n",
    "# Let's draw the tree\n",
    "# chunked_tree.draw() # Uncomment this to draw the tree - it opens a separate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"chunked_sentence_tree.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  By/IN\n",
      "  Election/NNP\n",
      "  Day/NNP\n",
      "  ,/,\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  60/CD\n",
      "  (NP %/NN)\n",
      "  of/IN\n",
      "  U.S./NNP\n",
      "  K-12/NNP\n",
      "  (NP public/JJ school/NN)\n",
      "  students/NNS\n",
      "  will/MD\n",
      "  be/VB\n",
      "  attending/VBG\n",
      "  schools/NNS\n",
      "  that/WDT\n",
      "  offer/VBP\n",
      "  (NP in-person/JJ learning/NN)\n",
      "  at/IN\n",
      "  least/JJS\n",
      "  a/DT\n",
      "  few/JJ\n",
      "  days/NNS\n",
      "  (NP a/DT week/NN)\n",
      "  ,/,\n",
      "  (NP an/DT updated/JJ tracker/NN)\n",
      "  finds/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# A more raw version of the tree\n",
    "print(chunked_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Time for named-entity chunking </font>\n",
    "The previous chunker just looked for a pattern in the sentence. Nothing was learned. Let's try out a learned chunker now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\prgzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\prgzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the chunker\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# Another download\n",
    "nltk.download('words')\n",
    "\n",
    "# Chunk\n",
    "ne_tree = nltk.ne_chunk(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne_tree.draw() # Uncomment this to draw the tree - it opens a separate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ner_chunked_sentence_tree.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='brown'> A Larger Example </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\prgzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# Download corpus\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Get a list of already tagged sentences\n",
    "treebank_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print(treebank_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find named-entities in the first 10\n",
    "chunked_sents = []\n",
    "for sent in treebank_sentences[:20]:\n",
    "    chunked_sent = nltk.ne_chunk(sent, binary=False)\n",
    "    chunked_sents.append(chunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Dr./NNP\n",
      "  (PERSON Talcott/NNP)\n",
      "  led/VBD\n",
      "  a/DT\n",
      "  team/NN\n",
      "  of/IN\n",
      "  researchers/NNS\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION National/NNP Cancer/NNP Institute/NNP)\n",
      "  and/CC\n",
      "  the/DT\n",
      "  medical/JJ\n",
      "  schools/NNS\n",
      "  of/IN\n",
      "  (ORGANIZATION Harvard/NNP University/NNP)\n",
      "  and/CC\n",
      "  (ORGANIZATION Boston/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# This one found 3 named-entities\n",
    "print(chunked_sents[12])\n",
    "# chunked_sents[12].draw() # Uncomment this to draw the tree - it opens a separate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ner_chunked_larger_sentence_tree.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> How does NER work? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLTK provides a previously trained NER chunker.\n",
    "* The chunker has been trained on a corpus that isn't available otherwise.\n",
    "* What we know: The corpus was annotated manually and a Max Entropy classifier was trained on it.\n",
    "* Basically, the objective was to classify phrases into a bunch of categories (locations, organizations, names, etc.) using a classifier that pretty much uses logistic regression.\n",
    "* **More info at**: https://mattshomepage.com/articles/2016/May/23/nltk_nec/\n",
    "* **Making your own named-entity chunker**: https://nlpforhackers.io/named-entity-extraction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> References </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you plan to read just one: https://www.nltk.org/book/ch07.html (Section 5)\n",
    "* NER with NLTK and SpaCy (another cool NLP library, considered the fastest): https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "* Another useful guide: https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb\n",
    "* NER workings: https://mattshomepage.com/articles/2016/May/23/nltk_nec/\n",
    "* Training your own NER: https://nlpforhackers.io/named-entity-extraction/\n",
    "* **A plug for all the NLTK corpora out their**: http://www.nltk.org/howto/corpus.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
