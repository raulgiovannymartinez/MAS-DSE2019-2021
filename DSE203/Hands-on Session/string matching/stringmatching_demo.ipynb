{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Matching Demo for DSE203-Fall2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### py_stringmatching documentation Link:\n",
    "    http://anhaidgroup.github.io/py_stringmatching/v0.4.1/Tutorial.html\n",
    "    \n",
    "###### jupyter no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_stringmatching as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The currently implemented similarity measures include:\n",
    "- sequence-based measures: affine gap, bag distance, editex, Hamming distance, Jaro, Jaro Winkler, Levenshtein, Needleman Wunsch, partial ratio, partial token sort, ratio, Smith Waterman, token sort.\n",
    "- set-based measures: cosine, Dice, Jaccard, overlap coefficient, Tversky Index.\n",
    "- bag-based measures: TF/IDF.\n",
    "- phonetic-based measures: soundex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Tokenzier type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a set of different tokenizer types provided by py_stringmatching: \n",
    "- alphabetical tokenizer, alphanumeric tokenizer, delimiter-based tokenizer, qgram tokenizer, and whitespace tokenizer (more tokenizer types can easily be added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Tokenizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = ' .hello, world!! data, science, is    amazing!!. hello.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an alphabetical tokenizer that returns a bag of tokens\n",
    "alphabet_tok = sm.AlphabeticTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'data', 'science', 'is', 'amazing', 'hello']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_tok.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an alphanumeric tokenizer\n",
    "alnum_tok = sm.AlphanumericTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'data', 'science', 'is', 'amazing', 'hello']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alnum_tok.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a delimiter tokenizer using comma as a delimiter\n",
    "delim_tok = sm.DelimiterTokenizer(delim_set=[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a whitespace tokenizer\n",
    "ws_tok = sm.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' .hello', ' world!! data', ' science', ' is    amazing!!. hello.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delim_tok.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a qgram tokenizer using q=3\n",
    "qg3_tok = sm.QgramTokenizer(qval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## ',\n",
       " '# .',\n",
       " ' .h',\n",
       " '.he',\n",
       " 'hel',\n",
       " 'ell',\n",
       " 'llo',\n",
       " 'lo,',\n",
       " 'o, ',\n",
       " ', w',\n",
       " ' wo',\n",
       " 'wor',\n",
       " 'orl',\n",
       " 'rld',\n",
       " 'ld!',\n",
       " 'd!!',\n",
       " '!! ',\n",
       " '! d',\n",
       " ' da',\n",
       " 'dat',\n",
       " 'ata',\n",
       " 'ta,',\n",
       " 'a, ',\n",
       " ', s',\n",
       " ' sc',\n",
       " 'sci',\n",
       " 'cie',\n",
       " 'ien',\n",
       " 'enc',\n",
       " 'nce',\n",
       " 'ce,',\n",
       " 'e, ',\n",
       " ', i',\n",
       " ' is',\n",
       " 'is ',\n",
       " 's  ',\n",
       " '   ',\n",
       " '   ',\n",
       " '  a',\n",
       " ' am',\n",
       " 'ama',\n",
       " 'maz',\n",
       " 'azi',\n",
       " 'zin',\n",
       " 'ing',\n",
       " 'ng!',\n",
       " 'g!!',\n",
       " '!!.',\n",
       " '!. ',\n",
       " '. h',\n",
       " ' he',\n",
       " 'hel',\n",
       " 'ell',\n",
       " 'llo',\n",
       " 'lo.',\n",
       " 'o.$',\n",
       " '.$$']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qg3_tok.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an alphabetical tokenizer that returns a set of tokens\n",
    "alphabet_tok_set = sm.AlphabeticTokenizer(return_set=True)\n",
    "ws_tok_set = sm.WhitespaceTokenizer(return_set=True)\n",
    "qg3_tok_set = sm.QgramTokenizer(qval=3, return_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'data', 'science', 'is', 'amazing']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_tok_set.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Similarity Measure Object and Using it to Compute a Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Jaccard similarity measure object\n",
    "jac = sm.Jaccard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Levenshtein similarity measure object\n",
    "lev = sm.Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'string matching package'\n",
    "y = 'string matching library'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Jaccard score over sets of tokens of x and y, tokenized using whitespace\n",
    "jac.get_raw_score(ws_tok_set.tokenize(x), ws_tok_set.tokenize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Jaccard score over sets of tokens of x and y, tokenized into qgrams (with q=3)\n",
    "jac.get_raw_score(qg3_tok_set.tokenize(x), qg3_tok_set.tokenize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev.get_raw_score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391304347826086"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get normalized Levenshtein similarity score between x and y\n",
    "lev.get_sim_score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get normalized Jaccard similarity score (this is the same as the raw score)\n",
    "jac.get_sim_score(ws_tok_set.tokenize(x), ws_tok_set.tokenize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
