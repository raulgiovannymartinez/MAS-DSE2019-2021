[
 {
  "title": "Four short links: 21 August 2017",
  "content": "Cloud Operations, Machine Learning Radio, Flying Cameras, and Text Organization\n\nParacloud: Bringing Application Insight into Cloud Operations -- In this work, we propose a uniform Paracloud interface (PaCI) to enable a bi-directional communication channel between application containers and the cloud management substrate.  An application knows how it's doing, which it reports through this interface so the cloud management layer can figure how/when to migrate, scale, load balance. (via A Paper A Day)\n\nDARPA Wants Machine Learning for Radio Signals -- An RFMLS would be able to discern subtle differences in the RF signals among identical, mass-manufactured IoT devices and identify signals intended to spoof or hack into these devices. \u201cWe want to ... stand up an RF forensics capability to identify unique and peculiar signals amongst the proverbial cocktail party of signals out there,\u201d Tilghman said.\n\n\nXPose: Reinventing User Interaction with Flying Cameras -- clever!  Drone takes a bunch of shots in a pre-programmed path, then the operators says which shot they like the best, and the drone returns to that location so the operator can refine the framing and focus for a final shot. Good to see the drone being servant to the photo, which is the thing the user cares about.\n\nTree Sheets -- freeform text organizer.  Nice!\n\nContinue reading Four short links: 21 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/OkCp26ZAQCs/four-short-links-21-august-2017"
 },
 {
  "title": "6 practical guidelines for implementing conversational AI",
  "content": "How organizations can create more fluid interactions between humans and machines.It has been seven years since Apple unveiled Siri, and three since Jeff Bezos, inspired by Star Trek, introduced Alexa. But the idea of conversational interfaces powered by artificial intelligence has been around for decades. In 1966, MIT professor Joseph Weizenbaum introduced ELIZA\u2014generally seen as the prototype for today\u2019s conversational AI.\n\nDecades later, in a WIRED story, Andrew Leonard proclaimed that \u201cBots are hot,\u201d further speculating they would soon be able to \u201cfind me the best price on that CD, get flowers for my mom [and] keep me posted on the latest developments in Mozambique.\u201d Only the reference to CDs reveals that the story was written in 1996.\n\nToday, companies such as Slack, Starbucks, Mastercard, and Macy\u2019s are piloting and using conversational interfaces for everything from customer service to controlling a connected home to, well, ordering flowers for mom. And if you doubt the value or longevity of this technology, consider that Gartner predicts that by 2019, virtual personal assistants \u201cwill have changed the way users interact with devices and become universally accepted as part of everyday life.\u201d\n\nNot all conversational AI is created equal, nor should it be. Conversational AI can come in the form of virtual personal (Alexa, Siri, Cortana, Google Home) or professional (such as X.ai or Skipflag) assistants. They can be built on top of a rules engine or based on machine learning technology. Use cases range from the minute and specific (Taco Bell\u2019s TacoBot) to the general and hypothetically infinite (Alexa, Siri, Cortana, Google Home).\n\nOrganizations considering implementing conversational interfaces, whether for personal or professional uses, typically rely on partners for the technology stack, but there are plenty of considerations that span beyond the technical. While it\u2019s too early in the game to call them \u201cbest practices,\u201d here are some guidelines for organizations considering piloting and/or implementing conversational AI:\n\n1. Start with a clear and focused use case\n\n\u201cThe focus as a product or brand,\u201d says Amir Shevat, director of developer relations at Slack, \u201cshould not be to think, \u2018I am building a bot;\u2019 it should be to think: \u2018What is the service I want to deliver?\u2019\u201d Beyond that, say Shevat and others, the best place to start is with a thorny problem that could conceivably be mitigated or solved with data\u2014a lot of it. This doesn\u2019t mean all successful bots should do only one thing, but it\u2019s critical to start with a narrow domain and a clear answer set, and design for an experience in which users typically don\u2019t know what they can ask and what they cannot.\n\n2. The objective determines the interaction model\n\nSome conversations lend themselves well to spoken interactions -- for example, in a car or turning on the heater in your home. Others, such as asking for a bank balance, may require the privacy and/or precision of text input. But there are other ways to help users interact with bots. Figure 1 illustrates two successful interaction examples.\n\n\nFigure 1. Screenshots by Susan Etlinger.\n\n\n\u201cMany people still have a misperception of bots just as speaking or typing,\u201d says Chris Mullins of Microsoft. In fact, there are various ways (or modalities) that bots can use to interact with and convey information:\n\n\n\tSpeech (Alexa, Siri, Google Home)\n\tTyping (bots in messaging apps)\n\tKeyboard support that provides cues that narrow the range of input options\n\tCards that display the information visually\n\n\n\u201cIn most successful scenarios,\u201d says Mullins, \u201cwe see a mix of modalities win. At the right time, spoken language is perfect. At another time, typing may be perfect. Sometimes you want cards or keyboard support. Conversational modeling is a phenomenally hard problem, and no one has gotten it quite right yet.\u201d\n\n3. Multiple contexts require careful planning and clear choices\n\nIf a customer asks a retailer a question such as, \u201cWhere can I find a power drill in the store near me?\u201d a developer must consider the context of that question based on the customer\u2019s location. Is she physically in the store? Is she on her phone, or on her computer at home? Developers must design for multiple scenarios and experiences.\n\nThis process is challenging because it requires different interaction models during the scoping process. \u201cInteracting with humans is complicated, and conversational modeling is hard,\u201d says Mullins. To do the best job, project teams have to make choices at the beginning of the project.\n\n4. Sustained interactions require sustained contextual understanding\n\nThere is a difference between giving a single command, such as \u201cPlay Beyonc\u00e9\u2019s Lemonade,\u201d or \u201cCheck my bank balance,\u201d and engineering for a sustained interaction between a human and a chatbot. This is why conversations that require multiple exchanges (\u201cturns\u201d) between human and bot, and which require context to understand, are complex and hard to engineer.\n\nThe example in Figure 2 from Kasisto illustrates the complexity of even a relatively simple payment interaction:\n\n\nFigure 2. Screenshot by Susan Etlinger.\n\n\nTurn 1:\n\n\n\tThe user asks Kai (the chatbot) to pay Emily $5.00.\n\tKai finds two people named Emily in the user\u2019s contact list and asks which one she means.\n\n\nTurn 2:\n\n\n\tThe user then switches gears and asks how much she has in checking.\n\tKai answers, and then says, \u201cNow, where were we?\u201d picking up from the original request to pay Emily $5.00.\n\n\nAt first glance, this looks like a fairly simple transaction, but from an engineering perspective, it requires quite a bit of language and contextual understanding:\n\n\n\tFirst, Kai must recognize and keep track of the user\u2019s goal \u2014 she wants to pay someone.\n\tSecond, Kai must identify who she wants to pay, realize that the user has two friends named Emily, and request clarification on which one is to receive the payment.\n\tThird, Kai must understand that the single word \u201cNeubig\u201d refers to the previous interaction and represents a request to pay Emily Neubig.\n\tFourth, Kai must interpret \u201cWhat do I have in my checking?\u201d as a new request unrelated to the previous two interactions (out of context).\n\tFinally, it must answer this new request, pick up the thread, and fulfill the original request, which is to pay Emily $5.00.\n\n\nThis conversation demonstrates why clear purpose, a narrow answer set, and deep domain expertise are important in chatbot development \u2014 because understanding the user\u2019s intent when it is expressed naturally is highly complex and critical to delivering an effective experience.\n\n5. EQ is as important as IQ\n\nIntelligence and clear intention aren\u2019t the only attributes required for successful bots. Detecting emotion, expressed in word choice and tone, is also critical to ensure that conversational experiences are satisfying for users. As a result, many research labs and startups are working on software intended to detect emotional states, whether via images, speech, text, or video.\n\nSRI International\u2019s Speech Technology and Research (STAR) laboratory has developed SenSay Analytics, a platform that claims to sense speaker emotions from audio signals. This is key to understanding when a user is frustrated and may need to be escalated to a human agent or in a receptive frame to hear about relevant offers.\n\n6. Branding opportunities are tiny but potent.\n\nBranding is a critical aspect of the bot\u2019s (and brand\u2019s) success. A poorly executed bot can damage reputation, while a strong brand presence can support the success of the bot. \u201cI would argue that the branding opportunity you have in a conversational interface is relatively small,\u201d says Lars Trieloff of Adobe. \u201cSo, leverage the brand in the daily interaction. Make sure it can do one thing really well and any way the customer likes to invoke it.\u201d\n\nWe are in the very earliest stages of using conversational interfaces in a meaningful way, and there is still a long way to go. If you\u2019ve ever spent any time revisiting some of these early web sites, you have an indication of where we are now.\n\nBut conversational AI\u2014the ability for machines to interact with us in more humanistic ways\u2014is here to stay. It may look primitive today, but advances in data science, natural language technology, machine learning, and other disciplines have finally created the necessary conditions for a shift from form-based to more fluid communications between humans and machines.\n\nWill conversational interactions ever be equal or better than human ones? Some types of conversations will never lend themselves easily to machine interactions. But for some uses, they probably will; we\u2019ve already seen massive innovation, and we\u2019ve barely scratched the surface. One thing is clear, says futurist and creative strategist Monika Bielskyte: \u201cWe are entering a future without screens. In the future,\u201d she predicts, \u201cthe world will be our desktop.\u201d\n\nContinue reading 6 practical guidelines for implementing conversational AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/6dI3_u6MNLA/6-practical-guidelines-for-implementing-conversational-ai"
 },
 {
  "title": "Four short links: 18 August 2017",
  "content": "Neural Style Transfer, Hype Cycles, Automation and Jobs, and Become a Bayesian\n\nNeural Style Transfer -- overview of the state of the art in recasting an image to have the style of another.\n\nEight Lessons from 20 Years of Hype Cycles -- Out of the more than 200 unique technologies that have ever appeared on a Gartner Hype Cycle for Emerging Technology, just a handful of technologies\u2014Cloud Computing, 3D Printing, Natural Language Search, Electronic Ink\u2014have been identified early and traveled even somewhat predictably through a Hype Cycle from start to finish. [...] [J]ust over 50 individual technologies appear for just a single year on the Hype Cycle\u2014never to reappear again. [...]  20% of all technologies that were tracked for multiple years on the Hype Cycle became obsolete before reaching any kind of mainstream success.  [...] I was often struck by how many times the Hype Cycle had an insight that was essentially correct, but the technology or the market just wasn\u2019t ready yet. [...] There are a number of core technologies that appear again and again in different guises over the years in Hype Cycles, sometimes under multiple aliases. Each reincarnation makes progress and leaves lessons for its successors without really breaking through. [...] It's remarkable the number of major technologies from the last 20 years that were either identified late or simply never appeared on a Hype Cycle.\n\n\nRobopocalypse Not (James Surowiecki) -- A rigorous study of the impact of robots in manufacturing, agriculture, and utilities across 17 countries, for instance, found that robots did reduce the hours of lower-skilled workers\u2014but they didn\u2019t decrease the total hours worked by humans, and they actually boosted wages. In other words, automation may affect the kind of work humans do, but at the moment, it\u2019s hard to see that it\u2019s leading to a world without work. McAfee, in fact, says of his earlier public statements, \u201cIf I had to do it over again, I would put more emphasis on the way technology leads to structural changes in the economy, and less on jobs, jobs, jobs. The central phenomenon is not net job loss. It\u2019s the shift in the kinds of jobs that are available.\u201d\n\n\nBecome a Bayesian in Eight Easy Steps: An Annotated Reading List -- The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. [...] Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment.  After consulting our guide and the outlined articles, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.\n\n\nContinue reading Four short links: 18 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/DbVctbF3HL4/four-short-links-18-august-2017"
 },
 {
  "title": "How Ray makes continuous learning accessible and easy to scale",
  "content": "The O\u2019Reilly Data Show Podcast: Robert Nishihara and Philipp Moritz on a new framework for reinforcement learning and AI applications.In this episode of the Data Show, I spoke with Robert Nishihara and Philipp Moritz, graduate students at UC Berkeley and members of RISE Lab. I wanted to get an update on Ray, an open source distributed execution framework that makes it easy for machine learning engineers and data scientists to scale reinforcement learning and other related continuous learning algorithms. Many AI applications involve an agent (for example a robot or a self-driving car) interacting with an environment. In such a scenario, an agent will need to continuously learn the right course of action to take for a specific state of the environment. Continue reading How Ray makes continuous learning accessible and easy to scale.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/fBvrLyrztog/how-ray-makes-continuous-learning-accessible-and-easy-to-scale"
 },
 {
  "title": "Julie Stanford on vetting designs through rapid experimentation",
  "content": "The O\u2019Reilly Design Podcast: Quickly test ideas like a design thinker.In this week\u2019s Design Podcast, I sit down with Julie Stanford, founder and principal of user experience agency Sliced Bread Design. We talk about how to get in the rapid experimentation mindset, the design thinking process, and how to get started with rapid experimentation at your company. Hint: start small.Continue reading Julie Stanford on vetting designs through rapid experimentation.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/73MUJchg6Dg/julie-stanford-on-vetting-designs-through-rapid-experimentation"
 },
 {
  "title": "Jack Daniel on building community and historical context in InfoSec",
  "content": "The O'Reilly Security Podcast: The role of community, the proliferation of BSides and other InfoSec community events, and celebrating our heroes and heroines.In this episode of the Security Podcast, I talk with Jack Daniel, co-founder of Security Bsides. We discuss how each of us (and the industry as a whole) benefits from community building, the importance of historical context, and the inimitable Becky Bace.Continue reading Jack Daniel on building community and historical context in InfoSec.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/q6fcZdfLp8I/jack-daniel-on-building-community-and-historical-context-in-infosec"
 },
 {
  "title": "Four short links: 17 August 2017",
  "content": "Implementing Compression, Eliminating Humans, Mapping NES, and Classifying Defects\n\nOn the Implementation of Minimum Redundancy Prefix Codes -- a paper that shows all the curls and challenges of the real world, instead of the idealized handwaving that's pretty much everywhere else. (via Fabian Giesen)\n\nEliminating the Human (MIT TR) -- David Byrne digs into the idea that modern tech is mostly about reducing need for human interaction. When interaction becomes a strange and unfamiliar thing, then we will have changed who and what we are as a species. Often our rational thinking convinces us that much of our interaction can be reduced to a series of logical decisions\u2014but we are not even aware of many of the layers and subtleties of those interactions. As behavioral economists will tell us, we don\u2019t behave rationally, even though we think we do. And Bayesians will tell us that interaction is how we revise our picture of what is going on and what will happen next. (via BoingBoing)\n\nAutomatic Mapping of NES Games with Mappy -- We describe a software system, Mappy, that produces a good approximation of a linked map of rooms given a Nintendo Entertainment System game program and a sequence of button inputs exploring its world. In addition to visual maps, Mappy outputs grids of tiles (and how they change over time), positions of non-tile objects, clusters of similar rooms that might in fact be the same room, and a set of links between these rooms. We believe this is a necessary step toward developing larger corpora of high-quality semantically annotated maps for PCG via machine learning and other applications.\n\n\nIBM's Defect Classification System -- for when you get devopsessive. (via Ryan Betts)\n\nContinue reading Four short links: 17 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/w5rks9LsSQA/four-short-links-17-august-2017"
 },
 {
  "title": "Contouring learning rate to optimize neural nets",
  "content": "Tips and tricks for treating learning rate as a hyperparameter, and using visualizations to see what\u2019s really going on.Learning rate is the rate at which the accumulation of information in a neural network progresses over time. The learning rate determines how quickly (and whether at all) the network reaches the optimum, most conducive location in the network for the specific output desired. In plain Stochastic Gradient Descent (SGD), the learning rate is not related to the shape of the error gradient because a global learning rate is used, which is independent of the error gradient.\n\nHowever, there are many modifications that can be made to the original SGD update rule that relates the learning rate to the magnitude and orientation of the error gradient.\n\nWhy contour the learning rate?\n\nContouring the learning rate over time is similar to contouring the speed of a car according to road conditions. On smooth, broad roads such as a highway, we can increase our speed (learning rate), but on narrow, hilly, or valley roads, we must slow down. Additionally, we don\u2019t want to drive too slowly on highways, or we\u2019ll take too long to reach the destination (longer training time because of improper parameters). Similarly, we don\u2019t want to drive too fast on hilly and narrow roads (like ravines in the optimization loss surface), because we could easily lose control of the car (be caught in jitter, or create too much bounce with little improvement) or skip the destination (the optima).\n\nKeep in mind that \u201c...a high learning rate... [indicates] the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into the deeper, but narrower parts of the loss function\u201d (see Karpathy\u2019s Notes for cs231n).\n\nFrom the same source, a good guesstimate for an initial learning rate can be obtained by training the network on a subset of the data. The ideal strategy is to start with a large learning rate and divide by half until the loss does not diverge further. When approaching the end of training, the decay of the learning rate should be approximately 100 or more. Such decay makes the learned network resistant to random fluctuations that could possibly reverse the learning. We\u2019ll start with a small LR, test out on a small set of data, and choose the appropriate value.\n\nLearning rate decay\n\nNon-adaptive learning rates can be suboptimal. Learning rate decay can be brought about through reducing decay by some constant factor every few epochs, or by exponential decay, in which the decay takes a mathematical form of the exponential every few epochs. \u201cDecay\u201d is often considered a negative concept, and in the current case of learning rate decay it\u2019s a negative, too: it refers to how much the learning rate is decreasing. However, the result of this kind of decay is actually something we very much want. In a car, for instance, we decrease the speed to suit the road and traffic conditions; this deacceleration can be understood as a \u201cdecay\u201d in the velocity of the car. Similarly, we gain benefits from decaying the learning rate to suit the gradient.\n\nDecreasing the learning rate is necessary because a high learning rate while proceeding into the training iterations has a high probability to fall into a local minima. Think of a local minima as a speeding ticket, a toll or traffic light, or traffic congestion\u2014something that basically increases the time taken to reach the destination. It\u2019s not possible to completely avoid all traffic lights and tolls, but there is an optimal route that we prefer while driving. Similarly, in training we want to avoid the zig-zag bouncing of the gradient while looking for the optimal route and prefer training on that path. Ideally, we don\u2019t want to speed up too much because we\u2019ll get a speeding ticket (jump into a local minima and get stuck). The same analogy applies to learning rates.\n\nMomentum is an adaptive learning-rate method parameter that allows higher velocity to collect along shallow directions, and lower velocity along steep directions. This definition of momentum is considered \u201cclassical momentum,\u201d in which a correction is applied to the velocity and then a big jump is taken in the direction of the velocity. Momentum helps to accelerate or decelerate the base learning rate with respect to the changing gradient, causing a change in the net speed of the learning rather than its location on the loss surface. Momentum makes the learned network more resistant to noise and randomness in the inputs.\n\nOther update rules that treat learning rate as a hyperparameter include:\n\n\n\t\nThe AdaGrad update by Duchi et al., 2011, adds an element-wise scaling of the gradient based on the historical sum of squares in each dimension.\n\t\nThe RMSProp adaptive learning rate by Tieleman and Hinton, 2012, keeps a moving average of the squared gradient for each weight to normalize the current gradient. RMSProp adds more resistance to fluctuations and random noise.\n\t\nThe Adam update by Kingma and Ba, 2014, introduces a bias correction compensating for the zero initialization.\n\tAnd rprop, which uses only use the sign of the gradient to adapt the step size separately for each weight. This should not be used on mini-batches.\n\n\nApart from these rules, there are the very computation-heavy second order methods that follow Newton\u2019s update rule. The second order methods do not treat learning rate as a hyperparameter, however; due to their high computational demands, they are rarely used in large-scale deep learning systems.\n\nFigure 1 shows the comparison of different optimization techniques under similar hyperparameter settings:\n\n\nFigure 1. A comparison of optimization techniques. Source: Alec Radford, used with permission.\n\n\nIn this image, the momentum update overshoots the target, but reaches the overall minimum faster. \u201cNAG\u201d is the Nesterov Accelerated Gradient, in which the first step is taken in the direction of the velocity and then a correction is made to the velocity vector based on the new location.\n\nEssentially, we aim not to fall into decay, but to use the decay to fall into the right place. It is imperative to selectively increase or decrease learning rate as training progresses in order to reach the global optimum or the desired destination. Don\u2019t be afraid of this because we often have to do it again and again.\n\nVisualizations\n\nVisualizations are necessary to know how the learning is progressing. A loss versus epochs plot, for instance, is very useful to understand how the loss is changing with the epochs. An epoch is completed when all data points have been seen at least once in the current run. It\u2019s preferable to track epochs in comparison to iterations, because the number of iterations depends on the arbitrary setting of batch size.\n\nA good way to generate this plot is by overlaying the loss per epoch curve for different sets of parameters. This process helps us to recognize the set of parameters that works best for the training at hand. These plots have loss along the y axis and number of epochs along the x axis. Overall, the loss curves shown in Figure 2 look similar, but there are slight differences in their optimization patterns, represented in the number of epochs necessary for convergence and the resultant error.\n\nThere are many kinds of loss functions, and selecting which one to use is an important step. For some classification tasks, the cross entropy error tends to be more suitable than other metrics, such as the mean squared error, because of the mathematical assumptions behind the cross entropy error. If we view neural networks as probabilistic models, then, cross entropy becomes an intuitive cost function with its sigmoid or softmax nonlinearity which maximizes the likelihood of classifying the input data correctly. The mean square error, on the other hand, focuses more on the incorrectly labeled data. The mean classification error thus becomes a crude yardstick.\n\nCross entropy\u2019s niceties include a log term, making it more granular, while taking into account the closeness of the predicted value with the target. Cross entropy also has nicer partial derivatives producing larger errors, which lead to larger gradients that ensure quicker learning. Generally, the cost function should be picked based on the assumptions that match the output units and the probabilistic modelling assumptions. For instance, a softmax and cross entropy serve best for multiclass classification. Plotting the cross entropy function might be more interpretable due to the log term simply because the learning process is mostly an exponential process taking the form of an exponential shape.\n\nExperimenting with different learning rates\n\nLearning rate is a hyperparameter that controls the size of update steps. As the learning rate increases, the number of oscillations increase. As seen in the plots of Figure 2, there is a lot of confusion or random noise with an increase in the learning rate. All the plots in Figure 2 are for a one-layer neural network trained on the MNIST data set.\n\nWe can infer from the plots that a high learning rate is more likely to blow up the whole model, resulting in numerical instability such as overflows or underflows, which was also empirically noted while running these experiments. In fact, NAN\u2019s started appearing just after the first training epoch.\n\n\n\n\t\n\t\t\n\t\t\tLearning Rate\n\t\t\tCross Entropy\n\t\t\tClassification Error\n\t\t\n\t\t\n\t\t\t1\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t.5\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t.2\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t.1\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t.01\n\t\t\t\n\t\t\t\n\t\t\n\t\n\n\nFigure 2. Loss per epoch for different learning rates. Source: Siddha Ganju.\n\n\nFigure 2 shows the loss per epoch curve for different learning rates. Two different loss functions, the cross entropy and the classification error, are compared. The plots show that at lower learning rates, the improvements are linear (Learning rate .01). When the learning rates are high, almost exponential jumps are seen (Learning rate 1 and .5). The higher learning rates are capable of decaying the loss much faster, but the disadvantage is that a big jump might land them in a local minima, and then get stuck at worse values of loss.\n\nThis phenomenon is often seen as oscillations in the plots, indicating that the parameters that were learned are mostly bouncing around, and not able to settle and make the gradient move towards the global min. If the validation curve closely follows the training curve, the network has trained correctly. However, large gaps between the validation and training curves indicate that the network is overfitting on the training set (Learning rate .01). Chances of overfitting can be reduced by using dropout or other regularization techniques.\n\nUse a separate, adaptive learning rate for each connection\n\nIt is common to have networks with more than one layer. Each layer has a different fan-in or the number of inputs, that determines the overshoot effect caused by simultaneously updating the incoming weights of a unit aimed at correcting the same error (as is the case in the global learning rate). Additionally, the magnitudes of the gradients vary for different layers, especially if the initial weights are small (always the case for initialization). Hence, the appropriate learning rates can vary widely between weights.\n\nA fix for this issue involves setting a global learning rate and multiplying it by an appropriate local gain that is determined empirically for each weight. In order to enhance performance of the network, the gains should lie in a reasonable predefined range. While using separate adaptive learning rates, either the entire batch should be used for learning, or larger mini-batches should be used in order to ensure the changes in the signs of the gradients are not mainly due to the sampling error of a mini-batch or random fluctuations. These per-weight adaptive learning rates can also be combined with momentum. (For more information, see Lecture 6b, Neural Networks, Coursera.)\n\nLearning rate for transfer learning\n\nTransfer learning is modifying an existing pretrained model for use in another application. This reuse of models is necessary because it is relatively rare for a sizable data set for training purposes to exist in the application domain. Fine-tuning is a type of transfer learning in which a slice of the network, like the last layers of a network are modified to give the application-specific number of outputs. There are other types of transfer learning methods which we do not discuss here. As Figure 3 demonstrates, fine-tuning modifies a network to learn a slightly different kind of data (such as the accordion) when the network has already been trained on a similar kind of data (the piano).\n\n\nFigure 3. The accordion fine-tunes the piano knowledge acquired previously. Slide from Anirudh Koul, Squeezing Deep Learning Into Mobile Phones. Used with permission.\n\n\nAs the network is pretrained, fine-tuning takes considerably less time to train, because the network has already acquired most of the information it needs and must only refine that knowledge during the fine-tuning phase.\n\nWhile fine-tuning, we decrease the overall learning rate while boosting the learning rate for the slice (last layer or the new layer). For example, in the open source Caffe framework, the base_lr should be decreased in the solver prototxt, while the lr_mult for the newly introduced layer should be increased. This helps to bring about a slow change in the overall model but an expeditious change in the new layer utilizing the new data. The rule of thumb is to keep the learning rate for the layer being learned at least 10 times more than the other static layers (the global learning rate).\n\nConclusion\n\nIn this article, we only briefly touched on a few things out of all the parameters in the parametric-multiverse of deep learning algorithms. The decay of learning rate is one such parameter, and is essential to avoid taxing local minimas. While performing your deep learning experiments, remember to make exhaustive use of visualizations. They are one of the most important ways to understand what\u2019s really happening inside a deep learning black box.\n\nImagine deep learning as full of little knobs and switches, like a pilot's dashboard. We need to learn how to tweak these to get the best outputs for the work at hand. We can always find a pre-trained model for the application we are developing, selectively add or delete parts of it, and ultimately fine-tune it for the application we need. And, if we contour our learning rate accurately, we reach our destination in good time.\nContinue reading Contouring learning rate to optimize neural nets.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/cVBfcrxpIH0/contouring-learning-rate-to-optimize-neural-nets"
 },
 {
  "title": "Creating better disaster recovery plans",
  "content": "Five questions for Tanya Reilly: How service interdependencies make recovery harder and why it\u2019s a good idea to deliberately and preemptively manage dependencies.I recently asked Tanya Reilly, Site Reliability Engineer at Google, to share her thoughts on how to make better disaster recovery plans. Tanya is presenting a session titled Have you tried turning it off and turning it on again? at the O\u2019Reilly Velocity Conference, taking place Oct. 1-4 in New York.\n\n1. What are the most common mistakes people make when planning their backup systems strategy?\n\nThe classic line is \"you don't need a backup strategy, you need a restore strategy.\" If you have backups, but you haven't tested restoring them, you don't really have backups. Testing doesn't just mean knowing you can get the data back; it means knowing how to put it back into the database, how to handle incremental changes, how to reinstall the whole thing if you need to. It means being sure that your recovery path doesn't rely on some system that could be lost at the same time as the data.\n\nBut testing restores is tedious. It's the sort of thing that people will cut corners on if they're busy. It's worth taking the time to make it as simple and painless and automated as possible; never rely on human willpower for anything! At the same time, you have to be sure that the people involved know what to do, so it's good to plan regular wide-scale disaster tests. Recovery exercises are a great way to find out that the documentation for the process is missing or out of date, or that you don't have enough resources (disk, network, etc.) to transfer and reinsert the data.\n\n2. What are the most common challenges in creating a disaster recovery (DR) plan?\n\nI think a lot of DR is an afterthought: \"We have this great system, and our business relies on it ... I guess we should do DR for it?\" And by that point, the system is extremely complex, full of interdependencies and hard to duplicate.\n\nThe first time something is installed, it's often hand-crafted by a human who is tweaking things and getting it right, and sometimes that's the version that sticks around. When you build the second one, it's hard to be sure it's exactly the same. Even in sites with serious config management, you can leave something out, or let it get out of date.\n\nEncrypted backups aren't much use if you've lost access to the decryption key, for example. And any parts that are only used in a disaster may have bit-rotted since you last checked in on them. The only way to be sure you've covered everything is to fail over in earnest. Plan your disaster for a time when you're ready for it!\n\nIt's better if you can design the system so that the disaster recovery modes are part of normal operation. If your service is designed from the start to be replicated, adding more replicas is a regular operation and probably automated. There are no new pathways; it's just a capacity problem. But there can still be some forgotten components of the system that only run in one or two places. An occasional scheduled fake disaster is good for shaking those out.\n\nBy the way, those forgotten components could include information that's only in one person's brain, so if you find yourself saying, \"We can't do our DR failover test until X is back from vacation,\" then that person is a dangerous single point of failure.\n\nParts of the system that are only used in disasters need the most testing, or they'll fail you when you need them. The fewer of those you have, the safer you are and the less toilsome testing you have to do.\n\n3. Why do service interdependencies make recovery harder after a disaster?\n\nIf you've got just one binary, then recovering it is relatively easy: you start that binary back up. But we increasingly break out common functionality into separate services. Microservices mean we have more flexibility and less reinvention of wheels: if we need a backend to do something and one already exists, great, we can just use that. But someone needs to keep a big picture of what depends on what, because it can get very tangled very fast.\n\nYou may know what backends you use directly, but you might not notice when new ones are added into libraries you use. You might depend on something that also indirectly depends on you. After an outage, you can end up with a deadlock: two systems that each can't start until the other is running and providing some functionality. It's a hard situation to recover from!\n\nYou can even end up with things that indirectly depend on themselves\u2014for example, a device that you need to configure to bring up the network, but you can't get to it while the network is down. Often people have thought about these circular dependencies in advance and have some sort of fallback plan, but those are inherently the road less traveled: they're only intended to be used in extreme cases, and they follow a different path through your systems or processes or code. This means they're more likely to have a bug that won't be uncovered until you really, really need them to work.\n\n4. You advise people to start deliberately managing their dependencies long before they think they need to in order to ward off potentially catastrophic system failure. Why is this important and what\u2019s your advice for doing it effectively?\n\nManaging your dependencies is essential for being sure you can recover from a disaster. It makes operating the systems easier too. If your dependencies aren't reliable, you can't be reliable, so you need to know what they are.\n\nIt's possible to start managing dependencies after they've become chaotic, but it's much, much easier if you start early. You can set policies on the use of various services\u2014for example, you must be this high in the stack to depend on this set of systems. You can introduce a culture of thinking about dependencies by making it a regular part of design document review. But bear in mind that lists of dependencies will quickly become stale; it's best if you have programmatic dependency discovery, and even dependency enforcement. My Velocity talk covers more about how we do that.\n\nThe other advantage of starting early is that you can split up your services into vertical \"strata,\" where the functionality in each stratum must be able to come completely online before the next one begins. So, for example, you could say that the network has to be able to completely start up without using any other services. Then, say, your storage systems should depend on nothing but the network, the application backends should only depend on network and storage, and so on. Different strata will make sense for different architectures.\n\nIf you plan this in advance, it's much easier for new services to choose dependencies. Each one should only depend on services lower in the stack. You can still end up with cycles\u2014things in the same stratum depending on each other\u2014but they're more tightly contained and easier to deal with on a case-by-case basis.\n\n5. What other parts of the program for Velocity NY are of interest to you?\n\nI've got my whole Tuesday and Wednesday schedule completely worked out! As you might have gathered, I care a lot about making huge interdependent systems manageable, so I'm looking forward to hearing Carin Meier's thoughts on managing system complexity, Sarah Wells on microservices and Baron Schwartz on observability. I'm fascinated to hear Jon Moore's story on how Comcast went from yearly release cycles to releasing daily. And as an ex-sysadmin, I'm looking forward to hearing where Bryan Liles sees that role going.\nContinue reading Creating better disaster recovery plans.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/tPT1Okv_Djk/creating-better-disaster-recovery-plans"
 },
 {
  "title": "Announcing the Rebecca Bace Pioneer Award for Defensive Security",
  "content": "Carrying on Becky Bace\u2019s legacy of encouraging and celebrating defenders.In her keynote at O\u2019Reilly Security Conference in New York 2016, Rebecca \u201cBecky\u201d Bace shared her vision for the future of cybersecurity. Within her talk, Becky encouraged defenders to study analogous safety industries in history and heed the lessons learned. She urged defenders to recognize the importance of understanding the context of the challenges we face today by considering historical parallels and our shared goals for the future. With more than three decades in what is an arguably young discipline, Becky was incredibly well qualified to provide this larger context. Unfortunately, only a few months after sharing this message, Becky Bace passed away.\n\nWe\u2019re excited and honored to announce a new Defender Award, commemorating Becky, her contributions, and her inimitable spirit\u2014The Rebecca Bace Pioneer Award. This annual award will recognize a defender who forged new paths in defensive security. We can think of no greater way to celebrate Becky and her role within the security community than by continuing her efforts to boost, support, and celebrate others within the space. The winner of the first annual Rebecca Bace Pioneer Award will be announced at the O\u2019Reilly Security Conference in New York. Please help us celebrate Becky by nominating worthy defenders for the first annual Rebecca Bace Pioneer Award here.\n\nWe created the O\u2019Reilly Defender Awards to celebrate our security heroes and heroines. And we can think of few as deserving of these honors as Becky Bace. Her technological contributions and her long-standing efforts to build, support, and boost the security community and its individual members make her a true pioneer in defensive security.\n\nBecky had more than 35 years of experience in technology and more than 30 years were focused specifically on security. It\u2019s impossible to fit her extensive technological achievements into a short paragraph, but here\u2019s a brief summary of just a few of her efforts. She was an early leader in exploring and documenting intrusion detection techniques, an influencer and founder in the earliest government forays into cybersecurity, including spending 12 years at the US National Security Agency leading the Computer Misuse and Anomaly Detection (CMAD) research program. As chief strategist at the Center for Forensics, Information Technology, and Security (CFITS) at the University of South Alabama, she developed a highly respected academic program in her home state.\n\nBeyond her considerable technological achievements and contributions, Becky was known for her role as a mentor, an encourager, and a fierce friend within the security community. She was affectionately called the \u201cden mother of computer security\u201d or \u201cinfomom,\u201d though as Jack Daniel tells us in the latest O\u2019Reilly Security podcast, she preferred the moniker, \u201ccranky broad.\u201d\n\nIn the words of others:\n\nWhen I think about Becky Bace, I remember her warm and friendly demeanor, her charming voice, and her welcoming hugs. Many of us knew her as \"infomom,\" the mother bear nurturing the information security community. Her professional accomplishments as a pioneer in cybersecurity research were numerous and tremendously impactful. She literally wrote the book on Intrusion Detection and led the Computer Misuse and Anomaly Detection Program at the NSA. She moved onto Los Alamos National Laboratory and then onto a number of private sector positions. Her favorite role was that of mentor and teacher. There's nothing like reading her story as told through her own words. Her oral history is preserved and can be read here. Becky Bace has left a lasting impression on our community and we will always remember her. \u2014 Caroline Wong, Vice President of Security Strategy, Cobalt\n\nBecky was known as the den mother of IDS (Intrusion Detection Systems), for her work fostering and supporting intrusion detection and network behavior analysis. But even beyond her amazing technical expertise and contributions, Becky gave the best hugs in the world. She was just an amazingly warm, friendly, and welcoming person. One of the things that always struck me about Becky is the number of people she mentored through the years, and the number of people whose careers got a start or a boost because of Becky. She was just pure awesome. She would go out of her way to help people, and the more they needed help, the more likely she would be to find them and help them. \u2014 Jack Daniel, Co-founder of Security, BSides\nContinue reading Announcing the Rebecca Bace Pioneer Award for Defensive Security.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0zsQPJCsEiI/announcing-the-rebecca-bace-pioneer-award-for-defensive-security"
 },
 {
  "title": "How synthetic biology startups are building the future at RebelBio",
  "content": "The accelerator is helping scientists turn their moonshot visions into viable businesses, fast.\n\n\n\n\nBiology is becoming the new digital. As the engineering of biology starts delivering solutions at industrial scale while becoming more data-driven and automated, investors are getting excited to fund breakthrough biotechnologies that unlock this potential for a whole range of industries.\nEurope has been steadily rising as a global hub for innovation and is attracting big bets from veteran VC investors. It has even been hailed as the \u201cNew Silicon Valley.\u201d Boasting an active DIY biology scene, world-class institutions and productive innovation ecosystems, the continent is a fertile base to grow global startups from the early stage to the next level.\n\nAccelerators are indispensable elements to harness this early-stage innovation, which often remains locked up in institutions or falls into the \u201cValley of Death,\u201d the often deadly funding gap between those first discoveries and a working prototype.\n\nThis is the stage the RebelBio accelerator comes in to help startups solve global grand challenges\u2014with life itself.\n\n\nAn Innovation Engine\n\nIt\u2019s as exciting in biology today as it was in the computer industry in the late '70s, when the Apple II came out. Computers became personal.\n\nAs Steve Jobs said, \u201cI think the biggest innovations of the 21st century will be at the intersection of biology and technology,\u201d the ethos of RebelBio is to bend and break the rules of the status quo at the intersection between these two disciplines.\n\nA total of 15 multidisciplinary teams from across the world have begun the latest program at RebelBio, garnering an investment of over $100,000 for each company. In addition to gaining access to fully equipped labs and office spaces, they also draw from a network of hundreds of mentors, including RebelBio-founder Bill Liao, who also cofounded Xing, Davnet, and CoderDojo.\n\nThe program helps the founders to make their longer-term moonshot visions (the \u201cinnovation\u201d part) into feasible projects which generate revenue early on (the \u201cengine\u201d part). It is transforming scientists into entrepreneurs across diverse areas of life sciences and is currently in its fourth batch.\n\n\n From novel biomaterials to new ways to brew the foods we love, from speeding up cancer lab tests turning days into hours, to a microbe-miner discovering life-saving antibiotics. We even have a machine learning startup for drug discovery, and another working on microbial fuel cell modules to treat wastewater while generating electricity!\n Bill Liao, founder of RebelBio and general partner at SOSV\n\n\nSo what kind of startups are brewing at RebelBio?\n\n\n \n Figure 1-1. The lab and some of the cohort IV at University College Cork (Image: RebelBio)\n \n\n\n\nDiagnostics 2.0: Rapid, Portable, and Personal\n\nOne RebelBio startup is Sex Positive, founded by Nico Bouchard and Mary Ward, cofounders of Counter Culture Labs and both biohackers from California. Developed as a smart diagnostic device for sexually transmitted infections, Sex Positive is enabling rapid self-testing of sexual health in the privacy of one\u2019s own home, without the need for a hospital lab. The first test will be for chlamydia. The device combines fluid dynamics, immunology, genetics and electronics designed by a Tesla engineer remotely in Palo Alto.\n\nMeanwhile, KaitekLabs is founded by Emilia Diaz from Chile, who is turning bacteria into living computers. They function as a ready-to-use, portable biosensor kit to detect shellfish toxin directly from shellfish\u2014a major food source for many people living in coastal regions around the world. It makes the invisible poison orange! The startup has already sold out a batch of the first prototype.\n\n\n \n Figure 1-2. KaitekLabs\u2019 founder Emilia with the \u201cMOSES\u201d (Microbial Optic Shellfish Evaluation Sensor) toxin-detection kit (Image: KaitekLabs)\n\n \n Then, OaCP (Oncology and Cytogenetic Products) is a university spin-out from Bologna, Italy, using its patented reagents to speed up diagnostics. Since many in vitro diagnostic tests for cancer can take over three days of agonized waiting, OaCP uses a novel reagent which enhances hybridization of nucleic acid probes. This reduces the diagnosis time to as little as two hours. As the applications are versatile, the reagent can also be used to accelerate bottlenecks in genome sequencing, genome editing, or liquid biopsies.\n \n \n \n Figure 1-3. Sex Positive\u2019s personal in vitro diagnostic test to detect sexually transmitted infections (Image: Sex Positive)\n\n\n\n\nDNA As the New Silicon\n\nAs biology is becoming an information technology, DNA emerges as the new silicon.\n\nFor instance, Helixworks Technologies is the first to offer storage of digital data in DNA. The startup is now developing a new product: a portable molecular machine (dubbed OpenMOSS) that converts digital data into DNA\u2014in your home.\n\nBy storing data completely offline in a medium as durable and dense as DNA, Helixworks could provide a biological solution to the rapidly expanding need for cost-effective, long-term data storage while addressing the rising threat of cybersecurity. Storing large or sensitive sets of data completely offline in a physical medium offers safety from cyberthreats. The startup recently won the \u201cMost Innovative\u201d award at pitch competition SXSW.\n\nTools that make it easier to program life are a running theme across the cohorts, with another example, Briefcase Biotec: it\u2019s building a DNA synthesizer called Kilobaser, \u201cthe Nespresso Machine of DNA synthesis\u201d: simply add a reagent-cartridge and enter your sequence! The machine will make oligos and primers quickly and cheaply directly on the lab bench.\n\nMoirai Biodesign is also adopting this modular approach, in their case to build programmable cancer therapeutics with \u201cPlug-and-Play RNA\u201d. The molecule consists of two domains, whereas the sensor part reacts to the presence of cancer-specific biomarkers, while the trigger part encodes a certain protein. Allowing for switch-like activation of the biodevice, specifically in cancer cells, this targeted therapeutic holds great promise to alleviate side-effects of cancer therapy in the future.\n\n \n \n Figure 1-4. Left, Conor Crosbie, Eshna Gogia, Sachin Chalapati, Nimesh Chandra, and feline buddy of the Helixworks team (Image: Sachin Chalapati); right, Kilobaser\u2019s new quality-control platform (Image: Briefcase Biotec)\n\n\n\n\n\nPersonal Maker Kits to Build with Biology\n\nTools like these could be used in conjunction with a bio-maker kit, like BentoLabs and AminoLabs\u2014or an entirely new kind. Hence, Cell-Free is breaking a billion-year-old processor out of the cell to enable anyone, anywhere to manufacture biomolecules with a cell-free machinery: a \u201cRaspberry Pi\u201d model for biology.\n\nIn coming years, point-of-care synthesis of products like insulin and vaccines could drastically improve the availability of medicines. For now, the startup is envisioning consumer biotech applications to enable anyone to make custom colors, smells, logic circuits\u2014even glow-in-the-dark ink! The founders aim to bring technology and biology closer together.\n\n\nBiological sensors, detectors and processors will be core to this. We are building the tools that will allow innovators from all backgrounds to engineer the materials of the future.\nDr. Thomas Meany, cofounder and CEO of Cell-Free Technologies\n\n\n \n \n Figure 1-5. Right, Rapid prototyping kit to take biology from imagination to creation (Image: Cell-Free); left,  a design-test-build cycle with Bio-Pixel visualization on the existing open-source maker platform (BioDesign Challenge), such that synthesis can be followed live via an app (Image: Helene Steiner, Biodesign Challenge, Royal College of Art)\n\n\n\n\n\nFrom Machine Learning to a \u201cMicrobeMiner\u201d to Unlock New Medicines \n\nContinuing on the health theme, clinical trials and approval for new medicines are devastatingly time-consuming, costly, and risky; while the rise of antibiotic resistance poses a rising challenge to global health.\n\nGalactica Biotech is using machine learning algorithms with multiple highly trained modules to identify potential new uses for existing medicines, from small molecules to complex plant alkaloids. The process even works backward to find new targets for these drugs, as well as identifying off-target effects for toxicology studies. This means they will offer their multiapproach A.I. as a service to pharmaceutical companies to help them unlock the full potential of their drug discovery pipeline toward a future of precision medicine. They\u2019ve just used their algorithm successfully to identify a new anticancer lead molecule in the lab, which is already approved for another indication. The team hails from Russia, Spain, Mexico, and the UK. It brings together expertise from PhD research in computational medicinal chemistry, artificial intelligence, systems, and synthetic biology.\n\nCyCa OncoSolutions is founded by Dr. Nusrat Jahan and curious things can happen when a chemist is doing biology. She discovered a biomolecular machinery to permeate the cell membrane. This allows for more effective and targeted delivery, for instance, of cancer drugs. The young innovator and principal investigator worked relentlessly all around the world, from the universities of Oxford, Leiden, and Kyoto to ETH Zurich, driven to find a solution to her father\u2019s life-shattering cancer diagnosis.\n\nValanx Biotech, on the other hand, makes programmable designer proteins, for instance, to conjugate them to a targeting antibody. It uses versatile click-chemistry to easily dock new molecules to the protein with its patented technology \u201cSnapIt.\u201d\n\nIn software, we mine for bitcoins, whilst in biology, we mine for antibiotics! Prospective Research, Inc. has developed a platform that mimics stimuli in the soil to mine for novel, life-saving antibiotics from Streptomyces bacteria.\n\nHow? With what it calls the MicrobeMiner platform. Why? Because the next billion-dollar drug could be buried in your backyard. Therefore, the team also sends out the MicrobeMiner kits for sample collection to crowd-source that discovery.\n\nAbout 90% of natural products remain hidden in the silent operons of the microbe\u2019s DNA, unless the pathway is induced. The biosynthetic machinery of antibiotics can be initiated by certain stimuli in the soil matrix though. Hence, the microbes are first screened with the GeneMiner technology, which identifies the gene clusters that are silenced in the absence of these inducers.\n\nFor the second step, StimKeys comes into play: this proprietary platform launches a variety of chemical inducers with the aim to unlock these potent chemical pathways. They\u2019ve indeed identified molecules which turn a seemingly uninteresting \u201cdirt microbe\u201d into a powerful factory for novel, potentially life-saving antibiotics.\n\n \n \n Figure 1-6. The Prospective Research team has built the MicrobeMiner platform to mine novel antibiotics from soil microbes. Pictures show Streptomyces cultures used for the applications MicrobeMiner, GeneMiner, and StimKeys. (Image: Prospective Research)\n\n\n\n\n\nGrowing the Circular Economy: Bio-inspired Design, High-tech Ecosystems and new Biomaterials\n\nUrbanization is a global trend that will drastically change how we live. By 2050, up to 66% of the world\u2019s population will live in cities, according to the United Nations. That means we need better solutions to power, feed and clean up our future megacities.\n\nTherefore, developing a productive circular economy is imperative to make human activity more sustainable and improve the health of our planet. Biomimicry can help us unlock nature\u2019s most resource-efficient blueprints to future-proof humanity. Hence, building smarter, zero-carbon cities with biology has already started.\n\nFor example, NuLeaf Tech is combining the technologies of engineered ecosystems with microbial plant fuel cells as part of a biologically inspired hardware module that treats wastewater to create clean water and generate energy. These were the ideas that gave rise to NuLeaf Tech in the NASA Ames Advanced Studies Lab in 2015.\n\nThe team is testing a first prototype in collaboration with local farmers, with the vision to create high-tech ecosystems and artificial, modular wetlands\u2014even in vertical arrangement for use in the home.\n\n\nOur bio-inspired technology will create purified water and clean energy solutions for industry and residential use.\n Rachel Major, cofounder and CEO of NuLeaf Tech\n \n So, bio-inspired design helps us uncover powerful engineering solutions\u2014and even novel materials!\n \n Examples of sustainably manufactured materials include the ability to 3D-print degradable bioplastics and make useful items from the plastic waste, in which the planet is drowning. This is an area Saphium and BioCellection are working on.\n \n On the other hand, Pili is growing beautiful, living color pigments for print and design from bacteria at an industrial scale, and Chinova Bioworks is turning to mushrooms for new biomaterials.\n \n \n \n Figure 1-7. Left, NuLeaf Tech; right, microbial fuel cell prototype (Images: NuLeaf Tech)\n\n\nThe Foods of the Future? They\u2019re Brewed, Too!\n\nIt\u2019s never been a more exciting time for animal lovers, because we\u2019re entering the post-animal economy. A rapidly increasing number of animal-free products are in development around the world. Examples include allergy-free peanuts (Aranex Biotech), genome-edited plants to grow the designer foods of the future (PlantEdit), and in vitro meat at our sister program IndieBio SF (Memphis Meats).\n\nBut what about sustainable beverages?\n\nPerfect Day (formerly Muufri) is a vegan alternative to milk which has been hitting the headlines as long-awaited animal-free dairy. At the same time, Spira is looking to lock on to the health market with a tasty, nutritious drink produced by Spirulina algae.\n\nAfineur, is cofounded by CEO Dr. Camille Delebecque. Their Cultured Coffee, produced by microbial fermentation, is taking New York by storm. And what better alternative to sweeten your caffeine fix, but with MilisBio\u2019s sweetener proteins?\n\nSeeing as these plug-and-play vegan plant proteins are up to 700 times sweeter than sugar, the MilisBio team is addressing the demand for non\u2013carbohydrate-based artificial sweeteners in a world hooked on sugar.\n\n \n \n Figure 1-8. Afineur Cultured Coffee (Image: Afineur); Spira Spirulina-based drink (Image: Spira); and Perfect Day cow-free milk (Image: Perfect Day)\n\n\n\n\nHacking the Plant Biofactory\n\nAnother major area of biotech innovation is programming microorganisms and plants to produce useful compounds and novel biomolecules.\n\nFor example, Hemoalgea is cofounded by a team of bioengineers from Costa Rica. The startup is using an optimized microalgal factory to make Hirudin, a major anticoagulant originally produced in leeches. The algae has been found to produce complex glycosylation patterns unlike other biomanufacturing platforms. Meanwhile, SwaLife Biotech is extracting novel alkaloids from plants, which have already shown promising results against DNA breakage. They could find applications in cosmetics, and later on, medicine.\n\nSimilarly, Alternative Plants is cofounded by CEO Anna Ramata-Stunda in Latvia. It\u2019s unlocking the hidden treasures of active compounds found in plants by using plant tissue stem cells. This allows to access nature\u2019s reservoir of compounds, while producing them sustainably at large scale, without harm to the often rare and endangered species. The team has already optimized the powerful platform for high yield and is now scaling up the production of cosmetic ingredients with industrial partners.\n\nCanuevo is an Uruguayan-Canadian startup cofounded by Dr. Nils Rehman to launch a paradigm shift for cannabinoid medicine. Their nanoparticle-encapsulation technology can be applied to creams, pills, supplements, and later on, new medicines. Finally, Hyasynth Bio in Canada produces THC and other cannabinoids in highly efficient yeast factories as featured here and in here.\n\nAs we see, very exciting things are happening at the intersection of biology and technology. Synthetic biology startups are at the forefront of tackling diverse areas of life sciences to build a better world by programming life.\n\nHopes are high for astonishing consumer biotech products, delicious foods and beverages, biomaterials and sustainable living, as well as novel medicines and therapeutics\u2014in short, to share the benefits of scientific innovation with people around the world.\n\n\n\n\nAccelerating the Biorevolution\n\nFind out more on the website or this youtube video and stay up to date by connecting on Twitter and Facebook. You can apply for next year\u2019s cohort on the application portal.\n\nRebelBio (previously IndieBio EU) is the world\u2019s first and leading early-stage startup accelerator and part of SOSV, the accelerator VC. The fund has $300 million in assets under management and is the world\u2019s most active investor in synthetic biology. SOSV is also running the leading seed-stage accelerator IndieBio SF in San Francisco.\n\n \n \n Figure 1-9. The 2017 cohort of 15 startups from around to world with the team and the RebelBio and SOSV teams\n\n\n \n \n Figure 1-10. Some of the RebelBio team (from left to right): John Carrigan (chief scientist), Elsa Sotiriadis (program director), Steven O\u2019Connell (program manager) and Bill Liao (founder and SOSV general partner)\n \n \n \n\n\n\nContinue reading How synthetic biology startups are building the future at RebelBio.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zX-N4R55yIc/how-synthetic-biology-startups-are-building-the-future-at-rebelbio"
 },
 {
  "title": "The impact of design at Shopify",
  "content": "Five questions for Cynthia Savard Saucier on how design impacts Shopify\u2019s business outcomes, and tips for improving designer-developer communication.I recently asked Cynthia Savard Saucier, Director of UX at Shopify and author of Tragic Design, to discuss the design process, its impact on Shopify\u2019s business outcomes, and how to improve communication between designers and developers. Cynthia is presenting a keynote on The impact of design: How design influences outcomes at the O\u2019Reilly Velocity Conference, taking place Oct. 1-4 in New York.\n\n1. Describe your design process and an example of the impact it\u2019s had on business outcomes at Shopify.\n\nDescribing the design process is quite difficult since every project and team finds their own. Not every designer sketches, or creates clickable prototypes; it wouldn\u2019t be appropriate in all cases! However, there are some commonalities across all teams at Shopify.\n\nFirst, design doesn\u2019t work in isolation from other disciplines. We have multidisciplinary, product-based teams. This is even visible in the architecture of our offices! The floor plan is pretty much a representation of our roadmap. Also, every project has a multidisciplinary trio of leads (product, engineering, and UX). They make decisions together, and ensure a good team alignment.\n\nSecond, we often start the design process with a design sprint, inspired by Google Ventures. During that sprint, we make sure members from a variety of disciplines, expertise, and locations attend and contribute. This diversity of thought, from the beginning, is very valuable. We aren\u2019t following Google Ventures\u2019 proposed methodology religiously, as we found it wasn\u2019t entirely relevant to the way we work. We adapted the activities, keeping what worked best for us, like the use of a war room, the four-step sketching process, and the voting system. We care just as much about how people work as the deliverable itself. A lot of effort goes into tweaking our processes so they work for the project, people, and product.\n\nThird, we believe that research with real users leads to real insights. This seems quite obvious, but I know from experience that surprisingly few UX professionals ever get to meet their users. That being said, we don\u2019t always do user testing or interviews. We are constantly researching what our merchants want and need (two very different concepts). We join unofficial merchant groups, and read what is posted on forums. We also listen to customer support calls, to learn the pain points and major complaints that our users have. Finally, everyone (regardless of their discipline) is encouraged to spend a day with a merchant. Observing their work, helping them with their business, learning how to process orders, deal with inventory, use the POS, helps build empathy and understanding for what our users go through.\n\nFor example, a merchant shared their process for dealing with shipping update requests with us. Replying to emails takes time and effort, both things we could lessen by leveraging our confirmation page differently. So we did. By presenting the order status on this page, we benefited both our merchants and their customers. It's a significant result\u2014they don\u2019t have to answer the same emails over and over again, and can spend that time in a more valuable way.\n\n2. Why is it important for engineers to start thinking about design\u2019s impact?\n\nLet\u2019s start with a simple statement. Developers and designers are doing the same work: solving problems for their users. They simply use complementary tools to get there. To use a biology reference: designers and developers are not in a neutral relationship (where both entities simply live together), but rather a mutualist association (where they benefit from each other).\u00a0\n\nTo quote Yonatan Zunger:\n\nEngineering is not the art of building devices; it\u2019s the art of fixing problems. Devices are a means, not an end. Fixing problems means first of all understanding them\u200a\u2014\u200aand since the whole purpose of the things we do is to fix problems in the outside world, problems involving people, that means that understanding people, and the ways in which they will interact with your system, is fundamental to every step of building a system.\n\nDevelopers have the power to impact the user experience in ways that are not always obvious to the designer (like security, performance, reliability, accessibility, exceptions handling). For example, while designers understand the importance of a good error message, they might not think of all the potential use cases. Chances are that the developers will be making most of the error and exception handling design decisions. We might think that error management is not as important as other pieces of the interface. However, as I explain in my talk, a good error message can make the difference between an unfortunate event and a crisis scenario.\n\nFinally, history has taught us that poorly designed products can anger, sadden, exclude, and even kill people who use them. The existence of a user experience designer on a team doesn't mean that developers aren't equally responsible for the user\u2019s well being.\n\n3. How do you measure the effect your design has on business outcomes?\n\nWe utilize Shopify-wide metrics that generally reflect our goal of making commerce better for everyone. This, however, can mean a lot of different things. We use a variety of long-term and short-term qualitative and quantitative success metrics depending on what area our project lives in. Our UX researchers will make sure we meet our qualitative metrics, and our data team takes care of the quantitative side of things.\n\nWe obviously do A/B testing and experiments to answer some questions. We do some user testing, but also collect feedback from surveys, chats, and customer support phone calls.\n\n4. What can designers and developers do to communicate better with each other?\n\nThere is a golden rule to every interaction: always assume good intentions. No one is intentionally trying to make your work harder. That being said, we must bridge the gap between the two disciplines by offering actionable, specific, and direct feedback. On that subject, I recommend the book Radical Candor by Kim Scott.\n\nAnother way to improve the communication is by building trust between the two teams. If you\u2019re a designer, invite developers to your design feedback sessions and vice versa. They will learn about the restrictions and complexity of your decisions. They will understand the rationale behind specific interactions and, as a bonus, will learn the language used to describe your work.\n\nInvolve each other early in the decision process and be intentional about creating overlaps in knowledge.\n\nWe often read or hear that designers should learn to code. I disagree. I mean, it\u2019s great if a designer wants to learn to code, but it is not a requirement. However, designers should learn how developers work. They should seek feedback and be interested in their tools, techniques, and methods. A designer shouldn\u2019t feel intimidated to attend a standup meeting. On the other hand, developers should go the extra mile to explain things in a way that is understandable to designers. Learn about the right level of abstraction required to offer an answer. Learn how to draw diagrams using a visual vocabulary. Read about the right way to give design feedback: it all comes down to focusing on the problem perceived, not the solution. While designers value feedback, understand that technical arguments aren\u2019t always more important than UX considerations. Be flexible and always defer to what\u2019s best for the user.\n\nFinally, don\u2019t use phrases like \u201cmy designer.\" While this might seem small, it gives the impression that designers are at your service, instead of actual and legitimate team members.\n\n5. What sessions at the O\u2019Reilly Velocity Conference in New York are you looking forward to attending?\n\nTo make the most of every conference I attend, I try go to talks in three categories\u2014at least one that is quite removed from my day-to-day or core expertise, one that contributes to my leadership skills, and one that is related to my craft. For Velocity NY, these are:\n\n\n\tOut of my comfort zone: Carin\u2019s Meier\u2019s keynote Unconventional programming paradigms for the future now\n\n\tMaking me a better manager: Kellan Elliott-McCrea\u2019s talk You are not an architect, this is not a bridge we're building: Leading technical decision making for high-performing teams\n\n\tAdvancing my craft: John Le Drew\u2019s talk Pay attention! Why you should care about psychological safety\n\n\nContinue reading The impact of design at Shopify.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/m0sUsJbL4Uk/the-impact-of-design-at-shopify"
 },
 {
  "title": "Take the 2018 Data Science Salary Survey",
  "content": "As a data professional, you are invited to share your valuable insights. Help us gain insight into the demographics, work environments, tools, and compensation of practitioners in our growing field. All responses are reported in aggregate to assure your anonymity. The survey will require approximately 5-10 minutes to complete.All responses to this survey are reported in aggregate to assure your anonymity.\nContinue reading Take the 2018 Data Science Salary Survey.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/j5ZGlY7HrrU/take-the-data-science-salary-survey"
 },
 {
  "title": "Four short links: 16 August 2017",
  "content": "IoT Hacking, Virtual School Fails, Neural Network NLP, and Using Logical Fallacies\n\nReverse-Engineering IoT Devices -- nice step-by-step of how the author figures out the protocol used by a Bluetooth lightbulb (oh to live in such times) and thus how to control it without the Approved Software.\n\nOf Course Virtual Schools Don't Work -- that graph. The horror.  Once seen, cannot unsee (or ever claim virtual schools are a good idea).\n\nA Primer on Neural Network Models for Natural Language Processing -- This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.\n\n\nEffective Presentations Using Applied Logical Fallacies -- what I also got out of this Informal Logic course was that here were a huge list of classically tested brain shortcuts that are surprisingly effective. Sure, they only work if you\u2019ve turned off some of your critical thinking skills [...] but we\u2019re predisposed to do this all the time! Why? Because critical thinking takes effort and time, and you\u2019re presented with probably hundreds of arguments or bits of reasoning every day, and it would be expensive to evaluate them all deeply. So, we brain shortcut when it\u2019s mostly safe to do so. This is really good!\n\nContinue reading Four short links: 16 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/OFUARzpOUzU/four-short-links-16-august-2017"
 },
 {
  "title": "Four short links: 15 August 2017",
  "content": "P != NP, Open Source Lab Notebook, Science Data, and Anonymous Feedback\n\nA Solution of the P versus NP Problem -- from a respected computability theory researcher, so not the usual crank writing\u2014but will need to withstand interrogation and verification from the rest of the academic community. Explainer: computing is interested in efficiently processing digital information. Some problems can be proven to have efficient solutions (\"P\"), some can be proven to have no efficient solution (\"NP\"). There are a lot of problems, so researchers turn new problems into an archetypical problem with a solution that we've already proven is efficient or inefficient. The big question has been: can we find an efficient solution to a known-inefficient problem?  To do so would mean we could quickly solve hard problems in maps, planning, allocation, even Donkey Kong and Bejeweled. This proof shows that both variations of one archetypical hard problem can't be reduced to an efficient solution: they're proven to be inefficient, which means (because we've shown all inefficient problems are equivalent) that all inefficient problems (\"NP\") can't be solved efficiently (\"aren't P\").\n\neLabFTW -- a free and open source electronic lab notebook.\n\nA Happy Moment for Neuroscience is a Sad Moment for Science -- Allen Institute for Brain Science release data. These data are the first complete set of neural activity recordings released before publication. No papers preceded it, not even a report. Nothing. Just: here you go guys, the fruits of the joint labour of around 100 people over four years.  Traditional research organizations (universities) couldn't do this: they rely on publications for funding, kudos, and measuring success.\n\nSarahah -- get anonymous feedback. Nice design: only you can see the feedback, so it doesn't promote pile-ons.  A lot of teens I know are using it positively, which may be a first for software. The Verge wrote it up if you need more social proof.\n\nContinue reading Four short links: 15 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/26xHnFPD7SE/four-short-links-15-august-2017"
 },
 {
  "title": "How to use Presto Sketching to clarify your team\u2019s purpose",
  "content": "The Team Purpose map can visually guide and capture a team\u2019s discussions about their identity, purpose and direction.This chapter is about using Presto Sketching techniques to envision the\u00a0future state. What will change about you and the people for whom you\u2019re designing? What solution are you going to create, and how will it change that present state? This area is a mix of conceptual and literal representation, with a bit of visual metaphor thrown in. Let\u2019s see how it applies to:\n\n  Helping your team rally around a common future with the Team Purpose map\n  Envisioning the future state of your customers with the Superhero Booth sketch\n  Exploring goals with the Build a Bridge sketch\n  Knocking down the barriers with the Goal Barriers sketch\n  Envision a future product with the experience canvas\n  Bringing future experiences to life with storyboarding\n\n\nRally around a common future with the Team Purpose map\nSetting out to create a new product or service is one of the most exciting things you can ever do. But sometimes it\u2019s worth parking that for a spell, and checking how you as a team are functioning first.\nIf you\u2019re part of a startup business, it\u2019s common to think of your business and your product as one and the same thing, having one and the same purpose. If you\u2019re part of a larger business with one or more products or services, your team purpose might need to be expressed in a different way (e.g. a product support team\u2019s purpose is different to the development team\u2019s purpose).\u00a0\nOr maybe you have had some staff changes in the team, and it\u2019s been a while since the team\u2019s purpose was really said out loud.\n\n\nFigure 2. The Team Purpose map: This is a fun visual activity to do with your team, especially if it\u2019s a new team and you need to form a shared understanding together of who you are, your purpose, and your destination.\n\nIn any case, the Team Purpose map is a large-format template you can use to visually guide and capture a team\u2019s discussions about their identity, purpose and direction.\nThere are loads of different kinds of these large-format graphic facilitation templates for capturing strategic outputs of large group discussions.49\u00a0The version you see above is my version, based on refining a template through lots of different strategic sessions. As with all of these visual patterns, do feel free to adapt it to your team\u2019s needs.\u00a0\nUse this pattern when you want to:\n\n  Set (or reset) a team\u2019s identity, purpose and direction, and get a shared understanding and sense of ownership\n  Expose any hidden assumptions or anxieties in a new team, or a new project\n  Align several stakeholders, teams or organisations around a common purpose and direction\n\nHow to do it\nDepending on your confidence in facilitating a discussion, you can either guide the discussion yourself, or give that duty to someone else while you capture what people are talking about on the wall or whiteboard.\nPreparation is crucial for this sort of activity. Make sure that everyone knows ahead of time what will be covered. If necessary, give them \u2018homework\u2019 to do, to help them prepare for various sections of this activity. You might want to ask everyone to list what their own hopes and fears are for the new team, or what particular skills and assets they want to bring to the team.\n\n  Gather your group and draw the team purpose map on the biggest whiteboard or wall you can find.\n  It\u2019s a good idea to have the boss or a senior stakeholder establish the intent of the strategic discussion, and give some background that would be helpful for everyone.\u00a0\n  Start everyone off with something fun: get them to draw themselves in the space where you see \u2018TEAM\u2019 displayed. This definitely shows everyone that this isn\u2019t your regular sort of meeting, plus it gets them used to using their hands, and not just their mouths.\n  Ask everyone to take a sticky note and write what they think is the purpose of the team (or product, or whatever you are focused on), and stick it in the \u2018PURPOSE\u2019 area. As a group, reflect on any differences that have been posted up. The aim here is to get a shared understanding.\n  Next, ask everyone to write what they think are the team\u2019s goals, and stick them in the \u2018GOALS\u2019 area. This is what the team is going to do.\n  This is a good time to ask your group what they think the \u2018SUCCESS FACTORS\u2019 will be for achieving those goals. Depending on your context, this could be anything from \u2018can-do attitude\u2019 to \u2018special sales campaigns\u2019.\n  As a facilitator, it\u2019s worth pausing and reflecting on the story unfolding on the Team Purpose map so far. You have a team with a specific purpose, and to achieve that purpose, there are several goals. There are also several things that have to happen for this to be a success. But now, you have to equip yourselves for what lies ahead by calling out the \u2018CHALLENGES\u2019. Go ahead and get the group to write those up on the Team Purpose map too.\n  Now that your group knows\u00a0what\u00a0they have to do, they should now fill the \u2018ACTIONS\u2019 area with notes about\u00a0how\u00a0they\u2019re going to do it. This might take some rearranging, but see if you can map it in a sequence of some sort.\n  The last part is to check if your group has the right \u2018SKILLS\u2019 to be able to achieve all those actions. Get your group to write and stick up what skills are necessary, and then reflect on that set, to see if your group does indeed have those skills. If ever there was a time to call out any skills-gaps to achieve those goals, now is that time!\n  It\u2019s a nice idea to end your strategic discussion session by getting everyone to reflect on the overall story that the Team Purpose map is telling, and ask them to summarise it in their own words. This helps people to really internalise that story, rather than staying mentally \u2018at arms length\u2019 from it.\u00a0\n\nContinue reading How to use Presto Sketching to clarify your team\u2019s purpose.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YKSNv7zSJJk/how-to-use-presto-sketching-to-clarify-your-teams-purpose"
 },
 {
  "title": "Four short links: 14 August 2017",
  "content": "Robotics Interviews, Customer Development, Engineering Method, and Complex System Failures\n\nRobotics Interviews -- Mike Salem (Robotics Nanodegree Service Lead), interviews professional roboticists.\n\n\n10 Things I've Learned About Customer Development -- my favorite: If someone says, \u201cI wouldn\u2019t personally use it, but I bet other people would,\u201d no one will use it. They're all true.\n\nMultiple Perspectives on Technical Problems and Solutions (John Allspaw) -- [The Engineering Method is] \u201cthe strategy for causing the best change in a poorly understood or uncertain situation within the available resources.\u201d\n\n\nHow Complex Systems Fail -- Complex systems contain changing mixtures of failures latent within them. The complexity of these systems makes it impossible for them to run without multiple flaws being present. Because these are individually insufficient to cause failure, they are regarded as minor factors during operations. Eradication of all latent failures is limited primarily by economic cost but also because it is difficult before the fact to see how such failures might contribute to an accident. The failures change constantly because of changing technology, work organization, and efforts to eradicate failures.\n\n\nContinue reading Four short links: 14 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/JFFLZH426Ok/four-short-links-14-august-2017"
 },
 {
  "title": "A multi-cloud strategy is the foundation for digital transformation",
  "content": "Using a single cloud provider is a thing of the past.When news broke that Amazon was acquiring Whole Foods, brick and mortar retailers panicked. Many industries are feeling pressure to undergo digital transformation for fear of being \u201cAmazoned\u201d (acquired by an online competitor). The International Data Corporation (IDC) forecasts worldwide spending on digital transformation technologies will grow to more than $1.2 trillion in 2017 (an increase of 17.8% over 2016. IDC\u2019s Worldwide Semiannual Digital Transformation Spending Guide reports that many industries have been affected by the frenzy, too, with manufacturing leading the pack, followed by retail, health care, insurance, and banking. No one wants to be \u201cdisrupted.\u201d\n\n\n\nDefining digital transformation\n\nSurprisingly, despite the anxiety around digital transformation and the money being invested, the term is not well defined. George Westerman, principal research scientist with the MIT Sloan Initiative on the Digital Economy, defines digital transformation as \u201cusing technology to radically improve the performance and reach of an organization.\u201d Technology, however, is accelerating at such a fast pace that it makes it difficult for businesses to keep up and stay ahead of the curve.\n\n\n\nDigital transformation, in practice\n\nHow do businesses unlock the power of rapidly evolving technology to maximize their investment? One answer, according to the 2017 Digital IQ survey, which looked at top performing IT and business leaders across industries is to focus less on one specific technology and more on the human experience. Focusing on the human experience means organizations will have to re-examine their customers\u2019 experiences, employees\u2019 experiences, and their entire organizational culture. In developing a better understanding of the human experience that surrounds digital technology, companies are better equipped to continuously adapt and anticipate marketplace changes.\n\n\n\nUsing a multi-cloud strategy to achieve digital transformation\n\nA growing number of companies have embraced a multi-cloud approach as part of their digital transformation strategy, mainly to meet various business and technology requirements because no single cloud model can fit the diverse requirements and workloads across different business units.\n\nEvery major cloud platform\u2014including Amazon Web Services, Microsoft Azure, and the Google Cloud Platform\u2014has a range of data-related services in the areas of cloud data warehousing, big data, NoSQL, and real-time streaming, to name a few. Some workloads run better on one cloud platform while other workloads achieve higher performance and lower cost on another platform. By adopting a multi-cloud strategy, companies are able to choose best-in-class technologies and services from different cloud providers to create the best possible solution for a business.\n\nAnother reason for a multi-cloud approach is that companies adopt new IT projects in increments. A multi-cloud environment creates an ideal sandbox for IT to experiment with and deploy proof-of-concepts. Various teams can set up and decommission scenarios quickly without the burden of the high cost and time needed to build and deploy new infrastructure.\n\nWith a multi-cloud strategy, each business unit can choose their ideal cloud service depending on their specific business needs, which may include:\n\n\n\tEase of administration\n\tConcurrency of workloads\n\tQuery performance\n\tUser roles and access levels\n\tSecurity and governance\n\tGovernment regulations and data privacy\n\tSpecialized functionality for a particular workload or application (i.e., machine learning or AI)\n\tAccess to big data tools and technologies\n\n\nAs businesses go through their digital transformation journey, multi-cloud environments will be increasingly used to transition and migrate from the old model of customer engagement to newer multi-pronged models. The trend is moving toward deeper and broader cloud engagement, so a well-defined multi-cloud strategy will be key to achieving maximum return on investment (ROI). Other benefits of adopting a multi-cloud strategy include:\n\n\n\n\n\t\nPrice flexibility: Better pricing flexibility by leveraging different cloud platforms.\n\t\nRisk mitigation or redundancy: By spreading workloads and data across multiple cloud platforms, companies can reduce downtime and have a smoother disaster recovery plan.\n\t\nUnlimited scalability and better agility: Elasticity and agility to empower business units as they grow, and meet the demands to access more data.\n\t\nAvoid vendor lock-in: The option to adapt to changes in the marketplace, without reworking the whole cloud architecture to suit one vendor.\n\t\nAdopt use of best practices: Best practices built on one cloud vendor can be applied across the enterprise to departments using other cloud vendors.\n\n\n\n\nAs more companies strategically position themselves in a multi-cloud world, IT departments in general, and CIOs in particular, need to take on more strategic roles as facilitators between various areas of the business. This trend reflects the key point that digital transformation is not a one-time phenomenon, but an ongoing process. Even as companies realize the impacts of social media, mobile and cloud computing, and other \u201cdisruptive\u201d technologies already on the rise (including machine learning, data collection via drones, and virtual reality), data remains a central tenet to their success. With the amount of data growing exponentially, a multi-cloud strategy is becoming a deliberate choice for companies and a way to chart their long-term growth. Companies that seek to remain ahead of the curve will be in a better position to respond and continually transform themselves in a quickly evolving digital landscape.\n\nThis post is a collaboration between Talend and O\u2019Reilly. See our statement of editorial independence.\nContinue reading A multi-cloud strategy is the foundation for digital transformation.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/wouNDAYHuZw/a-multi-cloud-strategy-is-the-foundation-for-digital-transformation"
 },
 {
  "title": "How to choose a cloud provider",
  "content": "Practical questions to help you make a decision.If you look up the phrase \u201cboiling the ocean,\u201d it\u2019s defined as writing a post on choosing a cloud provider\u2014there are so many different facets and use cases, and each variable complicates your choice. The key is to narrow the field to your specific situation and needs. In this article, I share some of the early questions and decisions I use when working with a team to choose a cloud provider.\n\nSimple pass/fails\n\nI recently worked with a large financial organization who was considering a move to the cloud. When I started the engagement, we began with quick pass/fail decisions to see if a cloud move was feasible. These pass/fail choices allowed the team to make an initial go-forward decision before they went too deep down the rabbit hole. The following considerations helped mitigate their risk.\n\nCan we use the cloud?\n\n\n\nThis might sound like an easy one, but some teams actually forget this step. There may be an outright prohibition on using the cloud at your organization, or the political climate may be so terrible that you decide not to fight that battle.\n\nWill the technologies work for your use case?\n\n\n\nCan the cloud technologies do what you need them to do? With small data problems, this isn\u2019t usually an issue: they can handle almost anything. With big data problems, however, capability can be a sticky issue\u2014there are bigger tradeoffs that the technology designers had to make.\n\n\n\nI\u2019ve written some in-depth posts covering some of these tradeoffs and comparisons. For example, I compared Amazon Kinesis and Apache Kafka, and Google Cloud Pub/Sub and Apache Kafka. You\u2019ll notice there are subtle differences. In the lens of a use case, these differences can make a requirement of your use case impossible to implement with a technology.\n\n\n\nIn order to make this initial call, you need to really understand and know your use case. You\u2019ll want to have a general idea of where and what you want to do in the future, and validate those use cases, too.\n\nCompare providers\n\nAt the 10,000-foot level, there isn\u2019t a glaring difference at small data scales. At these small data scales, there are fewer distinguishing factors. But at big data scales, the different cloud providers have bigger differences.\n\n\n\nGenerally, the providers distinguish themselves on:\n\n\n\tHow easy are the managed services to use and operate? For example, how easy is it for me to spin up a database and have it replicate all over the world?\n\tSince the services are managed, how good is their uptime and how reliable is the system? In other words, how often does the service go down, and for how long? Some outages are system-wide, but how often do your instances disappear?\n\tThe cloud providers fight each other on price. What is the total cost of running infrastructure on the cloud provider? Unfortunately, this question can be difficult to answer because working with a single technology can have three or four different costs associated with it. For example, a messaging system could have costs for the message transfer, the storage of the message, and an hourly fee for the managed messaging system.\n\tHow prevalent are engineers with knowledge of that cloud provider in the marketplace? These days, you can\u2019t throw a stone without hitting someone with some knowledge of Amazon Web Services. That said, the skills are largely similar at an operation level. The skills at a developer level are only somewhat similar.\n\tHow difficult will it be for you to move between cloud providers and technologies\u2014aka \u201clock in\u201d? For example, some providers use an open source technology or its API, but have their managed service behind the scenes. Conversely, using a managed service with a proprietary API really couples you to that cloud provider.\n\n\nKeep your options open\n\nAs you\u2019re looking over providers, don\u2019t just look at the big cloud providers. Take the time to investigate niche providers as well. They may be able to provide a level of service that the big providers don\u2019t (although, be warned that some smaller providers don\u2019t provide tech support). Still other niche providers handle the operations of open source technologies that aren\u2019t necessarily managed services.\n\nAsk the right questions\n\nOnce you\u2019ve considered the big-picture factors, you\u2019re ready to fine-tune your decision process. Here are the questions I ask teams to consider when choosing a provider:\n\n\n\n\n\tCost: Is cost a primary factor? Have you created an accurate calculation for your comparisons?\n\tPopularity contest: How popular is this cloud provider and do they have enough customers not to go the way of the dodo bird?\n\tAvailability of people: How difficult will it be to train the existing staff on the provider\u2019s technologies (for both operations and development)? How difficult will it be to hire people who are familiar with the provider?\n\tSLAs: What level of SLA does the provider give for the services you\u2019ll be using (remember that some services are ghosted under a main service)? Does management know that during a large-scale outage, you won\u2019t be able to scream loud enough to speed things up?\n\tUse case: Does the technology work for your use case? Do you really understand your use case well enough to validate the technologies?\n\tLock-in: Are you comfortable with the potential level of lock-in? Will you write your software to be highly coupled to that service?\n\tCompany politics: Have you settled the company politics? In other words, have you established who owns what, and who is responsible for each piece?\n\n\n\n\nOnce you\u2019ve thought about and answered these questions, you\u2019ll be in a better position to make an accurate comparison of the cloud provider landscape. A little preparation and careful groundwork are the keys to making the right choice.\nContinue reading How to choose a cloud provider.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FYMaMWyfMko/how-to-choose-a-cloud-provider"
 },
 {
  "title": "Four short links: 11 August 2017",
  "content": "Cracking Wi-Fi, Hacking with DNA, Animation Playthings, and Physics Simulation\n\nWi-Fi Cracking -- This is a brief walk-through tutorial that illustrates how to crack Wi-Fi networks that are secured using weak passwords. It is not exhaustive, but it should be enough information for you to test your own network's security or break into one nearby. The attack outlined below is entirely passive (listening only, nothing is broadcast from your computer) and it is impossible to detect, provided you don't actually use the password that you crack.\n\n\nComputer Security and Privacy in DNA Sequencing -- Here we highlight two key examples of our research below: (1) the failure of DNA sequencers to follow best practices in computer security and (2) the possibility to encode malware in DNA sequences. Buffer overflow in binary -> DNA -> crappy sequencer code -> binary -> \"I'm in!\".\n\nWick -- a free browser-based toolkit for creating small interactive things for the Internet. ... Wick is a hybrid of an animation tool and a coding IDE, heavily inspired by similar tools such as Flash, HyperCard, and Scratch. Open Source.\n\nPredictive Simulation in Game Physics -- Predictive simulation is traditionally used as part of planning and game AI algorithms; we argue that it presents untapped potential for game mechanics and interfaces. We explore this notion through 1) deriving a four-quadrant design space model based on game design and human motor control literature, and 2) developing and evaluating six novel prototypes that demonstrate the potential and challenges of each quadrant. Our work highlights opportunities in enabling direct control of complex simulated characters, and in transforming real-time action into turn-based puzzles. Based on our results, adding predictive simulation to existing game mechanics is less promising, as it may feel alienating or make a game too easy. Preprint paper and open source code.\n\nContinue reading Four short links: 11 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HhjHjsTkfXg/four-short-links-11-august-2017"
 },
 {
  "title": "Mike Roberts on serverless architectures",
  "content": "The O\u2019Reilly Programming Podcast: The next technological evolution of cloud systems.In this episode of the O\u2019Reilly Programming Podcast, I talk serverless architecture with Mike Roberts, engineering leader and co-founder of Symphonia, a serverless and cloud architecture consultancy. Roberts will give two presentations\u2014Serverless Architectures: What, Why, Why Not, and Where Next? and Designing Serverless AWS Applications\u2014at the O\u2019Reilly Software Architecture Conference, October 16-19, 2017, in London.Continue reading Mike Roberts on serverless architectures.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1qkI899uoTo/mike-roberts-on-serverless-architectures"
 },
 {
  "title": "How to craft a voice user interface that won\u2019t leave you frustrated",
  "content": "Design principles for creating a truly conversational UI.Continue reading How to craft a voice user interface that won\u2019t leave you frustrated.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/sOkZFTd0-D8/how-to-craft-a-voice-user-interface-that-wont-leave-you-frustrated"
 },
 {
  "title": "Four short links: 10 August 2017",
  "content": "Sarcasm Detector, GMO Salmon, New Deep Learning Courses, and Serverless Malware Detection\n\nAn Algorithm Trained on Emoji Knows When You're Being Sarcastic on Twitter (MIT TR) -- They also tested it against humans, using volunteers recruited through the crowdsourcing site Mechanical Turk. They found it was better than the humans at spotting sarcasm and other emotions on Twitter. It was 82% accurate at identifying sarcasm correctly, compared with an average score of 76% for the human volunteers.\n\n\nGMO Salmon On Sale (Guardian) -- Originally developed by a group of Canadian scientists at Newfoundland\u2019s Memorial University, the salmon can grow twice as fast as conventionally farmed Atlantic salmon, reaching adult size in some 18 months as compared to 30 months. The product also requires 75% less feed to grow to the size of wild salmon, reducing its carbon footprint by up to 25 times, the company has claimed.\n\n\nNew Coursera Deep Learning Courses -- topics: neural networks, tuning and optimization, structuring ML projects, convolutional neural networks, sequence models.\n\nBinaryAlert -- Serverless, real-time, and retroactive malware detection from Airbnb.  (via Airbnb Engineering blog)\n\nContinue reading Four short links: 10 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2RAyv6-no_A/four-short-links-10-august-2017"
 },
 {
  "title": "Four short links: 9 August 2017",
  "content": "Event Trend Detection, Separation of Duties, Online Communities, and Autonomous Tractors\n\nComplete Event Trend Detection in High-Rate Event Streams -- Complex Event Processing (CEP) has emerged as a prominent technology for supporting streaming applications from financial fraud detection to health care analytics. CEP systems consume high-rate streams of primitive events and evaluate expressive event queries to detect event sequences such as circular check kites and irregular heart rate trends in near real time. These event sequences may have arbitrary, statically unknown, and potentially unbounded length. (via Paper a Day)\n\nSeparation of Duties -- boring stuff that's worth getting right.\n\n30+ Case Studies of Building Online Communities -- from Burning Man to Twitch.\n\nJohn Deere's Autonomy Lessons -- in order to build a fully autonomous tractor, there are no shortcuts. While a blend of GPS and other location tracking sensors, image sensors, and telematics assist John Deere vehicles to navigate fields today, the company still can\u2019t truly replicate everything a human would see and feel sitting in the tractor cab. The company\u2019s latest commercially available machine with autonomous features, the S700 combine (a vehicle that harvests grain), can automatically adjust its harvesting equipment based on the condition of the crop it sees\u2014but still gives the farmer sitting in the tractor a camera on the process to make sure it\u2019s happening correctly. Right now all of John Deere\u2019s tractors still require a human to sit inside\u2014a sign that autonomy is a long road even in controlled environments.\n\n\nContinue reading Four short links: 9 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/c-1le4-TpsU/four-short-links-9-august-2017"
 },
 {
  "title": "Deep learning revolutionizes conversational AI",
  "content": "Recent AI breakthroughs transform speech recognition.The dream of speech recognition is a system that truly understands humans speaking\u2014in different environments, with a variety of accents and languages. For decades, people tackled this problem with no success. Pinpointing effective strategies for creating such a system seemed impossible.\n\nIn the past years, however, breakthroughs in AI and deep learning have changed everything in the quest for speech recognition. Applying deep learning techniques enabled remarkable results. Today, we see the leap forward in development manifesting in a wide range of products, such as Amazon Echo, Apple Siri, and many more. In this post, I\u2019ll review recent advances in speech recognition, examine the elements that have contributed to the rapid progress, and discuss the futureand how far we may be from solving the problem completely.\n\nA little background\n\nFor years, one of the most important tasks of AI has been to understand humans. People want machines to understand not only what they say but also what they mean, and to take particular actions based on that information. This goal is the essence of conversational AI.\n\nConversational AI encompasses two main categories: man-machine interface, and human-to-human interface. In a man-machine interface, a human interacts with a machine through voice or text, and the machine understands the human (even if in a limited manner), and takes some kind of action. Figure 1 demonstrates that the machine can be either a personal assistant (SIRI, Alexa, or similar) or some kind of chatbot.\n\n\nFigure 1. Man-machine AI. Source: Yishay Carmiel.\n\n\nIn a human-to-human interaction, the AI forms a bridge between two or more humans having conversations, interacting, or creating insights (see Figure 2). An example might be an AI that listens to conference calls, then creates a summary of the call, following up with the relevant people.\n\n\nFigure 2. Human-to-human AI. Source: Yishay Carmiel.\n\n\nMachine perception and cognition\n\nIn order to understand the challenges and the technologies behind conversational AI, we must examine the basic concepts in AI: machine perception and machine cognition.\n\nMachine perception is the ability of a machine to analyze data in a manner similar to the way humans use their senses to relate to the world around them; in other words, essentially giving a machine human senses. A lot of the recent AI algorithms that employ computer cameras, such as object detection and recognition, fall into the category of machine perception\u2014they concern the sense of vision. Speech recognition and profiling are machine perception technologies that use the sense of hearing.\n\nMachine cognition is the reasoning on top of the metadata generated from the machine perception. Machine cognition includes decision-making, expert systems, action taking, user intent, and more. Generally, without machine cognition, there isn\u2019t any impact from the outcome of the AI\u2019s perception; machine perception delivers the proper metadata information for a decision and action.\n\nIn conversational AI, machine perception includes all speech analysis technologies, such as recognition and profiling, and machine cognition includes all the language understanding-related technologies, which are part of Natural Language Processing (NLP).\n\nThe evolution of speech recognition\n\nThe research and development of speech recognition occurred over three major time periods:\n\nBefore 2011\n\nActive research on speech recognition has been taking place for decades; in fact, even in the 1950s and 1960s, there were attempts to build speech recognition systems. However, before 2011 and the advent of deep learning, big data, and cloud computing, these solutions were far from sufficient for mass adoptions and commercial use. Essentially, the algorithms were not good enough, there was not enough data on which to train the algorithms, and the lack of available high-performance computing prevented researchers from running more complicated experiments.\n\n2011 \u2013 2014\n\nThe first major effect of deep learning occurred in 2011, when a group of researchers from Microsoft, Li Deng, Dong Yu, and Alex Acero, together with Geoffrey Hinton and his student George Dahl, created the first speech recognition system that is based on deep learning. The impact was immediate: more than 25% relative reduction in the error rate. This system was the starting point for massive developments and improvements in the fields. Together with more data, available cloud computing, and adoption from big companies like Apple (SIRI), Amazon (Alexa), and Google, there were significant improvements in the performance and the products released to the market.\n\n2015 \u2013 Today\n\nAt the end of 2014, recurrent neural networks gained much more emphasis. Together with attention models, memory networks, and other techniques, this era comprised the third wave of progress. Today, almost every type of algorithm or solution uses some type of neural model, and, in fact, almost all of research in speech has shifted toward deep learning.\n\nRecent advances of neural models in speech\n\nThe past six years of speech recognition has created more breakthroughs then the previous 40+ years. This extraordinary recent advancement is due to neural networks. In order to understand the impact of deep learning and the part it plays, we need to understand first how speech recognition works.\n\nAlthough speech recognition has been in active research for almost 50 years, building machines that understand human speech is one of the most challenging problems in AI. It\u2019s much harder then it may appear. Speech recognition consists of well-defined tasks: given some kind human speech, try to convert the speech into words. However, the speech can be part of a noisy signal, requiring the work of extracting the speech from the noise and converting the relevant parts into meaningful words.\n\nThe basic building blocks of a speech recognition system\n\nSpeech recognition is basically divided into three main sections:\n\nThe signal level: The aim of the signal level is to extract speech from the signal, enhance it (if needed), do proper pre-processing and cleaning, and feature extraction. This level is very similar to every machine learning task; in other words, given some data, we need to do proper data pre-processing and feature extraction.\n\nThe acoustic level: The aim of the acoustic level is to classify the features into different sounds. Put another way, sounds themselves do not provide a precise enough criteria, but rather sub-sounds that are sometimes called acoustic states.\n\nThe language level: Since we assume the sounds are produced by a human and have meaning, we take the sounds, combine them into words, and then take the words and combine them into sentences. These techniques in the language levels are usually different types of NLP techniques.\n\nImprovements from deep learning\n\nDeep learning made a significant impact on the field of speech recognition. The impact is so far-reaching that even today, almost every solution in the field of speech recognition probably has one or more neural-based algorithms embedded within it.\n\nUsually the evaluation of speech recognition systems is based on an industry standard called Switchboard (SWBD). SWBD is a corpus of speech assembled from conversations improvised over the phone. SWBD includes audio and human-level transcriptions.\n\nThe evaluation of a speech recognition system is based on a metric called word error rate (WER). WER refers to how many words are misrecognized by the speech recognition system. Figure 3 shows the improvement in WER from 2008 \u2013 2017.\n\n\nFigure 3. Improvement in word error rate. Source: Yishay Carmiel.\n\n\nFrom 2008 to 2011, the WER was in a steady state, around 23 to 24%; deep learning started to appear in 2011; and as a result, reduced the WER from 23.6% to 5.5%. This development was a game changer for speech recognition, an almost 77% relative improvement. Now there are a wide range of applications, such as Apple SIRI, Amazon Alexa, Microsoft Cortana, and Google Now. We also see a variety of appliances activated by speech recognition, such as Amazon Echo and Google Home.\n\nThe secret sauce\n\nSo, what improved the system so drastically? Is there a technique that reduced the WER from 23.6% to 5.5%? Unfortunately, there isn\u2019t a single method. Deep learning and speech recognition are so entangled that creating a state-of-the-art system involves a variety of different techniques and methods.\n\nAt the signal level, for instance, there are different neural-based techniques to extract and enhance the speech from the signal itself (Figure 4). Also, there are techniques that replace the classical feature extraction methods with more complex and efficient neural-based methods.\n\n\nFigure 4. Signal level analysis. Source: Yishay Carmiel.\n\n\nThe acoustic and language levels also encapsulate a variety of different deep learning techniques, from acoustic state classification using different types of neural-based architectures to neural-based language models in the language level (see Figure 5).\n\n\nFigure 5. Acoustic and language-level analysis. Source: Yishay Carmiel.\n\n\nCreating a state-of-the-art system is not an easy task, and building it involves implementing and integrating all these different techniques into the system.\n\nCutting edge research\n\nWith so many recent breakthroughs in speech recognition, it\u2019s natural to ask, what\u2019s next? Three primary areas seem likely to be the main focus of research in the near future: algorithms, data, and scalability.\n\nAlgorithms\n\nWith the success of Amazon Echo and Google Home, many companies are releasing smart speakers and home devices that understand speech. These devices introduce a new problem, however: the user is often not talking closely into a microphone, as with a mobile phone, for instance. Dealing with distant speech is a challenging problem that is being actively researched by a lot of groups. Today, innovative deep learning and signal processing techniques can improve the quality of recognition.\n\nOne of the most interesting topics of research today is finding new and exotic topologies of neural networks. We see some promising results both in language models and acoustic models that are being applied. Two examples are Grid-LSTM in acoustic models and attention-based memory networks for language models.\n\nData\n\nOne of the key issues in a speech recognition system is the lack of real-life data. For example, it is hard to get high-quality data of distance speech. However, there is a lot of available data from other sources. One question is: can we create proper synthesizers to generate data for training? Generating synthesized data and training systems based on that is getting good attention today.\n\nIn order to train a speech recognition system, we need data sets that have both audio and transcriptions. Manual transcription is tedious work and sometimes can cause problems on large amounts of audio. As a result, there is active research on semi-supervised training and building proper confidence measure for the recognizers.\n\nScalability\n\nSince deep learning is so entangled with speech recognition, it consumes a non-trivial amount of CPU and memory. With users\u2019 massive adoption of speech recognition systems, building a cost-effective cloud solution is a challenging and important problem. There is ongoing research into how to reduce the computation cost and develop more efficient solutions. Today, most speech recognition systems are cloud-based, and have two specific issues that must be solved: latency and continuous connectivity. Latency is a key issue in devices that need an immediate response, such as robotics. And in a system that is listening all the time, continuous connectivity is a problem due to bandwidth cost. As a result, there is research toward edge speech recognition that preserves the quality of a cloud-based system.\n\nSolving the problems of speech recognition\n\nIn the recent years, there has been a quantum leap in speech recognition performance and adoption. How far are we from solving it completely? Are we five years, or maybe 10 from declaring victory? The answer is, probably\u2014but there are still challenging problems that will take some time to solve.\n\nThe first problem is the issue of sensitivity to noise. A speech recognition system works pretty well from a close microphone and non-noisy environment; however, distant speech and noisy data degrades the system rapidly. The second problem that must be addressed is language expansion: there are roughly 7,000 languages in the world, and most speech recognition systems support around 80. Scaling the systems poses a tremendous challenge. In addition, we lack data on many languages, and a speech recognition system is difficult to create with low data resources.\n\nConclusion\n\nDeep learning has made its mark on speech recognition and conversational AI. Due to recent breakthroughs, we are truly on the edge of a revolution. The big question is, are we going to achieve a triumph, solve the challenges of speech recognition, and begin to use it like any other commoditized technology? Or is a new solution waiting to be discovered? After all, the recent advances in speech recognition are just one piece of the puzzle: language understanding itself is a complex mystery\u2014maybe an even greater one.\nContinue reading Deep learning revolutionizes conversational AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/puFCZs0njgM/deep-learning-revolutionizes-conversational-ai"
 },
 {
  "title": "Cancer detection, one slice at a time",
  "content": "New technology is allowing researchers to use digitization to help detect cancer.\n\n\n\n\nKe Cheng\u2019s story begins in a way that\u2019s achingly familiar. She recalled late nights and long hours as a grad student, repeating a tedious task in the name of research. Like many of us, Ke dreamed of starting a company that would make that job easier and faster. She ended up building the world\u2019s largest online, preclinical pathology database, which catalyzes and could ultimately lead to the automated detection of cancer. But let\u2019s take a step back\u2014how did Ke go from an overworked grad student to building a fountain of digital data?\n\nFor her, the tedious grad-school task was cutting tissue and making slides for histopathology (the analysis of tissue to identify or determine changes due to a disease). The process starts by preserving a tissue sample and embedding it within a block of paraffin wax. A microtome is used to section the tissue into slices thin enough for microscopy (typically, about five microns thick). Each slice is placed on a microscope slide, where it can be stained to color different structures. The most common stain, H&E, is a mixture of an acidic dye (hematoxylin) and a basic one (eosin), which will stain cytoplasm pink and nuclei purple. This alone can be used to detect disease, just by allowing researchers to observe the organization (or disorganization) of the tissue structure. But there are hundreds of \u201cspecial stains\u201d to detect tissue elements (e.g., muscle fibers, glycoproteins, and mucins), microorganisms (e.g., fungi and bacilli), or specific ions like ferric iron (Fe3+). Immunohistochemistry (IHC) can also be performed on tissue slides. IHC uses antibody-based detection to confirm cancer subtypes. While standard H&E staining is automated, using special stains or IHC can take significant time to optimize. Most researchers prep the tissue themselves and then send it off for sectioning and staining. At the time of Ke\u2019s graduate work, it took two or more weeks to get the slides back.\n\nThe key step that transformed Ke\u2019s histology-service company into a pathology data powerhouse was simple: digitization. When Ke founded HistoWiz four years ago, there were already instruments available that would automate histopathology (though they were uncommon to see in everyday research labs). But automating slide histology wasn\u2019t the final vision; it was just the first step. Ke was inspired by the open source genomic research model, in which searchable databases exist that everyone can access and contribute to. She saw firsthand that a trove of histopathology data existed but was unable to be easily shared because it was siloed in slide boxes (and tucked away in dusty cabinets or drawers). Even in the literature, only representative images (tiny sections of the entire slide) were used; and of course, publishable results are a small fraction of the all data collected.\n\nKe started HistoWz on her own and drew an initial customer base by promising a three-day turnaround (compared to the two-week industry standard at the time). She spent her days going door to door in her old research buildings, asking if people would be interested in doing a free trial. Then she spent her nights processing the samples using core facility equipment. Four years later, HistoWiz supports a number of major research institutions (including Harvard, MD Anderson, and HHMI), as well as big pharma (Pfizer, Regeneron, and Novo Nordisk).\n\nThe difference between HistoWiz and the other histology competitors was that Ke was the first to digitize the slides. After placing an order, the customer receives a high-res, microscope-quality image of the entire slide that can be viewed (with up to 40x magnification) on a laptop, smartphone, or tablet (Figure\u00a01-1). If desired, the customer can receive the raw image or have the slide analyzed by one of HistoWiz\u2019s in-house pathologists. While providing faster, better histology as a service, HistoWiz had an open pipeline to data. Each customer receives a discount if they choose to contribute their data, so each slide HistoWiz processed could be incorporated into a database.\n\n\nFigure 1-1. An H&E stained slide of metastatic murine lung cancer, available in HistoWiz\u2019 demo slide gallery. Source: Ke Cheng.\n\n\nHistoWiz\u2019s PathologyMap now has over 30,000 slides and is growing at a rate of over 200 percent each year. While most of the data is from mouse tissue, PathologyMap has data from human and other preclinical/experimental models (zebrafish, rats, rabbits, etc.) as well. Ke\u2019s mission is to \u201cfight cancer cooperatively.\u201d No more slide boxes or static representative images\u2014this database allows researchers to do tissue-driven data mining by using whole slides in a massive dataset. It\u2019s important to Ke that this data is not restricted to just pathologists or academic researchers. For her, this opens a door to allow anyone to step inside the histology field and access the latest discoveries in cancer research.\n\nThe possibilities of what can be done with this database are pretty fascinating. This is a new era of \u201cdigital pathology.\u201d The FDA recently approved digital pathology for primary diagnosis, but acquiring enough data is still a very limiting factor. Ke\u2019s database provides the raw data needed to accomplish that goal.\n\nSecondly, Ke sees a potential for researchers to use the HistoWiz database as a tissue bank. She\u2019s collecting extra unstained slides from her contributors, with a vision that someday researchers could request an unstained slide to analyze with their experimental tools instead of having to develop a mouse model themselves. While clinical tissue banks exist, there are few analogous resources for the preclinical research community.\n\nAnd last but not least, the moonshot. Ke\u2019s team is in the process of developing a machine learning algorithm to automate the diagnosis and prognosis of cancer. The goal is, from histology alone, to be able to predict whether a patient will respond to a certain type of therapy (or be a \u201cnonresponder\u201d). This approach would be cheaper and faster than current methods (genetic sequencing, for example). And this future may be closer than we think! A team at Stanford working on genetics and biomedical informatics recently published an algorithm that can predict non-small cell lung cancer prognosis just from H&E slides (Snyder and Rubin, 2016).\n\nHistoWiz has a number of pathologists on staff to contribute \u201ctagging\u201d (identifying tumor types, tumor margins, etc.) for the database. An online annotation tool is under development so that the data and the analysis can be crowdsourced. Different users will have different levels to help distinguish tags from the in-house pathologists, versus the slide owner, versus the general public. But all in all, the more data, the better to feed the machine learning algorithm. Ke doesn\u2019t expect machine learning to take the place of pathologists for diagnosis any time soon but can see the value of starting with prognosis of preclinical models.\n\nWhile the potential to automate cancer detection would get anyone up in the morning, Ke finds inspiration from the end goal and the artistic beauty of the data itself. The company posts on Twitter \u201cimages of the day,\u201d sometimes with holiday themes (Figure\u00a01-2). If you\u2019d like to get to know Ke or learn more about HistoWiz, sign up to join one of the monthly dinners.\n\n\nFigure 1-2. Left: Clover pattern for St. Patrick\u2019s Day; Right: A heart for Valentine\u2019s Day. Source: Ke Cheng.\n\n\n\n\nContinue reading Cancer detection, one slice at a time.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YV8h61Evo6A/cancer-detection-one-slice-at-a-time"
 },
 {
  "title": "Integrating data with AI",
  "content": "Tamr\u2019s Eliot Knudsen on algorithms that work alongside human experts.As companies have embraced the idea of data-driven management, many have discovered that their hard-won stores of valuable data are badly siloed: separate troves of data live in different parts of the company on separate systems. These data sets may relate to each other in essence, but they\u2019re often difficult to integrate because they differ slightly in schemas and data definitions.\n\n\n\nIn this podcast episode, I speak with Eliot Knudsen, data science lead at Tamr, a company that uses AI to integrate data across silos. Data integration is often a painstaking, highly manual process of matching fields and resolving entities, but new tools can work alongside human experts to discover patterns in data and make recommendations for automatically merging it.\n\n\n\nThe participation of humans in the process is essential. Knudsen points to Andrew Ng\u2019s assessment that \u201cif a typical person can do a mental task with less than two seconds of thought, we can probably automate it using AI either now or in the near future.\u201d AI-driven products that make more complex judgments need to acknowledge their limitations and invite humans into the loop.\n\n\n\nKnudsen cites a Gmail feature as an example: the service launched in 2004 with a sophisticated spam filter that was able to work with a high degree of accuracy by training its model across all of Gmail\u2019s user accounts. A few years later, Gmail introduced another AI-driven feature that\u2019s considerably more complex: a flag that indicates whether you\u2019re likely to think a message is important. Knudsen emphasizes the fact that this feature is only a flag, not a filter like the spam detector.\n\n\n\n\u201cOn the surface, the idea of flagging email as being spam versus not spam, and important versus not important looks very similar,\u201d he says. \u201cBut if you think about what it takes for someone to figure out whether or not an email is actually important and actually deserving of their attention, that\u2019s usually something that\u2014at least for me\u2014takes longer than two seconds. I need to actually go through, I need to think about whether this is an email that I should be responding to at all. It\u2019s unlikely that artificial intelligence is in any near term going to be able to say that an email is important versus not important with a high degree of accuracy.\u201d\n\n\n\nSo, says Knudsen, Google\u2019s response to that challenge shows the \"important flag\" as a recommendation that, unlike a spam filter and more similar to a notification, fits better with the user experience.\n\n\n\nIn the same way, the process of data integration needs to draw on the expertise of human users\u2014whether technical or business managers\u2014who often have deep insight into the ways that their data sets are structured and used. By using AI-driven tools, managers can get the best possible leverage out of the time that humans need to put into the integration process\u2014what Knudsen calls \u201cthe machine-driven, human-guided approach.\u201d\n\n\n\nThis post is a collaboration between O\u2019Reilly and Tamr. See our statement of editorial independence.\nContinue reading Integrating data with AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/qCrPAenUow8/integrating-data-with-ai"
 },
 {
  "title": "Jupyter Insights: Lorena Barba, an associate professor of mechanical and aerospace engineering",
  "content": "Jupyter in education, Jupyter-in-the-loop, and reproducibility in science.Lorena Barba is an associate professor of mechanical and aerospace engineering at George Washington University. She will be speaking at JupyterCon, August 22-25, 2017, in New York City.\n\nBelow, Barba shares her thoughts on the current and future state of Jupyter.\n\n\n \n\n \n1. How has Jupyter changed the way you work?\n\nMy research group is pretty eclectic, with a focus on computational fluid dynamics and computational physics, but with a wide range of activities in education and open source software. My doctoral students and I use Jupyter daily. When any of us is studying a new topic\u2014like, say, a new method for solving the equations of fluid dynamics\u2014the exploration develops and gets recorded in a notebook. When a student is producing some preliminary results from simulations, they are organized for internal discussion in a notebook. Not to mention our educational materials: my students and I co-author many notebooks for teaching and learning. Some become an enhanced form of (free) textbooks for a course, others may support a one-off tutorial, and yet others encapsulate our own learning process with new topics.\n\n\n2. How does Jupyter change the way your team works? How does\u00a0it alter the dynamics of collaboration?\n\nCombined with public hosting on the web and version control (on GitHub, for example), we use Jupyter to loop through our conversations about research. Through an iterative process, we often co-write asynchronously and develop the unpolished ideas that later turn into a research paper. We use \"Jupyter-in-the-loop\" to help us think and organize our work!\n\n\n3. How do you expect Jupyter to be extended in the coming year?\n\nLike everyone, I am eagerly awaiting the arrival of JupyterLab. It will change everything! Adoption in data science will be solid. Users will jump with joy at being able to read in large data files and manipulate them without slow-down. In the educational realm, I\u2019m looking forward to the ecosystem\u2014with tools like nbgrader and nbtutor\u2014becoming easier to use and better integrated. We have brilliant people developing these tools, and I just hope their future employers will continue to support their open source contributions.\n\n\n4. What will you be talking about at JupyterCon?\n\nMy talk is titled Design for Reproducibility. In recent years, the debates about reproducibility in science have really heated up. Often, the concerns include open sharing of code and data that helped arrive at a new finding. This is still controversial in some fields, believe it or not. Jupyter is promoted as a solution for creating reproducible computational narratives. Some see it as a means of putting into practice Knuth\u2019s idea of literate programming, where code is directly annotated with comprehensible documentation. But here is one seeming contradiction: interactive tools were seen by the pioneers of reproducible research as the antithesis of reproducibility. What makes Jupyter different than, say, the dreaded spreadsheet? My talk will explore how we build into the\u00a0design\u00a0of our tools (like Jupyter) an\u00a0enabling capacity to support reproducible research.\n\n\nI also want to tell you about a Birds-of-a-Feather informal session that I\u2019m co-leading with Robert Talbert:\u00a0\"Jupyter for Teaching & Learning,\" on Thursday, August 25, 2017, at 7 p.m. We want to connect educators using Jupyter to share know-how and put our heads together to articulate the needs of teachers and learners to the Jupyter team. Anyone can join us! (Registration is at\u00a0bit.ly/jupyter-ed-bof.)\n\n\n5. What sessions are you looking forward to seeing at JupyterCon?\n\n\nThe conference turned out to have big focus areas in both education and reproducible research, which are of great interest to me. I\u2019m really looking forward to hearing about the large deployments of JupyterHub in educational initiatives with hundreds of students. I\u2019m also eager to learn more about how Jupyter is being used in contemporary AI work.\nContinue reading Jupyter Insights: Lorena Barba, an associate professor of mechanical and aerospace engineering.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VnVQVRCszTY/jupyter-insights-lorena-barba-an-associate-professor-of-mechanical-and-aerospace-engineering"
 },
 {
  "title": "Four short links: 8 August 2017",
  "content": "Better Bloom Filter, AI Future, Adversarial Benchmarking, and Civilized Discourse\u200b\n\nA General-Purpose Counting Filter: Making Every Bit Count -- like Bloom filters, but faster, resizable, you can delete items, and more. (via Paper a Day)\n\nJeff Dean's AI Lecture for YC -- not just a good intro to modern AI, but a glimpse at how Google sees the future playing out.\n\nCleverHans -- A library for benchmarking vulnerability to adversarial examples.\n\n\nCivilized Discourse: But How? (Jeff Atwood) -- You have to have those rules because if no one can say what it is you stand for, then you don't stand for anything. Anything is now fair game, technically. \n\n\nContinue reading Four short links: 8 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/M4Cakvug_58/four-short-links-8-august-2017"
 },
 {
  "title": "Why continuous learning is key to AI",
  "content": "A look ahead at the tools and methods for learning from sparse feedback.As more companies begin to experiment with and deploy machine learning in different settings, it\u2019s good to look ahead at what future systems might look like. Today, the typical sequence is to gather data, learn some underlying structure, and deploy an algorithm that systematically captures what you\u2019ve learned. Gathering, preparing, and enriching the right data\u2014particularly training data\u2014is essential and remains a key bottleneck among companies wanting to use machine learning.\n\nI take for granted that future AI systems will rely on continuous learning as opposed to algorithms that are trained offline. Humans learn this way, and AI systems will increasingly have the capacity to do the same. Imagine visiting an office for the first time and tripping over an obstacle. The very next time you visit that scene\u2014perhaps just a few minutes later\u2014you\u2019ll most likely know to look out for the object that tripped you.\n\nThere are many applications and scenarios where learning takes on a similar exploratory nature. Think of an agent interacting with an environment while trying to learn what actions to take and which ones to avoid in order to complete some preassigned task. We\u2019ve already seen glimpses of this with recent applications of reinforcement learning (RL). In RL, the goal is to learn how to map observations and measurements to a set of actions, while trying to maximize some long-term reward. (The term RL is frequently used to describe both a class of problems and a set of algorithms.) While deep learning gets more media attention, there are many interesting recent developments in RL that are well known within AI circles. Researchers have recently applied RL to game play, robotics, autonomous vehicles, dialog systems, text summarization, education and training, and energy utilization.\n\n\nFigure 1. Reinforcement learning involves learning mappings of measurements and observations into actions. Source: Ben Lorica.\n\n\nJust as deep learning is slowly becoming part of a data scientist\u2019s toolset, something similar is poised to happen with continuous learning. But in order for data scientists to engage, both the tools and algorithms need to become more accessible. A new set of tools and algorithms\u2014different from the ones used for supervised learning\u2014will be required. Continuous learning will require a collection of tools that can run and analyze massive numbers of simulations involving complex computation graphs, ideally with very low-latency response times.\n\n\nFigure 2. Typical set of tools (or \u201cstack\u201d) for continuous learning. Source: Ben Lorica.\n\n\nA team at UC Berkeley's RISE Lab recently released an open source distributed computation framework (Ray) that complements the other pieces needed for reinforcement learning. In complex applications\u2014like self-driving cars\u2014multiple sensors and measurements are involved, so being able explore and run simulations very quickly and in parallel provides a big advantage. Ray allows users to run simulations in parallel and comes with a Python API that makes it accessible for data scientists (Ray itself is written mainly in C++). While I\u2019m writing about Ray in the context of RL, it\u2019s more generally a fault-tolerant, distributed computation framework aimed at Python users. Its creators have made it simple for others to use Python to write and run their own algorithms on top of Ray, including regular machine learning models.\n\nWhy do you need a machine learning library and what algorithms are important for continuous learning? Recall that in RL one needs to learn how to map observations and measurements to a set of actions, while trying to maximize some long-term reward. Recent RL success stories mainly use gradient-based deep learning for this, but researchers have found that other optimization strategies such as evolution can be helpful. Unlike supervised learning where you start with training data and a target objective, in RL one only has sparse feedback, so techniques like neuroevolution become competitive with classic gradient descent. There are also other related algorithms that might become part of the standard collection of models used for continuous learning (e.g., counterfactual regret minimization used recently for poker). The creators of Ray are in the process of assembling a library that implements a common set of RL algorithms and makes them available via a simple Python API.\n\nMost companies are still in the process of learning how to use and deploy standard (offline) machine learning, so perhaps discussing continuous learning is premature. An important reason to begin this discussion is that these techniques are going to be essential to bringing AI into your organization. As with any other new method or technology, the starting point is to identify uses cases where continuous learning potentially provides an advantage over existing offline approaches. I provided a few examples where RL has been deployed or where research has indicated promising results, but those examples might be far removed from your organization\u2019s operations. The set of companies already using bandit algorithms (to recommend content or evaluate products) can probably quickly identify use cases and become early adopters. Technologies used to develop AI teaching agents might map to many other application domains that involve augmenting human workers (including software engineering).\n\nCompanies are realizing that in many settings machine learning models start degrading soon after they get deployed to production. The good news is that many AI startups are building continuous learning right into their products. Before you know it, your company might start using RL in the very near future.\n\nRelated resources:\n\n\n\t\nRay: A distributed execution framework for emerging AI applications (2017 Strata Data keynote by Michael Jordan)\n\t\nDeep reinforcement learning for robotics (2016 Artificial Intelligence Conference presentation by Pieter Abbeel)\n\t\nCars that coordinate with people (2017 Artificial Intelligence Conference keynote by Anca Dragan)\n\tIntroduction to reinforcement learning and OpenAI Gym\n\tNeuroevolution: A different kind of deep learning\nReinforcement learning explained\n\nContinue reading Why continuous learning is key to AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/UE8kg55LQxI/why-continuous-learning-is-key-to-ai"
 },
 {
  "title": "Four short links: 7 August 2017",
  "content": "Social Agents, Computational Zoom, Living Memory, and AI Policy\n\nProm Week Meets Skyrim: Developing a Social Agent Architecture in a Commercial Game -- This project\u2019s goal was to develop and implement a social architecture model, inspired in academic research, in a modern and commercially successful video game and investigate its impact on player experience. We choose to implement the social architecture in a popular Role Playing video game: \u201cThe Elder Scrolls V: Skyrim,\u201d due to its popularity and high \u201cmod-ability.\u201d\n\n\nComputational Zoom: A Framework for Post-Capture Image Composition -- we introduce computational zoom, a framework that allows a photographer to manipulate several aspects of composition in post-processing from a stack of pictures captured at diff erent distances from the scene. We further define a multi-perspective camera model that can generate compositions that are not physically attainable, thus extending the photographer\u2019s control over factors such as the relative size of objects at different depths and the sense of depth of the picture. We show several applications and results of the proposed computational zoom framework.\n\n\nSDF Projects -- zomg there's an authentic TOPS-20 system you can get logins on.  For a taste of the Old Days.\n\nISOC's AI and ML Policy Paper -- The paper explains the basics of the technology behind AI, identifies the key considerations and challenges surrounding the technology, and provides several high-level principles and recommendations to follow when dealing with the technology.\n\n\nContinue reading Four short links: 7 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3U830oHm090/four-short-links-7-august-2017"
 },
 {
  "title": "How to move your team closer to clarity",
  "content": "Design thinking, the Lean approach, and Agile software development can make a difference in your teams.Despite its title, this book is really about ability, learning, and adapting. Design thinking, Lean, and Agile are mindsets that, when practiced, help organizations develop new competencies. We learn to tackle problems and explore possibilities. We strive to make every action a learning opportunity for making better decisions. We put learning to work as we pursue outcomes in a way that\u2019s optimized for adapting to constant change. More than following steps, procedures, or instructions, this report describes the mindsets and ways of working that help teams to think differently, practice new skills, and develop new abilities.\n\nPopular culture depicts designers as precious snowflakes. In the movies, developers are socially inept propeller heads. And we all like to joke about bean-counting middle managers and executives who are asleep at the wheel. All of them are petrified of being disrupted by Silicon Valley\u2019s hoodie-wearing startups.\n\nConvention is dead in the modern age\u2014there are no rules, and job roles are so last century. It\u2019s no wonder people are so confused about how to do software better.\n\nThese are exaggerated generalizations, I know, but they do paint a picture of the mess we face when working together to build stuff that matters. Creating digital products and services is a pursuit that requires collaboration across many disciplines. Technology, design, and business all have their own kinds of architects. Strategy has different flavors, from corporate to customer to technology. Products require design, software is engineered, and someone needs to run the entire operation.\n\nDesign thinking, Lean, and Agile are prominent mindsets among product development teams today. Each mindset brings its own kind of value to the product development life cycle (see Figure 1). And although they come from different origins\u2014industrial design, manufacturing, and software development\u2014they share many similarities and are complementary and compatible with each other.\n\n\nFigure 1. Design thinking, Lean, and Agile.\n\n\n\nAt a distance, design thinking is a mindset for exploring complex problems or finding opportunities in a world full of uncertainty. It\u2019s a search for meaning, usually focusing on human needs and experience. Using intuitive and abductive reasoning, design thinking explores and questions what is, and then imagines what could be with innovative and inventive future solutions.\n\nThe Lean mindset is a management philosophy that embraces scientific thinking to explore how right our beliefs and assumptions are while improving a system. Lean practitioners use the deliberate practice of testing their hypotheses through action, observing what actually happens, and making adjustments based on the differences observed. It\u2019s how organizations set their course, learn by doing, and decide what to do next on their journey to achieve outcomes.\n\nThe heart of Agile is building great software solutions that adapt gracefully to changing needs. Agile begins with a problem\u2014not a requirement\u2014and delivers an elegant solution. The Agile mindset acknowledges that the right solution today might not be the right solution tomorrow. It\u2019s rapid, iterative, easily adapted, and focused on quality through continuous improvement.\n\nAlthough the strengths of each mindset come to bear more in some areas than others, no single mindset claims exclusivity over any particular activity. Too often, people ask, \u201cLean or Agile or design thinking?\u201d The answer is \u201cand,\u201d not \u201cor.\u201d\n\nIn this book, we take a detailed look at the origins of each mindset, their strengths, and how they all fit together. Then, we explore how to bring it all together in practice to define actionable strategies, act to learn, lead teams to win, and deliver software solutions.\nContinue reading How to move your team closer to clarity.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9dORxtW8idw/how-to-move-your-team-closer-to-clarity"
 },
 {
  "title": "Four short links: 4 August 2017",
  "content": "Chinese Spamsorship, History of Intelligence, Patreon Numbers, and Javascript WTFs\n\nQuackspeak Ascendant (Cory Doctorow) -- China's approach to networked control is one of three dominant strategies used in the world: in Russia, they fill the channel with an untanglable mess of lies and truth, leading people to give up on understanding the situation; for the west's alt-right trolls, the strategy is to be so outrageous that you get picked up and retransmitted by every channel, which lets you reach the tiny minority of otherwise too-thin-spread, broken people and recruit them to your cause; and in China, it's quackspeak, this anodyne, happy-talk-style chatter about nothing much.\n\n\nIntelligence: A History -- If we\u2019ve absorbed the idea that the more intelligent can colonize the less intelligent as a right, then it\u2019s natural that we\u2019d fear enslavement by our super-smart creations. If we justify our own positions of power and prosperity by virtue of our intellect, it\u2019s understandable that we see superior AI as an existential threat. (via Sam Kinsley)\n\nInside Patreon -- Half its patrons and creators joined in the past year, and it\u2019s set to process $150 million in 2017, compared to $100 million total over the past three years.\n\n\nWTFJS -- A list of funny and tricky examples of JavaScript.\n\n\nContinue reading Four short links: 4 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/IySuQA8T3IY/four-short-links-4-august-2017"
 },
 {
  "title": "JupyterHub on Google Cloud",
  "content": "A step-by-step tutorial on how to install and run JupyterHub on gcloud.\n    \n      \n      \n    \n    \nDeploying JupyterHub on Kubernetes on Google Cloud\n\nJupyterHub, a \"multi-user server for Jupyter Notebooks,\" is an essential tool for teaching and training at scale with Jupyter.  As described in The course of the future \u2013 and the technology behind it\n, JupyterHub is being used to power an introductory class in data science taken by hundreds of students at Berkeley every semester.\nJupyterHub is a complex piece of software, and setting up and operating has been out of reach for many organizations, but recent work by members of the Jupyter team - especially @CarolWilling, @choldgraf, @Mbussonn,  @minrk, and  @yuvipanda -- has put JupyterHub within reach of a host organizations and individuals.  \nTheir new project, a Helm package for JupyterHub and an accompanying article called Zero to JupyterHub on how to use it, describes the relatively straightforward steps needed to install and run JupyterHub on Google cloud.\nIn this article, I've followed along with the tutorial, adding additional detail on setting up gcloud, preparing a docker image with the content project you want to deploy in it, and provided more background on some of the tools used.\n\nIntroduction\n\nAlthough there are a lot of steps, there are three core things to do:\n\nCreate a k8s cluster on gcloud.  In a traditional ops setting, this is kind of like setting up a new server.\nInstall the JupyterHub application on the cluster using Helm, the k8s package manager.\nConfigure the new JupyterHub instance to serve a default content project.  In this case, I'll have it serve Allen Downey's ThinkDSP project.  See Computational Publishing with Jupyter for more background information on this step.\n\nBy the end of this tutorial, you'll have a public server (no DNS entry or https, which is something I'll need to figure out how to add) where anyone can log in and get a running instance of ThinkDSP.\nWhat's not covered here:\n\nHow to set up an \"authenticator\" for JupyterHub so that you can control who can log into Jupyter and get a notebook.  Right now, anyone can just log in with any username and password.  Probably unwise.\nHow to handle persistent storage for the container.  This is dependent on an unmerged issue on docker-stacks project.  So, you won't be able to save the notebooks served this instance of JupyterHub, but that should be fixed soon.\n\n\n\nInstall the gcloud CLI\n\nYou need the google cloud CLI for your platform.  Once you download and install it, you should be able to run the gcloud tool on your machine:\n$ gcloud version\nGoogle Cloud SDK 155.0.0\nbq 2.0.24\ncore 2017.05.10\ngcloud\ngsutil 4.26\n\n\nSet your login credentials\n\nOnce you've got it installed, do gcloud init to set your credentials.  This will open a browser and ask you login to the google account where you'll be deploying JupyterHub.\n\n\nInstall kubectl plugin\n\nInstall kubectl:\ngcloud components install kubectl\n\nYou only need to do this once.\n\n\nCreate a Kubernetes cluster\n\nOnce you've got the basic setup, you're ready to create a kubernetes cluster where we'll install the JupyterHub server.  In an AWS context, this is kind of like setting up your EC2 instance.\nI set up a cluster named notebook-test consisting of 3 high memory machine types with 2 virtual CPUs and 13 GB of memory operating in the US central zone.  \ngcloud container clusters create notebook-test \\\n    --num-nodes=3 \\\n    --machine-type=n1-highmem-2 \\\n    --zone=us-central1-b\n\nOnce the command completes, you can confirm your cluster is running like this:\n$ kubectl get node\nNAME                                           STATUS    AGE       VERSION\ngke-notebook-test-default-pool-fc9a005b-64z2   Ready     41s       v1.6.4\ngke-notebook-test-default-pool-fc9a005b-rjhm   Ready     43s       v1.6.4\ngke-notebook-test-default-pool-fc9a005b-tj84   Ready     44s       v1.6.4\n\n\nSet up Helm\n\nHelm is the package manager for Kubernetes; it's like apt/yum/homebrew for a cluster.  It's used to install charts, which are are packages of pre-configured Kubernetes resources.  \n$ brew install kubernetes-helm\n==> Downloading https://homebrew.bintray.com/bottles/kubernetes-helm-2.4.2.sierr\n######################################################################## 100.0%\n==> Pouring kubernetes-helm-2.4.2.sierra.bottle.tar.gz\n==> Using the sandbox\n==> Caveats\nBash completion has been installed to:\n  /usr/local/etc/bash_completion.d\n==> Summary\n\u07cd\ua820/usr/local/Cellar/kubernetes-helm/2.4.2: 48 files, 122.4MB\n\nThen you have to run helm init; this has to be done once per k8s cluster.\n$ helm init\nCreating /Users/odewahn/.helm\nCreating /Users/odewahn/.helm/repository\nCreating /Users/odewahn/.helm/repository/cache\nCreating /Users/odewahn/.helm/repository/local\nCreating /Users/odewahn/.helm/plugins\nCreating /Users/odewahn/.helm/starters\nCreating /Users/odewahn/.helm/repository/repositories.yaml\n$HELM_HOME has been configured at /Users/odewahn/.helm.\n\nTiller (the helm server side component) has been installed into your Kubernetes Cluster.\nHappy Helming!\n\n\nPrepare an initial JupyterHub config file\n\nNow we're ready to set up JupyterHub itself.  In a AWS context, this is like we've got the server stood up and now we want to put our application on it.\nGenerate two random keys that you'll using the in the config file:\n$ openssl rand -hex 32\nb85e9538b93761f2336025a3d5696cc237ee26c8115979d90b86b08b0c326957\n$ openssl rand -hex 32\nf13056563eafb75ab062020dadef8c941b18f69da623e8af58554c06c585881a\n\nThen create a file called config.yaml with the following contents:\nhub:\n  cookieSecret: \"b85e9538b93761f2336025a3d5696cc237ee26c8115979d90b86b08b0c326957\"\ntoken:\n  proxy: \"f13056563eafb75ab062020dadef8c941b18f69da623e8af58554c06c585881a\"\n\nYou'll need to keep this file around, so be sure to commit it to a GitHub repo. [THIS IS PROBABLY NOT GREAT ADVICE SINCE IT CONTAINS SECRETS, BUT WHAT IS THE BEST WAY TO DO IT?]\n\n\nInstall JupyterHub with Helm\n\nNow that we have helm, we can (finally!) use helm install to put the JupyterHub app on the cluster.  We'll use the config file we created in the previous step, and use the name jupyterhub-test as the name and namespace of the application (this is how Helm keeps up with the apps running on the cluster).\nhelm repo add jupyterhub https://jupyterhub.github.io/helm-chart/\nhelm repo update\nhelm install jupyterhub/jupyterhub \\\n --version=v0.4 \\\n --name=jupyterhub-test \\\n --namespace=jupyterhub-test \\\n -f config.yaml\n\nThis will run for a while.  When it finishes, it will produce some helpful log data, as well as the release notes for the JupyterHub app:\nNAME:   jupyterhub-test\nLAST DEPLOYED: Fri Jun  2 10:29:44 2017\nNAMESPACE: jupyterhub-test\nSTATUS: DEPLOYED\n\nRESOURCES:\n==> v1/PersistentVolumeClaim\nNAME        STATUS   VOLUME                       CAPACITY  ACCESSMODES  STORAGECLASS  AGE\nhub-db-dir  Pending  hub-storage-jupyterhub-test  1s\n\n==> v1/Service\nNAME          CLUSTER-IP     EXTERNAL-IP  PORT(S)       AGE\nhub           10.11.246.154  <none>       8081/TCP      1s\nproxy-api     10.11.240.251  <none>       8001/TCP      1s\nproxy-public  10.11.254.221  <pending>    80:30746/TCP  1s\n\n==> v1/Secret\nNAME        TYPE    DATA  AGE\nhub-secret  Opaque  2     1s\n\n==> v1/ConfigMap\nNAME          DATA  AGE\nhub-config-1  14    1s\n\n==> v1beta1/Deployment\nNAME              DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\nhub-deployment    1        1        1           0          1s\nproxy-deployment  1        1        1           0          1s\n\n==> v1beta1/StorageClass\nNAME                                 TYPE\nsingle-user-storage-jupyterhub-test  kubernetes.io/gce-pd  \nhub-storage-jupyterhub-test          kubernetes.io/gce-pd  \n\n\nNOTES:\nThank you for installing JupyterHub!\n\nYour release is named jupyterhub-test and installed into the namespace jupyterhub-test.\n\nYou can find if the hub and proxy is ready by doing:\n\n kubectl --namespace=jupyterhub-test get pod\n\nand watching for both those pods to be in status 'Ready'.\n\nYou can find the public IP of the JupyterHub by doing:\n\n kubectl --namespace=jupyterhub-test get svc proxy-public\n\nIt might take a few minutes for it to appear!\n\nNote that this is still an alpha release! If you have questions, feel free to\n  1. Come chat with us at https://gitter.im/jupyterhub/jupyterhub\n  2. File issues at https://github.com/jupyterhub/helm-chart/issues\n\nAs you can see in the release notes from the log, it will take a while to for the app to initialize.  Here's the instruction you can run to monitor its progress:\n$ kubectl --namespace=jupyterhub-test get svc proxy-public\nNAME           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nproxy-public   10.11.254.221   <pending>     80:30746/TCP   36s\n\nAfter a minute or two, the external IP field will be populated and the JupyterHub app is available:\n$ kubectl --namespace=jupyterhub-test get svc proxy-public\nNAME           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE\nproxy-public   10.11.254.221   104.155.179.31   80:30746/TCP   4m\n\nThen you can open you browser to http://104.155.179.31 and boom!, Notebooks:\n\nNote that JupyterHub is running with a default dummy authenticator, so you can just enter any username and password. See extending jupyterhub for details on how to set up authentication, which I won't cover here.\n\n\nPrepare Default Notebook to run on JupyterHub\n\nBy default, JupyterHub just gives you a blank Notebook.  However, if you're teaching a class and you want to give your students access to something you've already created, you need to prepare a docker image that will be served by default.\nTo make a Docker image you can deploy onto JupyterHub, you need to ADD the repo to the /home/jovyan directory, and then set the WORKDIR to /home/jovyan.  \nIf you're using Launchbot or otherwise have an existing Dockerfile, you can create a new Dockerfile and call it Dockerfile.prod.  For example:\nFROM jupyter/scipy-notebook:latest\n\nADD . /home/jovyan\n\nWORKDIR /home/jovyan\n\n# Expose the notebook port\nEXPOSE 8888\n\n# Start the notebook server\nCMD jupyter notebook --no-browser --port 8888 --ip=*\n\nOnce you have this, build the image:\ndocker build -f Dockerfile.prod . aodewahn/thinkdsp\n\nThen you'll need to log into the Docker Hub and create an image for it.  Once you do, you can do\ndocker push aodewahn/thinkdsp\n\n\n\n\nDeploying the Default Image to JupyterHub\n\nOnce we've made a default image, we need to update the config.yaml to set our new image as the default content served by JupyterHub.  (This is all covered in the extending jupyterhub article).  \nTo do it, add this section that maps the image to the image we just put on DockerHub:\nsingleuser:\n  storage:\n    type: none\n  image:\n    name: aodewahn/thinkdsp\n    tag: latest\n\nNext, get the release name of the app, which you set up earlier.  Note that if you forget the name, you can use helm to retrieve it:\n$ helm list\nNAME               REVISION    UPDATED                     STATUS      CHART                NAMESPACE      \njupyterhub-test    1           Fri Jun  2 10:29:44 2017    DEPLOYED    jupyterhub-v0.4    jupyterhub-test\n\nThen upgrade the cluster (this is what the doc says is necessary):\nhelm upgrade jupyterhub-test jupyterhub/jupyterhub --version=v0.4 -f config.yaml\n\nYou can run this command until the container is built:\n$  kubectl --namespace=jupyterhub-test get pod\nNAME                                READY     STATUS              RESTARTS   AGE\nhub-deployment-1010241260-x506q     0/1       ContainerCreating   0          57s\nproxy-deployment-2951954964-l94n5   1/1       Running             0          2h\n\nOnce it's done building, you should be able to create a new notebook based on the base image.\n\nNote that for now JupyterHub doesn't support persistent storage with the jupyter-stack images, but they're working on it.\n\n\nDelete the cluster\n\nOnce you're done, delete your cluster in order to stop further billing!\ngcloud container clusters delete notebook-test --zone=us-central1-b\n\n\n\nConclusion\n\nClearly, this is still a pretty technical process.  However, by combining the ease of use of Helm with the cost-effectiveness and scalability of kubernetes on gcloud, running a state-of-the-art JupyterHub deployment is within reach of most small organizations, or even an individual.\nBy removing the pain of installing and operating JupyterHub, this project opens the doors to the classroom of the future to everyone.   \n\n\n    \n      \n        \n        \n\n      \n\n    \n  Continue reading JupyterHub on Google Cloud.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/LPTViYqLV48/jupyterhub-on-gcloud"
 },
 {
  "title": "Why AI and machine learning researchers are beginning to embrace PyTorch",
  "content": "The O\u2019Reilly Data Show Podcast: Soumith Chintala on building a worthy successor to Torch and on deep learning within Facebook.In this episode of the Data Show, I spoke with Soumith Chintala, AI research engineer at Facebook. Among his many research projects, Chintala was part of the team behind DCGAN (Deep Convolutional Generative Adversarial Networks), a widely cited paper that introduced a set of neural network architectures for unsupervised learning. Our conversation centered around PyTorch, the successor to the popular Torch scientific computing framework. PyTorch is a relatively new deep learning framework that is fast becoming popular among researchers. Like Chainer, PyTorch supports dynamic computation graphs, a feature that makes it attractive to researchers and engineers who work with text and time-series.Continue reading Why AI and machine learning researchers are beginning to embrace PyTorch.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5o3MODW50tw/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch"
 },
 {
  "title": "A DevOps approach to data management",
  "content": "A multi-model approach to transforming data from a liability to an asset. Deriving knowledge from data has become a key competency for many\u2014if not most\u2014businesses. With the right data, and the right tools to handle it, businesses can gain keen insights into a variety of metrics, including operations, customer activity, and employee productivity.\n\nIn the free O'Reilly report, Defining Data-Driven Software Development, author Eric Laquer explores the advancements of DevOps and applies those lessons to managing data, addressing the challenges involved in handling business data and examining ways to fulfill the various needs of different stakeholders. Laquer also illustrates how using a multi-model approach allows for a variety of data types and schemas to operate side by side.\n\nUtilizing data in its natural form\n\nMulti-model technology accepts that our data comes in many types. It enables a variety of representational forms and indexing techniques for more creative and effective approaches to problems that might otherwise present major challenges in the confining, single-schema world of tables, rows, and columns.\n\nAs many of us know, today's data does not tend to be homogenous, fitting easily into a relational schema based on rows and tables. Modern applications must process data that includes records, documents, videos, text, and semantic triples in a variety of formats, including XML, JSON, text, and binary. A majority of the data that analysts work with today is document based, with inconsistent schema and metadata from document to document, even within the same collection. Businesses typically have documents that fulfill similar purposes, but are structured differently for historical or other reasons; this is especially true with large organizations that have merged with other organizations over time.\n\nHowever, since documents are not the only type of data that businesses must deal with, pure document-oriented database systems are not the best solution for large-scale data storage. Other options include a semantic triple store or a graph database, to surface useful facts about how entities are connected. In some cases, it may also be necessary to handle certain types of data (i.e., transactional data), using more conventional relational technologies.\n\nAbandoning rigid schemas\n\nFlexibility has become the key to handling all of the different types of data present in organizations today; however, achieving the necessary level of flexibility requires abandoning rigid schema definitions. Without rigid schemas, software developers can examine and work with data as it is, avoiding the common practice of writing application code to munge and reformat data to fit schemas. The ability to deal with data in its \"natural\" form, and mark it up as needed, makes development more nimble.\n\nIn the free report, Laquer examines some of the conflicting needs around data and proposes as a solution a DevOps model of data management, combined with a flexible, multi-model database system. Developers need flexibility and the ability to build out systems with as little administrative overhead as possible. DBAs, analysts, compliance officers, and other stakeholders also have their own needs in relationship to a database system. While new technology can help normalize the interactions between different types of data and the needs of various stakeholders, we also need a new model for data management. The DevOps movement can provide such a model.\n\nTo learn more, download the free O\u2019Reilly report \"Defining Data-Driven Software Development.\"\n\nThis post is a collaboration between O'Reilly and MarkLogic. See our statement of editorial independence.\nContinue reading A DevOps approach to data management.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/JSSSlIidGPQ/a-devops-approach-to-data-management"
 },
 {
  "title": "Four short links: 3 August 2017",
  "content": "Pricing Unicorns, Software Failures, Finding Prices, and Simulating Poverty\n\nHow Unicorns Are Made -- One provision frequently afforded to investors is called a liquidation preference. It guarantees a minimum payout in the event of an acquisition or other exit. The study found that it can exaggerate a company\u2019s valuation by as much as 94%.  [...] Ratchets can inflate a startup\u2019s value by 56% or more, the study said. The study is also interesting reading.\n\nIRIX 5.1 is a Disappointment -- leaked memo from the early '90s that rings true today.\n\nFinding the Right Price for Early Customers -- Setting an artificially high price, then waiting for the customer to wince (i.e., reject the price), forces an interested customer to negotiate it down. Each negotiation down will get you closer to the maximum price you can expect to charge, whereas always getting a \"yes\" will not tell you if you could charge more.\n\n\nPoverty Pick-A-Path -- turn-based game that challenges you to choose between options as a person experiencing poverty. I'm a fan of simulations that help you \"experience\" the difficulty of situations like balancing the budget, or (in this case) navigating a family downturn. Spoiler: UBI would be nice.\n\nContinue reading Four short links: 3 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BgMGqvBHiKk/four-short-links-3-august-2017"
 },
 {
  "title": "Declaring variables in Kotlin",
  "content": "Learn the difference between mutable and immutable variables and how to cut down on boilerplate code.Continue reading Declaring variables in Kotlin.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-n1eGscYhn8/declaring-variables-in-kotlin"
 },
 {
  "title": "Building\u2014and scaling\u2014a reliable distributed architecture",
  "content": "Five questions for Joseph Breuer and Robert Reta on managing dependencies, building for adaptability, and managing through change.I recently asked Joseph Breuer and Robert Reta, both Senior Software Engineers at Netflix, to discuss what they have learned through implementing a service at scale at Netflix. Joseph and Robert will be presenting a session on Event Sourcing at Global Scale at Netflix at O\u2019Reilly Velocity Conference, taking place October 1-4 in New York. Here are some highlights from our conversation.\nWhat were some of the obstacles you faced while implementing at scale?\nThe primary challenge when operating a service in a distributed architecture at scale is managing for the behavior of your downstream dependencies. Whether those dependencies are a datastore or a restful API defining timeouts, fallback data, and concurrency of the interactions will be the defining factor of your service. Your service may scale wonderfully, but if a dependency is not accounted for then the overall service can quickly fall over.Continue reading Building\u2014and scaling\u2014a reliable distributed architecture.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jOc8AtBaY-4/building-and-scaling-a-reliable-distributed-architecture"
 },
 {
  "title": "Operationalizing security risk",
  "content": "Bruce Potter on why and how to build a risk assessment program.Continue reading Operationalizing security risk.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/URSQ1Zu3PpE/operationalizing-security-risk"
 },
 {
  "title": "Reinforcement learning for complex goals, using TensorFlow",
  "content": "How to build a class of RL agents using a TensorFlow notebook.Reinforcement learning (RL) is about training agents to complete tasks. We typically think of this as being able to accomplish some goal. Take, for example, a robot we might want to train to open a door. Reinforcement learning can be used as a framework for teaching the robot to open the door by allowing it to learn from trial and error. But what if we are interested in having our agent solve not just one goal, but a set that might vary over time?\n\nIn this article, and the accompanying notebook available on GitHub, I am going to introduce and walk through both the traditional reinforcement learning paradigm in machine learning as well as a new and emerging paradigm for extending reinforcement learning to allow for complex goals that vary over time.\n\nI will start by demonstrating how to build a simple Q-learning agent that is guided by a single reward signal to navigate an environment and make deliveries. I will then demonstrate how this simple formulation becomes problematic for more complex behavior we might envision. To allow for greater flexibility, I will then describe how to build a class of reinforcement learning agents, which can optimize for various goals called \u201cdirect future prediction\u201d (DFP). All the code is available in TensorFlow in this accompanying iPython Jupyter Notebook.\n\nQ-learning\u2014for the greatest cumulative reward\n\nReinforcement learning involves agents interacting in some environment to maximize obtained rewards over time. This is typically formalized in the following way: an agent receives a state \\((s)\\) from the environment, and produces an action \\((a)\\). Given this state and action pair, the environment then provides the agent a new state \\((s\u2019)\\) and a reward \\((r)\\). The reinforcement learning problem is then to discover a mapping from states to actions, which produce the greatest amount of cumulative reward.\n\nOne approach to this is called Q-learning, in which we learn a direct mapping between state and action pairs \\((s, a)\\) and value estimations \\((v)\\). This value estimation should correspond to the discounted expected reward over time from taking action \\((a)\\) while in state \\((s)\\). Using the Bellman equation, we can iteratively update our estimations of \\(Q(s, a)\\) for all possible state action pairs. This capacity to iteratively update the Q-value comes from the following property of an optimal Q-function:\n\n\\(Q*(s, a) = r + \u03b3max_{a'}Q(s\u2019, a')\\)\n\nThe above means that the current Q-value for a given state and action can be decomposed into the current reward plus the discounted expected future reward in the next state. By collecting experiences, we can train a neural network to predict more accurate Q-values over time, and by taking actions that optimize the expected value, we can then, ideally, obtain the greatest cumulative reward possible from the environment. Using a universal function approximator such as a neural network, we can generalize our Q-estimation to unseen states, allowing us to learn Q-functions for arbitrarily large state spaces.\n\nDelivery drone scenario for goal-based RL\n\nQ-learning and other traditionally formulated reinforcement learning algorithms learn a single reward signal, and as such, can only pursue a single \u201cgoal\u201d at a time. Take, for example, a situation in which we would like a drone to learn to deliver packages to various locations around a city. This \u201cdelivery drone\u201d scenario will be our guiding paradigm in the discussion of goal-based RL going forward.\n\nIn this environment, the agent occupies a position in a 5x5 grid, and the delivery destination occupies another position. The agent can move in any of four directions (up, down, left, right). If we want our drone to learn to deliver packages, we simply provide a positive reward of +1 for successfully flying to a marked location and making a delivery.\n\n\nFigure 1. Rendering of simple drone delivery environment. Credit: Arthur Juliani.\n\n\nWhile the rendering in Figure 1 will be used to give an intuition of what our agent is learning, we will use an even simpler state representation, a 5x5 grid, of RGB pixels (75 values in total) to represent the environment. This will speed up the learning process from hours to minutes on a modern desktop computer. Each episode will last 100 steps, and the agent and delivery locations will be randomized at the beginning of each episode.\n\nQ-learning with TensorFlow\n\nThe TensorFlow implementation of Q-learning shown below is an asynchronous version of the algorithm, which allows for multiple agents to work in parallel to learn a policy. This both speeds up and increases the robustness of the training process. This implementation is in the Jupyter Notebook here.\n\nTraining on a machine with four workers, after 6,000 training episodes per worker, we end up with a performance curve that should look something like the graph in Figure 2. Our agent can consistently deliver about 20 packages per episode. This can be considered something close to the optimal number of deliveries possible within 100 steps, given the size of the environment. For an animated version, look here.\n\n\nFigure 2. Performance curve, after training on machine with four workers and 6,000 training episodes per worker. Credit: Arthur Juliani.\n\n\nDirect future prediction\n\nA real-world drone wouldn\u2019t be able to endlessly deliver packages. It would likely have a limited battery capacity and would need to be occasionally re-charged. With every movement, the agent\u2019s battery charge will decrease slightly. With this finite battery capacity, running out of a charge means falling out of the sky and no longer being able to deliver packages\u2014and no more packages means no more reward.\n\nWe can augment the environment with a location our agent can fly toward to recharge its battery. Now all the agent needs to do is to learn to fly to the recharge station when the battery is low and otherwise deliver packages as normal.\n\nCreating an optimal reward function\n\nGiven enough time, and the correct hyperparameter tuning, the Q-learning algorithm could eventually discover that recharging the battery is beneficial to delivering more packages in the long-term. This involves learning to take a complex series of actions for no immediate reward, in the understanding that a greater reward will come later. In this case, it is a tempting option to augment the reward signal to encourage battery-rewarding behavior. A na\u00efve approach might be to provide a reward (say +0.5) for flying to the specified location to recharge the battery. What our agent will learn in this situation, however, is to simply always fly to the recharge station, since there is a guaranteed reward every time it charges itself. What we run into now is the problem of coming up with a reward function that describes the kind of behavior we believe to be optimal. While this is simple for some problems, there are often unintended consequences of posing the reward function incorrectly. For some examples, see this recent OpenAI article, \u201cFaulty Reward Functions.\u201d\n\nChanging the goal\n\nIf we want to avoid the pitfalls of bad reward engineering, we need a more intuitive way to convey the structure of the task to our agent. It turns out that by providing our agent with an explicit goal that can vary based on both the episode and specific moment, we are better able to get the kinds of dynamic behaviors we want. In the case of the battery, we can simply change the goal from \u201cdeliver packages\u201d to \u201crecharge battery\u201d once the battery is below a certain capacity. In this way, we don\u2019t have to worry about reward formulation, and the neural network can just focus on learning the environment dynamics themselves.\n\n\nFigure 3. Drone delivery environment with battery. Credit: Arthur Juliani.\n\n\nFormalizing goal-seeking\n\nTo make this concept usable, we need to make the description above a little more formal. There are multiple ways to formalize goal-seeking in RL; the one I will be following is from a recently introduced paper that was presented at this year\u2019s International Conference on Learning Representations. The paper is called \u201cLearning to Act by Predicting the Future,\u201d and that is just what we will be training our agent to do!\n\nFirst, a slight disclaimer that the model presented here won\u2019t be a direct implementation of what the authors, Alexey Dosovitskiy and Vladlen Koltun, describe in their paper. In the paper, they refer to their network as \u201cdirect future prediction\u201d (DFP). We will be making something more akin to a DFP-lite. I have adjusted a few elements to make them more intuitive for the simple example discussed in this article.\n\nIn the original paper, the authors train their agent to play the first-person shooter game \u201cDoom\u201d competitively, something very impressive but more complex than the scope of this introduction.\n\n\nFigure 4. Diagram of a \u201cdirect future prediction\u201d network from Dosovitskiy and Koltun (2016), used with permission.\n\n\nInstead of training our agent to map a state \\((s)\\) to a Q-value estimate \\(Q(s, a)\\) and then receive a reward \\((r)\\) from the environment, we maintain a set of measurements \\((m)\\) and goals \\((g)\\) in addition to our state \\((s)\\), and train the network to predict the future changes in measurements \\((f)\\) for each action \\((a)\\).\n\nTraining our network to predict the expected future\n\nIn our delivery-drone scenario, the two measurements we will maintain are battery charge and number of packages delivered. Instead of predicting a value function like in Q-learning, we train our network to predict the expected future changes in battery and deliveries at 1, 2, 4, 8, 16, and 32 steps into the future. Formally this can be written as:\n\n\\(f = <m_{T1} \u2013 m_0, m_{T2} \u2013 m_0\u2026 m_{Tn} \u2013 m_0>\\)\n\nWhere T is our list of temporal offsets, [1, 2, 4, ...etc.].\n\nIn this paradigm, there are no longer explicit rewards; instead, success is measured by how well the goals and measurements align. In the case of the delivery drone, this would mean maximizing deliveries and ensuring the battery is charged when low.\n\nIf our agent were perfect at predicting the future measurements for each of these actions, we would simply need to take the action that optimized the measurements we were interested in. Our goals allow us to specify which measurements we care about at any given time.\n\nFormulating more complex goals\n\nSince we aren\u2019t simply predicting a single scalar value estimate like in Q-learning, we can formulate more complex goals. Imagine that we have a measurement vector [battery, deliveries]. If we want to maximize battery charge but ignore deliveries, then our goal would be [1 , 0], which would correspond to desiring a positive battery measurement in the future and indifference to the number of deliveries.\n\nIf we wanted to maximize deliveries, our goal would be [0, 1]. Since we (rather than the environment or the network itself) formulate the goal, we are free to change it at every time-step as we see fit. In this way, we can explicitly change the goal from optimizing deliveries to optimizing the battery, whenever the battery measurement drops below a certain threshold (in our case, 30% charge). By combining goals and measurement in this way, we now can flexibly adjust our agent\u2019s behavior on the fly, depending on the desire of the human operator. This is in contrast to Q-learning, where the Q-values would remain fixed after training, and only a single behavior pattern would be possible.\n\nThis new formulation changes our neural network in several ways. Instead of just a state, we will also provide as input to the network the current measurements and goal. Instead of Q-values, our network will now output a prediction tensor of the form [Measurements X Actions X Offsets]. Taking the product of the summed predicted future changes and our goals, we can pick actions that best satisfy our goals over time:\n\n\\(a = g^T*\u2211p(s, m, g)\\)\n\nWhere \\(\u2211p(s, m, g)\\) is the output of the network summed over future timesteps, and is the transpose of the goal vector.\n\nWe can train this new kind of agent using a simple regression loss to predict the true measurement changes into the future:\n\n\\(Loss = \u2211[P(s, m, g, a)-f(m)]^2\\)\n\nWhere \\(P(s, m, g, a)\\) refers to the output of the network for the chosen action .\n\nWhen we put all this together, we have an agent that can skillfully navigate our package delivery scenario, all the while maintaining a charged battery. We will again be using an asynchronous implementation in TensorFlow and the notebook with the model that is available here.\n\nAsynchronous implementation in TensorFlow\n\nAfter 10,000 episodes of training per worker (with four workers), we end up with training curves like those in Figure 5. The agent has learned to keep its battery charged (as indicated by average episode length approaching 100 steps) as well as to deliver the near-optimal number of packages within a single episode. Click here for an animated version.\n\n\nFigure 5. Number of deliveries (left) and steps in each episode (right) over time, as measured in episodes. Credit: Arthur Juliani.\n\n\nIt is best to keep in mind that the environment provided here is a greatly simplified version of what might exist in a real-world scenario. A grid-world was used to allow the network to train in a reasonable time to demonstrate goal learning. A real-world drone would likely utilize smooth, continuous control in what is obviously a more visually complex world.\n\nAdvancing this technique\n\nIf you have access to more powerful computing resources, I encourage you to try the technique with more complex environments, such as those provided by the OpenAI universe. A simple change that could be made is to provide a better encoder for the visual stream by using convolutional layers instead of the fully connected ones used here. The basic architecture, however, should be extensible.\n\nI hope this tutorial has provided an insight into the kinds of problems that reinforcement learning can solve and the benefits that can come from reformulating tasks in new contexts. The multi-goal approach described here may not be the best for all situations, but it provides another possible avenue for designing agents to solve complex tasks in the world.\n\nThis post is a collaboration between O'Reilly and TensorFlow. See our statement of editorial independence.\nContinue reading Reinforcement learning for complex goals, using TensorFlow.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vHJ_Rn0N3ac/reinforcement-learning-for-complex-goals-using-tensorflow"
 },
 {
  "title": "Jay Jacobs on data analytics and security",
  "content": "The O\u2019Reilly Security Podcast: The prevalence of convenient data, first steps toward a security data analytics program, and effective data visualization.In this episode of the Security Podcast, Courtney Nash, former chair of O\u2019Reilly Security conference, talks with Jay Jacobs, senior data scientist at BitSight. We discuss the constraints of convenient data, the simple first steps toward building a basic security data analytics program, and effective data visualizations.Continue reading Jay Jacobs on data analytics and security.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-M9dzO6qvZU/jay-jacobs-on-data-analytics-and-security"
 },
 {
  "title": "Four short links: 2 August 2017",
  "content": "App Size, Decision-Making, Headless Chrome, and Scientific Significance\n\nApp Sizes Are Out of Control -- amen!  I'm downloading a CD of data a day just for app updates.\n\nMake Great Decisions -- useful technique.\n\nChromeless -- Chrome automation made simple. Runs locally or headless on AWS Lambda.  Chromeless can be used to: run 1000s of browser integration tests in parallel; crawl the web and automate screenshots; write bots that require a real browser; do pretty much everything you've used PhantomJS, NightmareJS, or Selenium for before.\n\n\nMoving the Goalposts? (Thomas Lumley) -- responding to a paper proposing to make the threshold for scientific statistical significance be 10x harder to clear: The problem isn\u2019t the threshold so much as the really weak data in a lot of research, especially small-sample experimental research [large-sample observational research has different problems].  Larger sample sizes or better experimental designs would actually reduce the error rate; moving the threshold only swaps which kind of error you make. This isn't all of Thomas' thesis, but it's a good point.\n\nContinue reading Four short links: 2 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Vn1xxaQQ5wM/four-short-links-2-august-2017"
 },
 {
  "title": "The wisdom hierarchy: From signals to artificial intelligence and beyond",
  "content": "A framework for moving from data to wisdom.We are swimming in data. Or possibly drowning in it. Organizations and individuals are generating and storing data at a phenomenal and ever-increasing rate. The volume and speed of data collection has given rise to a host of new technologies and new roles focused on dealing with this data, managing it, organizing it, storing it.\n\nBut we don\u2019t want data. We want insight and value.\n\nWe can think about these new technologies and roles, and the way they help us move from data to insight and value, through the lens of something called the wisdom hierarchy. The wisdom hierarchy is a conceptual framework for thinking about how the raw inputs of reality (signals) are stored as data and transformed first into information, then into knowledge, and finally into wisdom. We might call these steps in the hierarchy \u201clevels of insight.\u201d\n\nTo illustrate, starting with the input of specific error codes coming out of a software system, we can draw out an example of each step in the hierarchy:\n\nData: All the error logs from that system.\n\nInformation: An organized report about the error codes.\n\nKnowledge: The ability to read a specific error report, understand it, and possibly fix the problem.\n\nWisdom: An understanding of how different and seemingly unrelated errors are actually connected symptoms of a larger, underlying problem.\n\n\nWhatever esoteric connotations we might associate with the word \u201cwisdom,\u201d we can define it here as deep subject matter expertise. From this kind of wisdom comes decision-making, business value, and leadership.\n\nFrom a human perspective, the levels of this hierarchy aren\u2019t absolute, though. The line between each level is fuzzy, and the growing insight within a particular organization doesn\u2019t usually follow a direct path; the journey circles back on itself. For example, as knowledge about the value of data increases, the nature of data collection might change. Insights gleaned from data analysis become data themselves. Additionally, insight doesn\u2019t simply move up the hierarchy, but comes in from all directions. There are many inputs aside from information that create knowledge, and many inputs aside from knowledge that create wisdom and business value.\n\nFrom a machine learning and AI perspective, the hierarchy undergoes a weird recursion process. While humans might be said to climb the hierarchy, machines flatten it. No matter how advanced a computer system is, it is really only dealing with data. The inputs to an AI system are data; the processing of those inputs is data centric; and the outputs are, from the computer\u2019s perspective, simply more data. But the outputs can be interpreted by humans at each level of the hierarchy. We can understand the results in terms of information, knowledge, or\u2014if the AI is sufficiently advanced\u2014wisdom.\n\nFrom signals to data to information\n\nThe first wave of innovation in big data was simply figuring out how to store and efficiently query massive amounts of data. NoSQL and distributed database systems like Hadoop and Cassandra made this possible.\n\nLooking back, it\u2019s seems inevitable that these technologies grew up with web search engines. This illustrates the point that machines collapse the hierarchy so that humans can ascend it. Countless individual humans have used their own knowledge to put content (information) onto the web. Using graph databases to turn the web into data about connections, and natural language processing to turn information into data about content, made the web navigable. Humans were now able to find and use information that was previously inaccessible.\n\nAI and data science yield information and knowledge\n\nMuch of the current excitement in AI and data science is about generating information and modeling knowledge. Classification tools\u2014which is where machine learning really excels\u2014generate new information. Natural language processing tools can summarize text and tell us how the writer felt about the subject. Multidimensional vector analysis can categorize inputs based on a near-limitless number of factors. Computer vision technology, combined with trained neural networks, can identify faces, street signs, or hot dogs. In other words, the data input (numbers representing pixels in an image) results in information (\u201cthis is a picture of X\u201d).\n\nIn a way, we might say that knowledge\u2014the knowledge of what a hot dog looks like or the knowledge of how a particular emotion manifests in text\u2014is contained in the model or neural network that is able to produce that information. More advanced AI uses data to simulate procedural knowledge. This could be relatively simple, like finding an efficient driving route. It could also be unimaginably complex, like a self driving car. In either case, knowledge is a process enabled by data.\n\nMoving toward wisdom\n\nBeyond the classification or procedural knowledge available with neural networks and deep learning, lies the realm of wisdom\u2014the deep experiential insight where one not only knows what a thing is, but also knows why, and in what context. Wisdom allows one to be reminded of things: that new thing \u201crings a bell,\u201d it is like this other thing I already know\u2026 and here\u2019s why I think so.\n\nIf you\u2019ve ever listened to Car Talk on NPR, or just gone to a skilled auto mechanic, you\u2019ve experienced this. An incomplete description of the problem, a poor imitation of the funny noise from under the hood, and the experts start diving into their memory, pulling up past experiences that are like the present problem.\n\nThis is where artificial intelligence is heading. We have a long way to go before we can call machines \u201cwise,\u201d but some AI platforms are moving in that direction. For example, Saffron, a cognitive computing platform offered by Intel, uses a \u201cmemory fabric\u201d to enhance the functional expertise of highly skilled humans. Saffron\u2019s reasoning, based on graph relationships, provides instances of similarity out of huge sets of unstructured data. Another example is Watson, IBM\u2019s AI platform, which is able to answer questions posed in natural language. In both cases, and indeed with all similar technologies, these platforms extend human capabilities rather than replacing them. The machines aren\u2019t wise, but they help make humans wiser.\n\nHuman wisdom will always be needed\n\nHuman wisdom will always be needed. Especially human wisdom that understands what the AI is doing.\n\n\nI believe human beings will always be in the loop, helping us interpret streams of information and finding meaning in the numbers. We will move higher up in the food chain, not be pushed out of the picture by automation. The future of work enhanced by data will enable us to focus on higher-level tasks.\n\n\u2014William Ruh, chief digital officer, GE Software, from the preface to \u201cLearning to Love Data Science,\u201d by Mike Barlow.\n\n\n\n\nLogging error reports are meaningless if the logs are never looked at, and the information generated by a search engine or a classification model can\u2019t become knowledge if it is never used. We build data systems and AI tools to support human needs, and we will always need humans to turn the potential insights provided by technology into real value.\n\nIt is humans who build, operate, and understand the tools that turn data into information, and information into knowledge, and knowledge into wisdom. Those people bridging the gap between computer data and human wisdom will fill a variety of roles from executive leadership to low-level analyst. But what they will have in common is an understanding of the fundamental tools of data science, and the value that data-driven wisdom can create.\n\nMachines cannot ascend the wisdom hierarchy on their own; it is beyond their capacity because they cannot intuit meaning. Without meaning, everything is just data. Humans have been ascending the wisdom hierarchy as long as we have been human, but the inputs of modern life, the masses of data we collect and accumulate, have exceeded our capacity to process on our own. Only together, humans and intelligent machines working collaboratively, can we bring the insights available from our data into the realm of wisdom that guides value creation and decision-making.\n\n\n\nThis post is part of a collaboration between O'Reilly and Intel Saffron. See our statement of editorial independence.\n\n\n\n\n\n\n\n\nContinue reading The wisdom hierarchy: From signals to artificial intelligence and beyond.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FIpNMWHmyHQ/the-wisdom-hierarchy-from-signals-to-artificial-intelligence-and-beyond"
 },
 {
  "title": "Four short links: 1 August 2017",
  "content": "RNA Coding, x86 Fuzzing, 1960s Coding, and AI Traps\n\nA Living Programmable Biocomputing Device Based on RNA (Kurzweil AI) -- Similar to a digital circuit, these synthetic biological circuits can process information and make logic-guided decisions, using basic logic operations \u2014 AND, OR, and NOT. But instead of detecting voltages, the decisions are based on specific chemicals or proteins, such as toxins in the environment, metabolite levels, or inflammatory signals. The specific ribocomputing parts can be readily designed on a computer.\n\n\nSandsifter -- audits x86 processors for hidden instructions and hardware bugs by systematically generating machine code to search through a processor's instruction set and monitoring execution for anomalies. Sandsifter has uncovered secret processor instructions from every major vendor; ubiquitous software bugs in disassemblers, assemblers, and emulators; flaws in enterprise hypervisors; and both benign and security-critical hardware bugs in x86 chips.\n\n\nProgramming in the 1960s -- uphill both ways in rain the colour of a television tuned to a dead station.\n\nHow to Make a Racist AI Without Even Trying -- My purpose with this tutorial is to show that you can follow an extremely typical NLP pipeline, using popular data and popular techniques, and end up with a racist classifier that should never be deployed.  Exploitability is the failure mode of doing what's easy.\n\nContinue reading Four short links: 1 August 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HFR72dJOYcY/four-short-links-1-august-2017"
 },
 {
  "title": "How can I add simple, automated data visualizations and dashboards to Jupyter Notebooks",
  "content": "Learn how to use PixieDust in Jupyter Notebooks to create quick, easy, and powerful visualizations for exploring your data.Continue reading How can I add simple, automated data visualizations and dashboards to Jupyter Notebooks.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/KxwLxOj1f4Q/how-can-i-add-simple-automated-data-visualizations-and-dashboards-to-jupyter-notebooks"
 },
 {
  "title": "From prototype to product with hybrid neural networks",
  "content": "Apache MXNet and the middle path between declarative and imperative programming.After several decades as an interest of academic computer scientists and specialized research labs, deep learning is appearing widely in real products. That transition has led to several exciting new deep-learning frameworks that tend to emphasize either rapid prototyping or efficient deployment at scale. For product developers looking to experiment with an idea and then reinforce and deploy it, a single framework that supports both ends of the process is helpful.\n\nApache MXNet, an open-source deep learning framework first published in 2015, aims to achieve exactly that. I recently talked with Mu Li, principal scientist at Amazon and one of the original authors of MXNet\u2014he was lead author of the \u201cParameter Server\" paper that enables MXNet to have almost linear scale with additional processing power. He walked me through the origins of the framework and his vision of \u201chybridized\u201d neural network models that can carry a single project from early prototyping through deployment at scale.\n\nMXNet emerged from two goals that are classically opposed in computer science: ease of use and high performance. Declarative structures enable high performance in deep learning: the programmer specifies a network structure at a high level, and the framework implements low-level native routines to build the network in the most efficient manner possible.\n\nThe drawback is that these declarative frameworks require the programmer to know and specify the network structure\u2014its computation graph\u2014at the outset. That makes iteration slow and experimentation difficult; discovering the best network structure for a particular problem is arguably the principal task for a deep-learning engineer. In some cases, like long short-term memory (LSTM) networks, the structure of the neural network can depend on control statements--loops and \u2018if\u2019 statements--that can\u2019t be evaluated until data is fed in, so programmers need to use equivalent statements provided by the frameworks\u2014meaning lots of mucking about with low-level code.\n\nDeclarative frameworks can also be unintuitive to programmers who are accustomed to an interactive trial-and-error approach\u2014an essential shortcoming as interest in deep learning explodes. Many newcomers to deep learning don\u2019t have systematic training in symbolic linear algebra and want to teach themselves through active experimentation.\n\nThe deep learning community has responded by introducing a handful of imperative frameworks\u2014notably PyTorch and Chainer\u2014that execute programs line-by-line and allow programmers to use complex control statements to change network structure programmatically and on-the-fly. The drawback here is in performance: if the programmer doesn\u2019t specify the full structure of the network before running it, the framework can\u2019t pre-compile it to wring the best possible performance out of specialized hardware accelerators like GPUs.\n\n\u201cThe problem [with imperative programming] is that it\u2019s hard to optimize, because you never know what you\u2019re going to write in the next sentence,\u201d says Li. \u201cYou don\u2019t know whether results in memory will be re-used, so it\u2019s hard to get performance right. But it\u2019s ideal for people who want to hack some code together for fast prototyping.\u201d The result is that deep learning implementation sometimes gets split into a research stage using imperative frameworks, and a product stage using declarative frameworks. Li points to Facebook as an example: the company supports both PyTorch and Caffe2, a declarative framework, and uses the former for exploration and the latter for products.\n\nLi and the MXNet developers have taken a hybrid approach that supports experimenting and prototyping with imperative programs, then seamlessly refactoring critical sections into declarative blocks as you move toward production at scale. \u201cWe want to have a single interface for users,\u201d says Li. \u201cYou can start with imperative code, and then when you want to deploy, you hybridize.\u201d\n\nDevelopers will have more opportunities to run fast neural networks on all sorts of devices in the next several years. Researchers today depend on computers or cloud services with expensive GPUs to accelerate neural network training. NVIDIA\u2019s GPUs are dominant in this field not just due to raw speed but because CUDA, its programming interface for GPUs, and cuDNN, its deep learning library, are broadly supported by deep learning frameworks, which compile neural networks into code that runs efficiently on NVIDIA GPUs by calling functions provided by cuDNN.\n\nResearchers envision a future in which neural networks will conduct both inference and training on \u201cedge\u201d devices like mobile phones and embedded systems. With this variety of devices in play, from high-end phones to stripped-down IoT devices (and let\u2019s not forget legacy equipment\u2014many of today\u2019s devices will still be in use for much of the coming decade), these networks won\u2019t be able to depend on high-end GPUs; instead, they may use purpose-designed field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs)\u2014or even rely on currently available digital signal processors (DSPs).\n\nThat means a flowering of different devices that deep-learning frameworks need to support. The framework authors can\u2019t write extensions for every conceivable accelerator; instead, they\u2019re focusing on general-purpose compilers that can recognize any accelerator hardware and compile neural networks to run efficiently on it.\n\nIn the next month or so, MXNet will launch its general compiler, allowing developers to implement neural networks on any accelerator, from high-end GPUs down to inexpensive DSPs and specialized processors in mobile phones. Li says the compiler will initially be open-sourced for early users.\n\n\nThis post is part of a collaboration between O'Reilly and Amazon. See our statement of editorial independence.\nContinue reading From prototype to product with hybrid neural networks.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jClButFaNGY/from-prototype-to-product-with-hybrid-neural-networks"
 },
 {
  "title": "Four short links: 31 July 2017",
  "content": "Statistics & Fiction, Staying Anonymous, Attacking Machine Learning, and Digital Native is Fiction\n\nThe Heretical Things Statistics Tell Us About Fiction (New Yorker) -- Almost without fail, the words evoke their authors\u2019 affinities and manias. John Cheever favors \u201cvenereal\u201d\u2014a perfect encapsulation of his urbane midcentury erotics, tinged with morality. Isaac Asimov prefers \u201cterminus,\u201d a word ensconced in a swooping, stately futurism; Woolf has her \u201cmantelpiece,\u201d Wharton her \u201ccompunction.\u201d (Melville\u2019s \u201csperm\u201d is somewhat misleading, perhaps, when separated from his whales.)\n\n\nThings Not to Do When Anonymous -- so many ways to surrender anonymity, so difficult to remain anonymous.\n\nRobust Physical-World Attacks on Machine Learning Models -- Our algorithm can create spatially constrained perturbations that mimic vandalism or art to reduce the likelihood of detection by a casual observer. We show that adversarial examples generated by RP2 achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions. We physically realized and evaluated two attacks, one that causes a stop sign to be misclassified as a speed limit sign in 100% of the testing conditions, and one that causes a right turn sign to be misclassified as either a stop or added lane sign in 100% of the testing conditions.\n\n\nThe Digital Native is a Myth (Nature) -- The younger generation uses technology in the same ways as older people\u2014and is no better at multitasking.\n\n\nContinue reading Four short links: 31 July 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/B_P1te61Z6o/four-short-links-31-july-2017"
 },
 {
  "title": "Four short links: 28 July 2017",
  "content": "Usable Security, Philosophy Time, AR A-Ha, and Machine Knitting\n\t\nA Rant on Usable Security (Jessie Frazelle) -- By making a default for all containers, we can secure a very large amount of users without them even realizing it\u2019s happening. This leads perfectly into my ideas for the future and continuing this motion of making security on by default and invisible to users. Kernel/Docker-heavy thoughts.\n\t\nPhilosophy Time -- James Franco and philosophers tackle beauty, metaphor, imagination, and more. (via BoingBoing)\n\t\nAugmented Reality's A-ha Moment -- \u200bprototype app from TRIXI Studios that turns your living room into an A-ha video. Finally, someone using technology for the forces of good. (via BoingBoing)\n\t\nMachine Knitting Lingo -- not just vocab but pointers to hardware and software like Img2Track which connects certain models of Brother electronic machines directly to your computer. [...] Img2Track has a full UI, no need for Python or the command line. You import an image, it does some needed manipulation for you, you download it to your machine, and then knit as usual. To use Img2Track you only need to buy or make a special cable. You make no changes to your knitting machine itself. Demo up to 60 stitches wide for free, pay for the full-width software.\n\nContinue reading Four short links: 28 July 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VmhujuuyPII/four-short-links-28-july-2017"
 },
 {
  "title": "Eric Freeman and Elisabeth Robson on design patterns",
  "content": "The O\u2019Reilly Programming Podcast: Creating designs that are more flexible and resilient to change.In this episode of the O\u2019Reilly Programming Podcast, I talk with Eric Freeman and Elisabeth Robson, presenters of the live online training course Design Patterns Boot Camp, and co-authors (with Bert Bates and Kathy Sierra) of Head First Design Patterns, among other books. They are also co-founders of WickedlySmart, an online learning company for software developers.Continue reading Eric Freeman and Elisabeth Robson on design patterns.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Z3j7tp3UlUw/eric-freeman-and-elisabeth-robson-on-design-patterns"
 },
 {
  "title": "John Whalen on using brain science in design",
  "content": "The O\u2019Reilly Design Podcast: Designing for the \u201csix minds,\u201d the importance of talking like a human, and the future of predictive AI.In this week\u2019s Design Podcast, I sit down with John Whalen, chief experience officer at 10 Pearls, a digital development company focused on mobile and web apps, enterprise solutions, cyber security, big data, IoT, \u00a0and cloud and dev ops. We talk about the \u201csix minds\u201d that underlie each human experience, why it\u2019s important for designers to understand brain science, and what people really look for in a voice assistant.Continue reading John Whalen on using brain science in design.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4RmuX135RHg/john-whalen-on-using-brain-science-in-design"
 },
 {
  "title": "When you hear hooves, think horse, not zebra",
  "content": "Don't overcomplicate cybersecurity. Focus on building a strong security foundation and go from there.A month or two ago I was having a discussion with a physician about obscure diseases\u2014commonly referred to as zebras. While I was considering these zebras in the context of effective data mining strategies for medical diagnosis, he made an interesting point. One of the things that they teach new physicians is the phrase \u201cWhen you hear hoofs, think horse, not zebra.\u201d The principle is quite simple\u2014the odds are the patient has the more common diagnosis than a rare, improbable one. A simple but illustrative example would be the following (stolen from a physician family member):\n\nAn adolescent female patient presents with a three-week history of headache, fatigue, and intermittent fevers but was historically healthy. The physical exam was unremarkable and aside from the occasional fevers, the only symptom of note was that she was pale in color. The zebra could have been meningitis or a brain tumor\u2014and the inexperienced practitioner would order thousands of dollars of tests and subject the patient to multiple procedures. But a routine blood count showed that she was simply anemic\u2014the horse\u2014and just needed extra iron. The rule: Think horse without ruling out zebras.Continue reading When you hear hooves, think horse, not zebra.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/P-HykUFHeao/when-you-hear-hooves-think-horse-not-zebra"
 },
 {
  "title": "Classifying traffic signs with Apache MXNet: An introduction to computer vision with neural networks",
  "content": "Step-by-step instructions to implement a convolutional neural net, using a Jupyter Notebook.Although there are many deep learning frameworks, including TensorFlow, Keras, Torch, and Caffe, Apache MXNet in particular is gaining popularity due to its scalability across multiple GPUs. In this blog post, we'll tackle a computer vision problem: classifying German traffic signs using a convolutional neural network. The network takes a color photo containing a traffic sign image as input, and tries to identify the type of sign.\nThe full notebook can be found here\nIn order to work through this notebook, we expect you'll have a very basic understanding of neural network, convolution, activation units, gradient descent, NumPy, and OpenCV. These prerequisites are not mandatory, but having a basic understanding will help.\nBy the end of the notebook, you will be able to:\n\nPrepare a data set for training a neural network;\nGenerate and augment data to balance the data set; and\nImplement a custom neural network architecture for a multiclass classification problem.\n\n\nPreparing your environment\n\nIf you're working in the AWS Cloud, you can save yourself the installation management by using an Amazon Machine Image (AMI) preconfigured for deep learning. This will enable you to skip steps 1-5 below.  \nNote that if you are using a conda environment, remember to install pip inside conda, by typing 'conda install pip' after you activate an environment. This step will save you a lot of problems down the road.\nHere's how to get set up: \n\nFirst, get Anaconda, a package manager. It will help you to install dependent Python libraries with ease.\nInstall the OpenCV-python library, a powerful computer vision library. We will use this to process our image. To install OpenCV inside the Anaconda environment, use 'pip install opencv-python'. You can also build from source. (Note: conda install opencv3.0 does not work.)\nNext, install scikit learn, a general-purpose scientific computing library. We'll use this preprocess our data. You can install it with 'conda install scikit-learn'.\nThen grab the Jupyter Notebook, with 'conda install jupyter notebook'.\nAnd finally, get MXNet, an open source deep learning library.\n\nHere are the commands you need to type inside the anaconda environment (after activation of the environment):\n\nconda install pip \npip install opencv-python\nconda install scikit-learn\nconda install jupyter notebook\npip install mxnet\n\n\nThe data set\n\nIn order to learn about any deep neural network, we need data. For this notebook, we use a data set already stored as a NumPy array. You can also load data from any image file. We'll show that process later in the notebook.\nThe data set we'll use is the German Traffic Sign Recognition Benchmark (J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. \"The German Traffic Sign Recognition Benchmark: A multi-class classification competition.\" In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453\u20131460. 2011.). \nThis data set consists of 39,209 training samples and 12,630 testing samples, representing 43 different traffic signs\u2014stop signs, speed limits, various warning signs, and so on).\nWe'll use a pickled version of the data, training.p and valid.p. \nEach image in the dataset is 32*32 size with three channel (RGB) color, and it belongs to a particular image class. The image class is an integer label between 0 and 43. The 'signnames.csv' file contains the mapping between the sign name and the class labels. \nHere's the code for loading the data:\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\ntraining_file = \"traffic-data/train.p\"\nvalidation_file =  \"traffic-data/valid.p\"\n\nwith open(training_file, mode='rb') as f:\n    train = pickle.load(f)\n\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\n\nX_train, y_train = train['features'], train['labels']\nX_valid, y_valid = valid['features'], valid['labels']\n\nWe are loading the data from a stored NumPy array. In this array, the data is split between training, validation, and test sets. The training set contains the features of 39,209 images of size 32 X 32 with 3 (R,G,B) channels. As a result, the NumPy array dimension is  39,209 X 32 X 32 X 3. We will only be using the training set and validation set in this notebook. We will use real images from the internet to test our model.\nSo X_train is of dimension 39,209 * 32 X 32 X 3. The y_train is of dimesion 39,209 and contains a number between 0-43 for each image.\nNext, we load the file that maps each image class ID to natural-language names:\n# The actual name of the classes are given in a separate file. Here we load the csv file which allows mapping from classes/labels to \n# file name\nimport csv\ndef read_csv_and_parse():\n    traffic_labels_dict ={}\n    with open('signnames.csv') as f:\n        reader = csv.reader(f)\n        count = -1;\n        for row in reader:\n            count = count + 1\n            if(count == 0):\n                continue\n            label_index = int(row[0])\n            traffic_labels_dict[label_index] = row[1]\n    return traffic_labels_dict\ntraffic_labels_dict = read_csv_and_parse()\nprint(traffic_labels_dict)\n\nWe can see there are 43 labels for the 43 image classes. For example,\n0 image class represents a 20 km/h speed limit:\n{0: 'Speed limit (20km/h)', 1: 'Speed limit (30km/h)', 2: 'Speed limit (50km/h)', 3: 'Speed limit (60km/h)', 4: 'Speed limit (70km/h)', 5: 'Speed limit (80km/h)', 6: 'End of speed limit (80km/h)', 7: 'Speed limit (100km/h)', 8: 'Speed limit (120km/h)', 9: 'No passing', 10: 'No passing for vehicles over 3.5 metric tons', 11: 'Right-of-way at the next intersection', 12: 'Priority road', 13: 'Yield', 14: 'Stop', 15: 'No vehicles', 16: 'Vehicles over 3.5 metric tons prohibited', 17: 'No entry', 18: 'General caution', 19: 'Dangerous curve to the left', 20: 'Dangerous curve to the right', 21: 'Double curve', 22: 'Bumpy road', 23: 'Slippery road', 24: 'Road narrows on the right', 25: 'Road work', 26: 'Traffic signals', 27: 'Pedestrians', 28: 'Children crossing', 29: 'Bicycles crossing', 30: 'Beware of ice/snow', 31: 'Wild animals crossing', 32: 'End of all speed and passing limits', 33: 'Turn right ahead', 34: 'Turn left ahead', 35: 'Ahead only', 36: 'Go straight or right', 37: 'Go straight or left', 38: 'Keep right', 39: 'Keep left', 40: 'Roundabout mandatory', 41: 'End of no passing', 42: 'End of no passing by vehicles over 3.5 metric tons'}\n\n\n\nVisualization\n\nThe following code will help us to visualize the images along with the labels (image classes):\n# Exploratory data visualization\n# This gives a better, intuitive understanding of the data\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\n# Visualizations will be shown in the notebook.\n%matplotlib inline\n\n#This functions selects one image per class to plot\ndef get_images_to_plot(images, labels):\n    selected_image = []\n    idx = []\n    for i in range(n_classes):\n        selected = np.where(labels == i)[0][0]\n        selected_image.append(images[selected])\n        idx.append(selected)\n    return selected_image,idx\n\n# function to plot the images in a grid    \ndef plot_images(selected_image,y_val,row=5,col=10,idx = None):     \n    count =0;\n    f, axarr = plt.subplots(row, col,figsize=(50, 50))\n\n    for i in range(row): \n         for j in range(col):\n                if(count < len(selected_image)):\n                    axarr[i,j].imshow(selected_image[count])\n                    if(idx != None):\n                        axarr[i,j].set_title(traffic_labels_dict[y_val[idx[count]]], fontsize=20)\n                axarr[i,j].axis('off')\n                count = count + 1\n\nselected_image,idx = get_images_to_plot(X_train,y_train)\nplot_images(selected_image,row=10,col=4,idx=idx,y_val=y_train)\n\nHere are the visualized traffic signs, with their labels:\n\n\n\nPreparing the data set\n\nX_train and Y_train make the training data set. We'll employ real images for the purpose of testing. \nYou could also generate a validation set by splitting the training data into train and validation sets using scikit-learn (this is how you avoid testing your model on images that it's already seen). Here's the Python code for that:\n#split the train-set as validation and test set\nfrom sklearn.model_selection import train_test_split\nX_train_set,X_validation_set,Y_train_set,Y_validation_set = train_test_split( X_train, Y_train, test_size=0.02, random_state=42)\n\nThe image dimension order of MXNet is similar to Theano and uses the format 3X32X32. The number of channels is the first dimension, followed by height and width of the image. TensorFlow uses image dimension ordering of 32X32X3, i.e the color channels come last. If you're switching from TensorFlow to MXNet this discussion of dimension ordering may be helpful. Below is the helper function to convert image ordering to MXNet's 3X32X32 format from 32X32X3:\n#change the image dimensioning from 32 X 32 X 3 to 3 X 32 X 32 for train\nX_train_reshape = np.transpose(X_train, (0, 3, 1, 2))\nplt.imshow(X_train_reshape[0].transpose((1,2,0)))\nprint(X_train_reshape.shape)\n\n\n#change the image dimensioning from 32 X 32 X 3 to 3 X 32 X 32 for validation\nX_valid_reshape = np.transpose(X_valid, (0, 3, 1, 2))\nplt.imshow(X_valid_reshape[1].transpose((1,2,0)))\nprint(X_valid_reshape.shape)\n\n\n\nBuilding the deepnet\n\nNow, enough of preparing our data set. Let's actually code up the neural network. You'll note that there are some commented-out lines; I've left these in as artifacts from the development process\u2014building a successful deep learning model is all about iteration and experimentation to find what works best. Building neural networks is something of a black art at this point in history; while you might experiment to solve your particular problem, for a well-explored issue like image recognition, you'll do best to implement a published architecture with proven performance. Here, we'll build up a simplified version of the AlexNet architecture, which is based on convolutional neural networks.. \nThe neural code is concise and simple, thanks to MXNet's symbolic API:\ndata = mx.symbol.Variable('data')\nconv1 = mx.sym.Convolution(data=data, pad=(1,1), kernel=(3,3), num_filter=24, name=\"conv1\")\nrelu1 = mx.sym.Activation(data=conv1, act_type=\"relu\", name= \"relu1\")\npool1 = mx.sym.Pooling(data=relu1, pool_type=\"max\", kernel=(2,2), stride=(2,2),name=\"max_pool1\")\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(3,3), num_filter=48, name=\"conv2\", pad=(1,1))\nrelu2 = mx.sym.Activation(data=conv2, act_type=\"relu\", name=\"relu2\")\npool2 = mx.sym.Pooling(data=relu2, pool_type=\"max\", kernel=(2,2), stride=(2,2),name=\"max_pool2\")\n\nconv3 = mx.sym.Convolution(data=pool2, kernel=(5,5), num_filter=64, name=\"conv3\")\nrelu3 = mx.sym.Activation(data=conv3, act_type=\"relu\", name=\"relu3\")\npool3 = mx.sym.Pooling(data=relu3, pool_type=\"max\", kernel=(2,2), stride=(2,2),name=\"max_pool3\")\n\n#conv4 = mx.sym.Convolution(data=conv3, kernel=(5,5), num_filter=64, name=\"conv4\")\n#relu4 = mx.sym.Activation(data=conv4, act_type=\"relu\", name=\"relu4\")\n#pool4 = mx.sym.Pooling(data=relu4, pool_type=\"max\", kernel=(2,2), stride=(2,2),name=\"max_pool4\")\n\n# first fullc layer\nflatten = mx.sym.Flatten(data=pool3)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500, name=\"fc1\")\nrelu3 = mx.sym.Activation(data=fc1, act_type=\"relu\" , name=\"relu3\")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=relu3, num_hidden=43,name=\"final_fc\")\n# softmax loss\nmynet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n\nLet's break down the code a bit. First, it creates a data layer(input layer) that actually holds the dataset while training:\ndata = mx.symbol.Variable('data')\n\nThe conv1 layer performs a convolution operator on the image, and is connected to the data layer:\nconv1 = mx.sym.Convolution(data=data, pad=(1,1), kernel=(3,3), num_filter=24, name=\"conv1\")\n\nThe relu2 layer performs non-linear activation on the input, and is connected to convolution 1 layer:\nrelu2 = mx.sym.Activation(data=conv2, act_type=\"relu\", name=\"relu2\")\n\nThe max pool layer performs a pooling operation (dropping some pixels and reducing image size) on the previous layer's output (relu2).\npool2 = mx.sym.Pooling(data=relu2, pool_type=\"max\", kernel=(2,2), stride=(2,2),name=\"max_pool2\")\n\nA neural network is like a Lego block\u2014we can easily repeat some of the layers (to increase the learning capacity of model)\u2014 and then follow them with a dense layer. A dense layer is a fully connected layer, in which every neuron from the previous layer is connected to every neuron in the dense layer.\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500, name=\"fc1\")\n\nThis layer is followed again by a fully connected layer with 43 neurons, each neuron representing a class of the image. Since the output from the neuron is real valued, but our classification requires a single label as output, we use another activation function. This step makes the output of one particular neuron (out of 43 neurons) as 1 and remaining neurons as zero.\nfc2 = mx.sym.FullyConnected(data=relu3, num_hidden=43,name=\"final_fc\")\n# softmax loss\nmxnet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n\n\n\nTweaking training data\n\nA neural network takes a lot of time and memory to train. We're going to split our data into minibatches of 64, not just so that they fit into memory but also because it enables MXNet to make the most of GPU computational efficiency demands it (among other reasons).\nWe'll also normalize the value of the image colors (0-255) to the range of 0 to 1. This helps the learning algorithm to converge faster. You can read about the reasons to normalize the input.\nHere's the code to normalize the value of the image color: \nbatch_size = 64\nX_train_set_as_float = X_train_reshape.astype('float32')\nX_train_set_norm = X_train_set_as_float[:] / 255.0;\n\nX_validation_set_as_float = X_valid_reshape.astype('float32')\nX_validation_set_norm = X_validation_set_as_float[:] / 255.0 ;\n\n\ntrain_iter =mx.io.NDArrayIter(X_train_set_as_float, y_train_extra, batch_size, shuffle=True)\nval_iter = mx.io.NDArrayIter(X_validation_set_as_float, y_valid, batch_size,shuffle=True)\n\n\nprint(\"train set : \", X_train_set_norm.shape)\nprint(\"validation set : \", X_validation_set_norm.shape)\n\n\nprint(\"y train set : \", y_train.shape)\nprint(\"y validation set :\", y_valid.shape)\n\n\n\nTraining the network\n\nWe are training the network using GPUs, since it's faster. A single pass-through of the training set is referred to as one \"epoch,\" and we are training the network for 10 epochs \"num_epoch = 10\". We also periodically store the trained model in a JSON file, and measure the train and validation accuracy to see our neural network 'learn.' \nHere is the code: \n#create adam optimiser\nadam = mx.optimizer.create('adam')\n\n#checking point (saving the model). Make sure there is folder named models exist\nmodel_prefix = 'models/chkpt'\ncheckpoint = mx.callback.do_checkpoint(model_prefix)\n\n#loading the module API. Previously mxnet used feedforward (deprecated)                                       \nmodel =  mx.mod.Module(\n    context = mx.gpu(0),     # use GPU 0 for training if you dont have gpu use mx.cpu(). \n    symbol = mynet,             \n    data_names=['data']\n   )\n\n#actually fit the model for 10 epochs. Can take 5 minutes                                      \nmodel.fit(\n    train_iter,\n    eval_data=val_iter, \n    batch_end_callback = mx.callback.Speedometer(batch_size, 64),\n    num_epoch = 10, \n    eval_metric='acc', # evaluation metric is accuracy. \n    optimizer = adam,\n    epoch_end_callback=checkpoint\n)\n\n\n\nLoading the trained model from the filesystem\n\nSince we have check-pointed the model during training, we can load any epoch and check its classification power. In the following example, we load the 10th epoch. We also set the binding in the model loaded to training as false, since we are using this network for testing, not training. Furthermore, we reduce the batch size of input from 64 to 1 (data_shapes=[('data', (1,3,32,32))), since we are going to test it on a single image. \nYou can use the same technique to load any other pre-trained machine learning model:\n#load the model from the checkpoint , we are loading the 10 epoch\nsym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, 10)\n\n# assign the loaded parameters to the module\nmod = mx.mod.Module(symbol=sym, context=mx.cpu())\nmod.bind(for_training=False, data_shapes=[('data', (1,3,32,32))])\nmod.set_params(arg_params, aux_params)\n\n\n\nPrediction\n\nTo use the loaded model for prediction, we convert a traffic sign image (Stop.jpg) into 32  32  3 (32 * 32 dimension image with 3 channels) and try to predict their label. Here's the image I downloaded.\n\n#Prediction for random traffic sign from internet\nfrom collections import namedtuple\nBatch = namedtuple('Batch', ['data'])\n\n#load the image , resizes it to 32*32 and converts it to 1*3*32*32 \ndef get_image(url, show=False):\n    # download and show the image\n    img =cv2.imread(url)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (32, 32))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2) #swaps axis to make it 3*32*32\n    #plt.imshow(img.transpose(1,2,0))\n    #plt.axis('off')\n    img = img[np.newaxis, :] # Add a extra axis to the image so it becomes 1*3*32*32\n    return img\n\ndef predict(url):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    mod.forward(Batch([mx.nd.array(img)]))\n    prob = mod.get_outputs()[0].asnumpy()\n    # print the top-5\n    prob = np.squeeze(prob)\n    prob = np.argsort(prob)[::-1]\n    for i in prob[0:5]:\n        print('class=%s' %(traffic_labels_dict[i]))\n\npredict('traffic-data/Stop.jpg',)\n\nWe then get the model's top five predictions for what this image is, find that our model got it right! The predictions:\nclass=Stop\nclass=Speed limit (30km/h)\nclass=Speed limit (20km/h)\nclass=Speed limit (70km/h)\nclass=Bicycles crossing\n\n\nConclusion\n\nIn this notebook, we explored how to use MXNet to perform a multi-class image classification. While the network we built was simpler than the most sophisticated image-recognition neural network architectures available, even this simpler version was surprisingly performant! We also learned techniques to pre-process image data, we trained the network and stored the trained neural network on the disk. Later, we loaded the pre-trained neural network model to classify images from the web. This model could be deployed as a web service or app (you could build your own what-dog!).  You could also use these techniques on other data for the purpose of classification, whether that's analyzing sentiment and intent in chats with your help desk, or discovering illegal intent in financial behaviors. \nIn our next notebook, we'll develop a state-of-the-art sentiment classifier using MXNet. \n\n\nContinue reading Classifying traffic signs with Apache MXNet: An introduction to computer vision with neural networks.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/MvCxvA2rpYA/classifying-traffic-signs-with-mxnet-an-introduction-to-computer-vision-with-neural-networks"
 },
 {
  "title": "Four short links: 27 July 2017",
  "content": "Next Death, Evolution of Trust, Indoor Robots, and Embryo Editing\n\t\nPredicting Who Dies Next in Game of Thrones -- waiting for the org chart version of this.\n\t\nThe Evolution of Trust -- fun illustration of what game theory has to say about trust.\n\t\nIndoor Robots -- overview of companies in the various spaces.\n\t\nFirst Human Embryos Edited in USA (MIT TR) -- Mitalipov is believed to have broken new ground both in the number of embryos experimented upon and by demonstrating that it is possible to safely and efficiently correct defective genes that cause inherited diseases. Although none of the embryos were allowed to develop for more than a few days\u2014and there was never any intention of implanting them into a womb\u2014the experiments are a milestone on what may prove to be an inevitable journey toward the birth of the first genetically modified humans.\n\n\nContinue reading Four short links: 27 July 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/6x2fFTIfXeU/four-short-links--27-july-2017"
 },
 {
  "title": "R\u2019s tidytext turns messy text into valuable insight",
  "content": "Authors Julia Silge and David Robinson discuss the power of tidy data principles, sentiment lexicons, and what they're up to at Stack Overflow.\u201cMany of us who work in analytical fields are not trained in even simple interpretation of natural language,\u201d write Julia Silge, Ph.D., and David Robinson, Ph.D., in their newly released book Text Mining with R: A tidy approach. The applications of text mining are numerous and varied, though; sentiment analysis can assess the emotional content of text, frequency measurements can identify a document\u2019s most important terms, analysis can explore relationships and connections between words, and topic modeling can classify and cluster similar documents.\n\nI recently caught up with Silge and Robinson to discuss how they\u2019re using text mining on job postings at Stack Overflow, some of the challenges and best practices they\u2019ve experienced when mining text, and how their tidytext package for R aims to make text analysis both easy and informative.\n\nLet\u2019s start with the basics. Why would an analyst mine text? What insights can be derived from mining instances of words, sentiment of words?\n\nText and other unstructured data is increasingly important for data analysts and data scientists in diverse fields from health care to tech to nonprofits. This data can help us make good decisions, but to capitalize on it, we must have the tools and the skills to get from unstructured text to insights. We can learn a lot by exploring word frequencies or comparing word usage, and we can dig deeper by implementing sentiment analysis to analyze the emotion or opinion content of words, or by fitting a topic model to discover underlying structure in a set of documents.\n\nWhy did you create the tidytext text mining package in R? How does it make an R user\u2019s life easier?\n\nWe created the tidytext package because we believe in the power of tidy data principles, and we wanted to apply this consistent, opinionated approach for handling data to text mining tasks. Tidy tools like dplyr and ggplot2 are widely used, and integrating natural language processing into these tools allows R users to work with greater fluency.\n\nOne feature in tidytext 0.1.3 is the addition of the Loughran and McDonald sentiment lexicon of words specific to financial reporting, where words like \u201ccollaborate\u201d and \u201ccollaborators\u201d seem to be tagged as positive and words like \u201ccollapsed\u201d and \u201ccollapsing\u201d seem to be tagged as negative. For someone who is new to text mining, what is the general purpose of a sentiment lexicon? What are some ways this lexicon would be used in by an analyst?\n\nSentiment lexicons are lists of words that have been assigned scores according to how positive or negative they are, or what emotions (such as \u201canticipation\u201d or \u201cfear\u201d) they might be associated with. We can analyze the emotion content of text by adding up the scores of the words within it, which is a common approach to sentiment analysis. The tidytext package contains several general purpose English lexicons appropriate for general text, and we are excited to extend these with a context-specific lexicon for finance. A word like \u201cshare\u201d has a positive meaning in most contexts, but is neutral in financial contexts, where it usually refers to shares of stock. Applying the Loughran-McDonald lexicon allows us to explore the sentiment content of documents dealing with finance with more confidence.\n\nIn your book, you perform text analysis on data sets ranging from classic Jane Austen novels to NASA metadata to Twitter archives. What are some of the ways you\u2019re analyzing text data in your daily work at Stack Overflow?\n\nWe are swimming in text data at Stack Overflow! One example we deal with is text in job postings; we use text mining and modeling to match job listings with people who may be interested in them. Another example is text in messages between companies who are hiring and developers they want to hire; we use text mining to see what makes a developer more likely to respond to a company. But, we\u2019re certainly not unique in this; many organizations are dealing with increasing amounts of text data that are important to their decision-making.\n\nText data is messy, and things like abbreviations, \u201cfiller\u201d words, or repeated words can present many challenges. What are some common challenges practitioners might confront when wrangling or visualizing text data, as opposed to more traditional data types (e.g., numerical)?\n\nData scientists and analysts like us are usually trained on numerical data in a rectangular shape like a table (i.e., data frame), so it takes some practice to fluently wrangle raw text data. We find ourselves reaching for regular expressions and the stringr package a lot, to deal with challenges such as stripping out HTML tags or email headers, or extracting subsets of text we are interested in. We often put such tasks into practice using the purrr package; it\u2019s a very useful tool for dealing with iteration.\n\nWhat are some best practices you can offer to data scientists and analysts looking to overcome text mining problems?\n\nWe come from a particular, opinionated perspective on this question; our advice is that adopting tidy data principles is an effective strategy to approach text mining problems. The tidy text format keeps one token (typically a word) in each row, and keeps each variable (such as a document or chapter) in a column. When your data is tidy, you can use a common set of tools for exploring and visualizing them. This frees you from struggling to get your data into the right format for each task and instead lets you focus on the questions you want to ask.\n\nYour book demonstrates how to do text mining in R. Which R tools do you commonly use to support text mining? And why is R your tool of choice?\n\nOur main toolbox for text mining in R focuses on our package tidytext, along with the packages dplyr, tidyr, and ggplot2. These are all tools from the tidyverse collection of packages in R, and the availability and cohesion of these tools are the reasons why we use R for text mining. Using consistent tools designed for handling tidy data gives us a dependable framework for understanding how to represent text data in R, visualize the characteristics of text, model topics, and move smoothly to more complex machine learning applications.\n\nWhat is the difference between text mining and natural language processing?\n\nIn our experience, definitions for these terms are somewhat vague and sometimes interchangeable. When people talk about text mining, they often mean getting insight from text through statistical analysis, perhaps looking at word frequencies or clustering. When people talk about natural language processing, they\u2019re often describing the interaction between language and computers, and sometimes the goal of extracting meaning to enable human-computer conversations. We describe our work as \u201ctext mining\u201d because our goal is extracting and visualizing insights, but there is a great deal of overlap.\nContinue reading R\u2019s tidytext turns messy text into valuable insight.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Y_OASP6WQKY/rs-tidytext-turns-messy-text-into-valuable-insight"
 },
 {
  "title": "Four short links: 26 July 2017",
  "content": "Blockchain and Securities Law, Low-Energy Sensing, Robotics Deep Learning, and Chipped Employees\n\t\nReport of Investigation ... DAO (SEC) -- This Report reiterates these fundamental principles of the U.S. federal securities laws and describes their applicability to a new paradigm\u2014virtual organizations or capital raising entities that use distributed ledger or blockchain technology to facilitate capital raising and/or investment and the related offer and sale of securities.\n\n\t\nHow to Make a Wireless Sensor Live for a Year on One Tiny Coin Cell Battery -- We see that making the right choices in software can make a stunning 400x difference in power consumption \u2013 two orders of magnitude \u2013 even with an already extremely power-efficient hardware.\n\n\t\nDeep Learning in Robotics: A Review of Recent Research -- what it says on the cover.\n\t\nA Wisconsin Company Will Let Employees Use Microchip Implants to Buy Snacks and Open Doors -- Participating employees will have the chips, which use near field communication (NFC) technology, implanted between their thumb and forefinger. It\u2019s an extension of the long-running implantable RFID chip business, based on a partnership with Swedish company Biohax International. The vending kiosk company, also known as 32M, will \u201cchip\u201d employees at a party on August 1st.  This is fine. What could possibly go wrong?\n\nContinue reading Four short links: 26 July 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gbTf9E9nWbo/four-short-links--26-july-2017"
 },
 {
  "title": "Making great hires in your design organization",
  "content": "How to help your team succeed and develop a stronger design practice.Continue reading Making great hires in your design organization.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hjaPgkz0QHs/making-great-hires-in-your-design-organization"
 },
 {
  "title": "A lesson in prescriptive modeling",
  "content": "Simulate new business models and practices with open source code.Continue reading A lesson in prescriptive modeling.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1OclojJb3SQ/a-lesson-in-prescriptive-modeling"
 },
 {
  "title": "Four short links: 25 July 2017",
  "content": "AI Sentencing, AI Vocabulary, Soft U2F, and Encrypted Email\n\t\nOpening the Lid on Criminal Sentencing -- Duke researchers building a (socially) better algorithm. CORELS makes it possible for judges and defendants to scrutinize why the algorithm classifies a particular person as high or low risk. [...] None of the research team\u2019s models rely on race or socioeconomic status.\n\n\t\nAgents that Imagine and Plan -- the relentless appropriation of terms from cognition and psychology bugs me. But I can't figure out whether I'm long-term wrong, i.e. whether the future will look on the distinction between software and wetware (Ai and human intelligence) as irrelevant.\n\t\nSoft U2F -- Authenticators are normally USB devices that communicate over the HID protocol. By emulating a HID device, Soft U2F is able to communicate with your U2F-enabled browser, and by extension, any websites implementing U2F. Improves site security by preventing phishing. (The magic numbers intercepted for one site can't be reused on another)\n\t\nMagma -- The magma server daemon is an encrypted email system with support for SMTP, POP, IMAP, HTTP and MOLTEN. Additional support for DMTP and DMAP is currently in active development.\n\n\nContinue reading Four short links: 25 July 2017.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5ZlrxGWhDKc/four-short-links-25-july-2017"
 },
 {
  "title": "Data science startups focus on AI-enabled efficiency",
  "content": "Recapping winners of the Strata San Jose Startup Showcase.Every five years, we invent a new technology that, when sprinkled atop existing business problems, acts as a panacea for managers. In the \u201890s it was the Web, followed quickly by SaaS, mobility, clouds, data, and now AI.\n\n\n\nBut there's a bigger underlying pattern. Web and SaaS gave us interfaces anyone could use. Mobility made them ubiquitous, taking us away from the workday\u2014we check our phones dozens of times a day, and, often, they're the last thing we look at before sleep and the first thing we grab upon waking. Clouds gave us elastic, on-demand computing. Big data gave clouds something to do. And AI is a set of algorithms that make sense of that big data, teasing threads of gold from the digital hay.\n\n\n\nTake, for example, the winners of the Strata San Jose Startup Showcase. From a field of applicants, our panel winnowed the list down to 10 finalists, and then a combination of on-site judges and audience voting helped us pick the winners:\n\n\n\tThe first-place winner, Nexla, makes data wrangling easier with a product that allows teams to share, automatically process, transform, and monitor data. Over 80% of time is spent getting data sources to cooperate: cleaning them, moving them around, and ensuring they're consistent and available as companies build data products atop them.\n\n\n\n\n\n\tSecond-place winner Gluent tries to unify the many rivers of data in an organization, pulling them into a single, central, agnostic repository. One truism of AI is that data beats algorithms\u2014so the more data you have at your disposal, the better the models you create and the greater your ability to test them properly, avoiding perils of overfitting and bias.\n\n\n\n\n\n\tThird-place winner Repable uses the torrent of data that gamers generate to understand publisher, brand, and player trends. E-sports is a huge industry, and any early insights into what's happening are invaluable. But gamers are notoriously fickle, and they engage with their audiences across a wide range of platforms. So, pulling this information together and normalizing it is a huge challenge.\n\n\n\n\n\n\tThe popular vote winner, Outlier, harnesses existing business information and looks for anomalies. Most business managers don't worry about what's expected; they care about what's not. Sudden growth, overnight churn, or a rash of alerts often point to something urgent. But patterns in data can often reveal hidden advantages, if the organization has the time to mine it.\n\n\n\n\nThese four companies underscore the unbroken link between on-demand computing, big data, and machine learning. While the \u201890s and \u201coughties\u201d were about building up the front-end user interface\u2014and in the process, making powerful technology simple enough to find billions of users\u2014more recent years have been about laying the groundwork for adaptive, always-aware organizations.\n\n\n\nFull disclosure: Alistair Croll is a board member of Repable; he recused himself from the voting and selection process involving the company during the showcase.\nContinue reading Data science startups focus on AI-enabled efficiency.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/eSXOe0meotk/data-science-startups-focus-on-ai-enabled-efficiency"
 }
]