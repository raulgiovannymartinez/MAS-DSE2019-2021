{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping\n",
    "======\n",
    "Adapted from a page created by [John Beieler](https://gist.github.com/johnb30/4743272)\n",
    "\n",
    "> Even with the best of websites, I don’t think I’ve ever encountered a scraping job that couldn’t be described as *“A small and simple general model with heaps upon  piles of annoying little exceptions”* \n",
    "\n",
    ">> \\- Swizec Teller [http://swizec.com/blog/scraping-with-mechanize-and-beautifulsoup/swizec/5039](http://swizec.com/blog/scraping-with-mechanize-and-beautifulsoup/swizec/5039)\n",
    "\n",
    "## What is it?\n",
    "\n",
    "A large portion of the data that we as social scientists are interested in resides on the web in manner. Web scraping is a method for pulling data from the structured (or not so structured!) HTML that makes up a web page. Python has numerous libraries for approaching this type of problem, many of which are incredibly powerful. If there is something you want to do, there's usually a way to accomplish it. Perhaps not easily, but it can be done. \n",
    "\n",
    "\n",
    "## How is it accomplished?\n",
    "\n",
    "In general, there are three problems that you might face when undertaking a scraping task:\n",
    "\n",
    "1. You have a single page, or a set of pages, that you know of and you want to scrape.\n",
    "2. You have a source that generates links, e.g., [RSS feeds](http://rss.nytimes.com/services/xml/rss/nyt/World.xml), to various pages with the same structure.\n",
    "3. You have a page that contains many pages of interest that are scattered across the file system and you only have general rules for reaching these pages. \n",
    "\n",
    "The key is that you must identify which type of problem you have. After this, you must look at the HTML structure of a webpage and construct a script that will select the parts of the page that are of interest to you.\n",
    "\n",
    "##  There's a library for that! \n",
    "\n",
    "As mentioned previously, Python has various libraries for scraping tasks. The ones I have found the most useful are:\n",
    "\n",
    "- [pattern](http://www.clips.ua.ac.be/pages/pattern)\n",
    "- [lxml](http://lxml.de/)\n",
    "- [requests](https://3.python-requests.org/)\n",
    "- [Scrapy](http://doc.scrapy.org/en/0.16/)\n",
    "- [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package versions\n",
      "--------------------------\n",
      "pattern        : 2.6\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lxml' has no attribute 'etree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-032e848790ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# !pip install lxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"lxml           : {lxml.etree.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#!pip install requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lxml' has no attribute 'etree'"
     ]
    }
   ],
   "source": [
    "#directions for installing these packages and for printing out their versions\n",
    "\n",
    "# !pip install pattern\n",
    "import pattern3\n",
    "print(\"package versions\")\n",
    "print(\"--------------------------\")\n",
    "print(f\"pattern        : {pattern3.__version__}\")\n",
    "\n",
    "# you might face a error due to lack of mysql configs\n",
    "# for mac :\n",
    "#!brew install mysql\n",
    "#!export PATH=$PATH:/usr/local/mysql/bin\n",
    "#!pip install pattern\n",
    "\n",
    "# !pip install lxml\n",
    "import lxml\n",
    "print(f\"lxml           : {lxml.etree.__version__}\")\n",
    "\n",
    "#!pip install requests\n",
    "import requests\n",
    "print(f\"requests       : {requests.__version__}\")\n",
    "\n",
    "#!pip install scrapy\n",
    "import scrapy\n",
    "print(f\"scrapy         : {scrapy.__version__}\")\n",
    "\n",
    "#!pip install beautifulsoup4\n",
    "import bs4\n",
    "print(f\"beautifulsoup4 : {bs4.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a web page\n",
    "let's inspect a [mongabay web page](https://names.mongabay.com/). Open the page in a separate tab, it should look something like the image below. Mongapay is a site that has a nice collection of statistics about first and last names.\n",
    "\n",
    "<img src=\"images/page.png\" alt=\"web page\" height=\"420\" width=\"600\">\n",
    "\n",
    "The source code language for web pages is HTML (Hyper-Text Markup-Language]. HTML is a hierarchical description of the visual content of a page. You can view the source, in Chrome, by choosing `More Tools/Developer Tools`. However, commercial web pages are very complex and inspecting them requires a more powerful method.\n",
    "\n",
    "[mongabay web page](http://names.mongabay.com/data/1000.html)\n",
    "\n",
    "The Chrome browsr comes with such a tool built in. You can open this tool by choosing `More Tools/Developer Tools`\n",
    "\n",
    "If you are using Firefox you can install the [Firebug](https://getfirebug.com/) plugin which gives similar functionality.\n",
    "\n",
    "The bottom half of the browser page will now have a sophisticated development environment for all things web (HTML, CSS, Javascript).\n",
    "In particular it allows you to click on a visual element in the page and find out where it is in the source and what tags it is associated with\n",
    "\n",
    "**note that, image below might be different in newer versions of chrome browser**\n",
    "<img src=\"images/InspectingHTML.png\" alt=\"web page\" height=\"420\" width=\"600\">\n",
    "\n",
    "Note that the element SMITH is surrounded by `<td>` and `</td>` and that this element and all of the elements in that row :`SMITH  1  2376206 ...` are surrounded by `<tr> ..... </tr>`. In HTML-speak we say that each element in the table is of type 'td' and that the whole row is an element if type 'tr' (stands for \"table row\").\n",
    "\n",
    "We will now see how to inspect these elements from the command line using `scrapy shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "\n",
    "* [Official Scrapy Tutorial](http://doc.scrapy.org/en/0.24/intro/tutorial.html)\n",
    "* Other important sections in the Scrapy documentation are **selector** and **Scrapy shell**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scrapy shell\n",
    "*Everything here is done from the terminal window, not inside a notebook*\n",
    "\n",
    "Install scrapy. Use one of those or something similar:\n",
    "> `pip install scrapy`\n",
    "\n",
    "> `sudo pip install scrapy`\n",
    "\n",
    "> `conda install scrapy`\n",
    "\n",
    "Start Scrapy Shell from a terminal window:\n",
    "> `scrapy shell`\n",
    "\n",
    "Fetch a page: \n",
    "> `fetch('http://names.mongabay.com/data/1000.html')`\n",
    "\n",
    "View the page in a Browser:\n",
    "> `view(response)`\n",
    "\n",
    "Get the HTML text of the response:\n",
    "> `response.body`\n",
    "\n",
    "Get just the header\n",
    "> `response.headers`\n",
    "\n",
    "Get all href links to other web pages `<a href=\"http://web.site/file\"> link text </a>`:\n",
    "> `response.xpath('//a')`\n",
    "\n",
    "Filter out of the href links the ones that match a particular regular expression:\n",
    "> `response.xpath('//a').re('data/.+\\.html?')`\n",
    "\n",
    "scrapy contains much more than the shell. You can use scrapy as a library to perform web operations. You can also write a **Spider** or **Crawler** i.e. a program that will follow links to extract and process all of the pages of a particular type from a web site. Later in this notebook there is an example of using Scrapy for crawling. \n",
    "\n",
    "For now, we switch from scrapy to the libraries `requests` and `lxml.html` which are somewhat easier to use for web page parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a page that you know\n",
    "\n",
    "The easiest approach to webscraping is getting the content from a page that you know in advance. I'll go ahead and keep using the page we looked at earlier. There are three basic steps to scraping a single page:\n",
    "\n",
    "1. Get (request) the page\n",
    "2. Parse the page content\n",
    "3. Select the content of interest using an XPath selector\n",
    "\n",
    "The following code executes these three steps and prints the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "\n",
    "url = 'http://names.mongabay.com/data/1000.html'\n",
    "page = requests.get(url)\n",
    "doc = lh.fromstring(page.content)\n",
    "page.content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tag tr (table row) is used in many places, \n",
    "# among them the table of interest to us.\n",
    "# we can identify those rows by the fact that \n",
    "# the table contains 11 columns.\n",
    "tr_elements = doc.xpath('//tr')\n",
    "for T in tr_elements[:20]:\n",
    "    for e in T.iterchildren():\n",
    "        print(e.text_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tr_elements[0]), len(tr_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element=tr_elements[7]\n",
    "#element.  # uncomment this line and hit <tab> after the dot to see the methods and properties of an HTML element"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(tr_elements)):\n",
    "    if len(tr_elements[i])==15:\n",
    "        print(i, tr_elements[i].text_content())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=[]  # collect column names into col\n",
    "T=tr_elements[0]\n",
    "print(type(T))\n",
    "i=0\n",
    "print(len(T))\n",
    "for t in T.iterchildren():\n",
    "    i+=1\n",
    "    name=t.text_content()\n",
    "    print('%d:\"%s\"'%(i,name))\n",
    "    col.append((name,[]))\n",
    "\n",
    "print('the columns are:',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,len(tr_elements)):\n",
    "    #print(j)\n",
    "    T=tr_elements[j]\n",
    "    #print(len(T))\n",
    "    if len(T)!=15:\n",
    "        break\n",
    "    i=0\n",
    "    for t in T.iterchildren():\n",
    "        data=t.text_content()\n",
    "        if i>0:\n",
    "            try:\n",
    "                data=float(data if '%' not in data else data.split('%')[0])\n",
    "            except:\n",
    "                print(data,'cannot be converted to float, row,col=',j,i)\n",
    "                data=None\n",
    "        col[i][1].append(data)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(C) for (title,C) in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len=min([len(C) for (title,C) in col])\n",
    "min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict={title:column for (title,column) in col}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(Dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have our lovely output. This output can be manipulated in various ways, or written to an output file.\n",
    "\n",
    "### Scraping generated links\n",
    "\n",
    "Let's say you want to get a stream of news stories in an easy manner. You could visit the homepage of the NYT and work from there, or you can use an [RSS feed](http://rss.nytimes.com/services/xml/rss/nyt/World.xml). RSS stands for Real Simple Syndication and is, at its heart, an XML document. This allows it to be easily parsed. The fantastic library `pattern` allows for easy parsing of RSS feeds. Using `pattern`'s `Newsfeed()` method, it is possible to parse a feed and obtain attributes of the XML document. The `search()` method returns an iterable composed of the individual stories. Each result has a variety of attributes such as `.url`, `.title`, `.description`, and more. The following code demonstrates these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern.web\n",
    "num=5;\n",
    "url = 'http://rss.nytimes.com/services/xml/rss/nyt/World.xml'\n",
    "results = pattern.web.Newsfeed().search(url, count=num)\n",
    "\n",
    "print('The current top headers from the NY times are:')\n",
    "for i in range(num):\n",
    "    print (\"%d\\t%s\"%(i,results[i].title))\n",
    "\n",
    "print('\\n\\nURL: %s \\n\\n Header\\n%s \\n\\nFull Article\\n %s \\n' % (results[0].url, results[0].title,\n",
    "                                                                  results[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "That looks pretty good, but the description looks nastier than we would generally prefer. Luckily, `pattern` provides functions to get rid of the HTML in a string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%s \\n\\n %s \\n\\n %s \\n\\n' % (results[0].url, results[0].title, pattern.web.plaintext(results[0].description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's all well and good to have the title and description of a story this is often insufficient (some descriptions are just the title, which isn't particularly helpful). To get further information on the story, it is possible to combine the single-page scraping discussed previously and the results from the RSS scrape. The following code implements a function to scrape the NYT article pages, which can be done easily since the NYT is wonderfully consistent in their HTML, and then iterates over the results applying the `scrape` function to each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "outputFile = codecs.open('tutorialOutput.txt', encoding='utf-8', mode='w')\n",
    "\n",
    "def scrape(url):\n",
    "    page = requests.get(url)\n",
    "    #print(page.content)\n",
    "    doc = lh.fromstring(page.content)\n",
    "    #print(doc)\n",
    "    text = doc.xpath('//p[@id=\"article-summary\"]') #('//p[@itemprop=\"<find required id here, article body doesn't seem to be present>\"]') \n",
    "    finalText = str()\n",
    "    for par in text:\n",
    "        finalText += par.text_content()\n",
    "    return finalText+'\\n\\n\\n'\n",
    "\n",
    "for result in results:\n",
    "    #print(result.url)\n",
    "    outputText = scrape(result.url)\n",
    "    #print(outputText)\n",
    "    outputFile.write(outputText)\n",
    "\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -4 tutorialOutput.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping arbitrary websites\n",
    "\n",
    "The final approach is for a webpage that contains information you want and the pages are spread around in a fairly consistent manner, but there is no simple, straightfoward manner in which the pages are named.\n",
    "\n",
    "I'll offer a brief aside here to mention that it is often possible to make slight modifications to the URL of a website and obtain many different pages. For example, a website that contains Indian parliament speeches has the URL `http://164.100.47.132/LssNew/psearch/Result13.aspx?dbsl=` with differing values appended after the `=`. Thus, using a `for-loop` allows for the programatic creation of different URLs. Some sample code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://164.100.47.132/LssNew/psearch/Result13.aspx?dbsl='\n",
    "\n",
    "for i in range(5175,5973):\n",
    "    newUrl = url + str(i)\n",
    "    print('Scraping: %s' % newUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back on topic, it is often more difficult than the above to iterate over numerous webpages within a site. This is where the `Scrapy` library comes in. `Scrapy` allows for the creation of web spiders that crawl over a webpage, following any links that it finds. This is often far more difficult to implement than a simple scraper since it requires the identification of rules for link following. The [State Department](http://www.state.gov/r/pa/prs/dpb/2012/index.htm) offers a good example. I don't really have time to go into the depths of writing a `Scrapy` spider, but I thought I would put up some code to illustrate what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "from scrapy.item import Item\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "class MySpider(CrawlSpider):\n",
    "    name = 'statespider' #name is a name\n",
    "    start_urls = ['http://www.state.gov/r/pa/prs/dpb/2010/index.htm',] #defines the URL that the spider should start on. adjust the year.\n",
    "\n",
    "    #defines the rules for the spider\n",
    "    rules = (Rule(LinkExtractor(allow=('/2010/'), restrict_xpaths=('//*[@id=\"local-nav\"]'),)),\n",
    "             #allows only links within the navigation panel that have /year/ in them.\n",
    "             Rule(LinkExtractor(restrict_xpaths=('//*[@id=\"dpb-calendar\"]',), deny=('/video/')),\n",
    "                  callback='parse_item'),\n",
    "             #follows links within the caldendar on the index page for the individuals years, while denying any links with /video/ in them\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.log('Hi, this is an item page! %s' % response.url) #prints the response.url out in the terminal to help with debugging\n",
    "        \n",
    "        #Insert code to scrape page content\n",
    "\n",
    "        #opens the file defined above and writes 'texts' using utf-8\n",
    "        with codecs.open(filename, 'w', encoding='utf-8') as output:\n",
    "            output.write(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Google news provides RSS feeds that can be filtered, at the source, according to the country, the language and a query term. See [this description](http://thinktostart.com/creating-custom-rss-feeds-with-google/).\n",
    "\n",
    "Create a feed for all spanish news about san diego that prints the latest 5 headers and descriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Pitfalls of Webscraping\n",
    "\n",
    "Web scraping is much, much, *much*, more of an art than a science. It is often non-trivial to identify the XPath selector that will get you what you want. Also, some web programmers can't seem to decide how they want to structure the pages they write, so they just change the HTML every few pages. Notice that for the NYT example if `articleBody` gets changed to `articleBody1`, everything breaks. There are ways around this that are often convoluted, messy, and hackish. Usually, however, where there is a will there is a way.\n",
    "\n",
    "...brief pause to demonstrate the lengths this can go to.\n",
    "\n",
    "##PITF Human Atrocities\n",
    "\n",
    "As a wrap up, I show the workflow I have been using to perform real-time scraping from various news sites of stories pertaining to human atrocities. This is illustrative both of web scraping and of the issues that can accompany programming. \n",
    "\n",
    "The general flow of the scraper is:\n",
    "\n",
    "RSS feed -> identify relevant stories -> scrape story -> place results in mongoDB -> repeat every hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
