{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OAuth Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to scrape twitter data and do a tf-idf analysis on that (src-uwes twitter analysis). We will need OAuth authentication, and we will follow a similar approach as detailed in the yelp analysis notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define the following variables\n",
    "#api_key = #<get api key> \n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "\n",
    "#defining them right here is not safe. insteadm create a file in a different directory\n",
    "# (I use ~/VaultDSE) and in it put a file called, say, twitterkeys.py whose\n",
    "# content is:\n",
    "#api_key = #<get api key>\n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "#\n",
    "#def getkeys():\n",
    "#    return api_key,api_secret,access_token_key,access_token_secret\n",
    "\n",
    "# then use the following commands\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/raulmartinez/Documents/VaultDSE') # write your own path here\n",
    "import TwitterKeys\n",
    "api_key,api_secret,access_token_key,access_token_secret=TwitterKeys.getkeys()\n",
    "\n",
    "#api_key,api_secret,access_token_key,access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workaround for SSL Verification\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.request.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.request.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "    req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "    req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "    headers = req.to_header()\n",
    "\n",
    "    if http_method == \"POST\":\n",
    "        encoded_post_data = req.to_postdata()\n",
    "    else:\n",
    "        encoded_post_data = None\n",
    "        url = req.to_url()\n",
    "\n",
    "    opener = urllib.request.OpenerDirector()\n",
    "    opener.add_handler(http_handler)\n",
    "    opener.add_handler(https_handler)\n",
    "\n",
    "    response = opener.open(url, encoded_post_data)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stream.twitter.com/1.1/statuses/sample.json\"\n",
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"delete\":{\"status\":{\"id\":1198242897365127168,\"id_str\":\"1198242897365127168\",\"user_id\":147534903,\"user_id_str\":\"147534903\"},\"timestamp_ms\":\"1575255708231\"}}\\r\\n'\n",
      "\n",
      "\n",
      "b'{\"delete\":{\"status\":{\"id\":187608948822638593,\"id_str\":\"187608948822638593\",\"user_id\":135626797,\"user_id_str\":\"135626797\"},\"timestamp_ms\":\"1575255708858\"}}\\r\\n'\n",
      "\n",
      "\n",
      "b'{\"delete\":{\"status\":{\"id\":655458708751826944,\"id_str\":\"655458708751826944\",\"user_id\":2511682935,\"user_id_str\":\"2511682935\"},\"timestamp_ms\":\"1575255709511\"}}\\r\\n'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the first three responses\n",
    "count = 0\n",
    "for line in response:\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    count+=1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1.1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def find_key_value(key, dict_):\n",
    "    \"\"\"\n",
    "    Function taken from the website below:\n",
    "    https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-python-dictionaries-and-lists\n",
    "    \n",
    "    Not used below, I just took it for testing purposes\n",
    "    \"\"\"\n",
    "    for k, v in (dict_.items() if isinstance(dict_, dict) else\n",
    "               enumerate(dict_) if isinstance(dict_, list) else []):\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, (dict, list)):\n",
    "            for result in find_key_value(key, v):\n",
    "                yield result\n",
    "\n",
    "def fetchData(url, key, top_lines):\n",
    "    \n",
    "    #get twitter's response\n",
    "    parameters = []\n",
    "    response = getTwitterStream(url, \"GET\", parameters)\n",
    "    \n",
    "    #save top lines\n",
    "    response_top = []\n",
    "    i = 0\n",
    "    for line in response:\n",
    "        \n",
    "        try:\n",
    "            #convert bytest to dictionary using json library\n",
    "            line_tweet_text = json.loads(line.decode('utf-8'))['text']\n",
    "            #find key value on multi-level dictionary, use next because function returns a generator (not used)\n",
    "#             response_top.append(next(find_key_value(key,line_tweet_text))) \n",
    "            response_top.append(line_tweet_text) \n",
    "\n",
    "        except KeyError:\n",
    "            response_top.append(np.nan)\n",
    "        \n",
    "        #break when top_lines value are found\n",
    "        i+=1\n",
    "        if i == top_lines: break\n",
    "    \n",
    "    #return response\n",
    "    return response_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Naalala ko bigla wala pala ako utakkk hhshshsh gsnqeqrgyg',\n",
       " '귀엽군',\n",
       " '@Yes4G Hi,hahaha napee😂',\n",
       " '君が君らしく居てくれた時に 僕は僕らしく居られたかなぁ？',\n",
       " 'RT @Strangestone: 月曜日のたわわ\\u3000その２５０ 『愛猫写真フォルダ』 https://t.co/vSUFrzDcx5',\n",
       " 'RT @_honeyjimin: แจก 100 บาท\\nเนื่องจากครอบครัวได้ 8 รางวัลเมื่อวาน\\n*เฉพาะอาร์มี่ที่ฟอลโลเวอร์ \\n\\nรี+ติดแท็ก\\n#MAMAVOTE #bts @BTS_twt \\n\\nประกาศ…',\n",
       " '出)レベルボール入りヒトツキ\\n     孵化余りミミッキュ\\n     孵化余りダルマッカ\\n     他ソード限定ポケモン\\n\\n求)夢サニーゴ\\n\\n#ポケモン剣盾交換 \\n#ポケモン交換']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test fetchData function for stream data\n",
    "response_tweet = fetchData(url = \"https://stream.twitter.com/1.1/statuses/sample.json\", key = 'text', top_lines=10)\n",
    "response_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also request twitter stream data for specific search parameters as follows\n",
    "search_query = 'example'\n",
    "url= \"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_query #changed to stream API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 5 lines for \"UCSD\"\n",
    "search_query = 'UCSD'\n",
    "response_tweet = fetchData(\"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_query, key = 'text', top_lines=5)\n",
    "response_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @Otto_English: \"If all of us were caught out ... on a night out after the drink\"\\n\\nNigel Farage defending Donald Trump while raising some…',\n",
       " '🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣',\n",
       " 'RT @charliekirk11: Facts:\\n\\nJoe Biden held a rally in Crawford County, Iowa—where 67% voted for Trump in 2016\\n\\n125 people showed up\\n\\nDonald…',\n",
       " 'RT @Timcast: News Media is collapsing, salaries are slashed, thousands are laid off, ragebait runs rampant in a desperate bid to bring in m…',\n",
       " 'RT @SpockResists: If Donald Trump is not impeached and manages to cheat to a second term. \\nAmerica is done. His supporters do not understan…']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 5 lines for \"Donald Trump\"\n",
    "search_query = 'Donald Trump'\n",
    "response_tweet = fetchData(\"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_query, key = 'text', top_lines=5)\n",
    "response_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @evanchill: “Sent candy.” That’s how a Russian pilot confirmed his airstrike on a busy Syrian street this July. Our new investigation re…',\n",
       " 'RT @shane_bauer: So Halper and @mtaibbi think, “Hey, we should do a show on Syria for Rolling Stone. Who should we have on to explain? The…',\n",
       " 'RT @minhtngo: #Lebanon: Protesters chant — \"revolution in every country\" — in solidarity with pro-democracy movements around the world from…',\n",
       " 'RT @GmanFan45: @SpeakerPelosi your net worth is $195 million. Why are you mooching off my money to get sloshed in Spain??? On a climate hoa…',\n",
       " 'RT @LoriHandrahan2: Remember #Syria https://t.co/LZ6ap6ZqN7']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 5 lines for \"Syria\"\n",
    "search_query = 'Syria'\n",
    "response_tweet = fetchData(\"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_query, key = 'text', top_lines=5)\n",
    "response_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 'Sat Nov 30 06:57:01 +0000 2019', 'id': 1200670047212949524, 'id_str': '1200670047212949524', 'text': 'RT @Knit_The_Terror: Reading about the USCGC Maple transiting the Northwest Passage in 2017. Harry Goodsir would be pleased with the zoopla…', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'Knit_The_Terror', 'name': 'Jillian | Mayhem in a High Degree', 'id': 824752702978658304, 'id_str': '824752702978658304', 'indices': [3, 19]}], 'urls': []}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 888996200225157120, 'id_str': '888996200225157120', 'name': 'We Stan Henry Goodsir', 'screen_name': 'Saint_Hemlock', 'location': 'https://ko-fi.com/A7336VR', 'description': 'Queer 🔞\\nHe/Him/They/Them cryptid NB \\n26\\nLazy pagan\\nsurvivor \\nI write queer monsters\\nOh god I love old men\\nAll about them Icy Terror Boys\\nPfp by @brokemycrown', 'url': 'https://t.co/XoQIDAfFAp', 'entities': {'url': {'urls': [{'url': 'https://t.co/XoQIDAfFAp', 'expanded_url': 'https://www.amazon.com/hz/wishlist/ls/1JMTVJ4RD1NZD', 'display_url': 'amazon.com/hz/wishlist/ls…', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 271, 'friends_count': 350, 'listed_count': 1, 'created_at': 'Sun Jul 23 05:36:25 +0000 2017', 'favourites_count': 10000, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 38330, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1190388634358370304/WpwHWgSo_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1190388634358370304/WpwHWgSo_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/888996200225157120/1572644766', 'profile_link_color': '8CA6B5', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'retweeted_status': {'created_at': 'Sat Aug 03 02:33:58 +0000 2019', 'id': 1157479693190356997, 'id_str': '1157479693190356997', 'text': 'Reading about the USCGC Maple transiting the Northwest Passage in 2017. Harry Goodsir would be pleased with the zoo… https://t.co/LcgDuTH6YW', 'truncated': True, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/LcgDuTH6YW', 'expanded_url': 'https://twitter.com/i/web/status/1157479693190356997', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 824752702978658304, 'id_str': '824752702978658304', 'name': 'Jillian | Mayhem in a High Degree', 'screen_name': 'Knit_The_Terror', 'location': 'New Hampshire, USA', 'description': 'History M.A. now spending most of my time knitting. Liable to yell about #TheTerror, #GoodOmens, birds, and prog rock. 29/she/her.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 110, 'friends_count': 194, 'listed_count': 0, 'created_at': 'Thu Jan 26 22:55:42 +0000 2017', 'favourites_count': 1828, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1157, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1193013965242208256/U_Hp_TN5_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1193013965242208256/U_Hp_TN5_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/824752702978658304/1570850556', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 2, 'favorite_count': 5, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}, 'is_quote_status': False, 'retweet_count': 2, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}\n",
      "\n",
      "\n",
      "{'created_at': 'Sat Nov 30 06:36:00 +0000 2019', 'id': 1200664758745104384, 'id_str': '1200664758745104384', 'text': 'Another @CASeaGrant fellow making us proud! @KatieRFilipp @drewtalley \\nhttps://t.co/EEji5VlDt0', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'CASeaGrant', 'name': 'California Sea Grant', 'id': 216094888, 'id_str': '216094888', 'indices': [8, 19]}, {'screen_name': 'KatieRFilipp', 'name': 'Katie Filipp', 'id': 1244164333, 'id_str': '1244164333', 'indices': [44, 57]}, {'screen_name': 'drewtalley', 'name': 'Fundulus', 'id': 16098884, 'id_str': '16098884', 'indices': [58, 69]}], 'urls': [{'url': 'https://t.co/EEji5VlDt0', 'expanded_url': 'https://caseagrant.ucsd.edu/blogs/the-grief-of-climate-change-how-to-avoid-burnout-in-the-climate-crisis', 'display_url': 'caseagrant.ucsd.edu/blogs/the-grie…', 'indices': [71, 94]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 4403679252, 'id_str': '4403679252', 'name': 'Christine Whitcraft', 'screen_name': 'whitcraftmudlab', 'location': '', 'description': 'Official account for CSULB Wetlands Ecology Lab. A mud-loving wetland ecologist, educator, and ocean enthusiast. she | her | hers', 'url': 'https://t.co/pa6DaPm2W9', 'entities': {'url': {'urls': [{'url': 'https://t.co/pa6DaPm2W9', 'expanded_url': 'http://www.csulb.edu/~cwhitcra', 'display_url': 'csulb.edu/~cwhitcra', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 482, 'friends_count': 342, 'listed_count': 16, 'created_at': 'Sun Nov 29 23:20:38 +0000 2015', 'favourites_count': 1585, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 550, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/671106946322202624/IzFVg7d7_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/671106946322202624/IzFVg7d7_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/4403679252/1448949503', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 2, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}\n",
      "\n",
      "\n",
      "{'created_at': 'Sat Nov 30 06:10:11 +0000 2019', 'id': 1200658263219539968, 'id_str': '1200658263219539968', 'text': 'Congrats. Happy to see that the community effort of MIADB hosted on @GNPS_UCSD is used  by researchers all over the… https://t.co/AjUY6UMCJZ', 'truncated': True, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'GNPS_UCSD', 'name': 'GNPS - UC San Diego', 'id': 2521441274, 'id_str': '2521441274', 'indices': [68, 78]}], 'urls': [{'url': 'https://t.co/AjUY6UMCJZ', 'expanded_url': 'https://twitter.com/i/web/status/1200658263219539968', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 778517155, 'id_str': '778517155', 'name': 'Mehdi Beniddir', 'screen_name': 'BeniddirM', 'location': 'Paris', 'description': 'Associate professor of natural products chemistry at Université Paris-Saclay. Discovery of unprecedented natural products', 'url': 'https://t.co/QCQVvNL8cD', 'entities': {'url': {'urls': [{'url': 'https://t.co/QCQVvNL8cD', 'expanded_url': 'http://www.biocis.u-psud.fr/?-BENIDDIR-Mehdi-', 'display_url': 'biocis.u-psud.fr/?-BENIDDIR-Meh…', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 489, 'friends_count': 941, 'listed_count': 8, 'created_at': 'Fri Aug 24 16:25:00 +0000 2012', 'favourites_count': 2135, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1505, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/903555601032237056/hqNVReFQ_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/903555601032237056/hqNVReFQ_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/778517155/1513950230', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': True, 'quoted_status_id': 1200581731780313089, 'quoted_status_id_str': '1200581731780313089', 'quoted_status': {'created_at': 'Sat Nov 30 01:06:05 +0000 2019', 'id': 1200581731780313089, 'id_str': '1200581731780313089', 'text': 'Thanks to the developers!! What an amazing tool. @GNPS_UCSD https://t.co/5lrMm7BbKm', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'GNPS_UCSD', 'name': 'GNPS - UC San Diego', 'id': 2521441274, 'id_str': '2521441274', 'indices': [49, 59]}], 'urls': [], 'media': [{'id': 1200581705201070081, 'id_str': '1200581705201070081', 'indices': [60, 83], 'media_url': 'http://pbs.twimg.com/media/EKlSsy5XsAEaKeh.jpg', 'media_url_https': 'https://pbs.twimg.com/media/EKlSsy5XsAEaKeh.jpg', 'url': 'https://t.co/5lrMm7BbKm', 'display_url': 'pic.twitter.com/5lrMm7BbKm', 'expanded_url': 'https://twitter.com/HectorKoolen/status/1200581731780313089/photo/1', 'type': 'photo', 'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'large': {'w': 1280, 'h': 720, 'resize': 'fit'}, 'small': {'w': 680, 'h': 383, 'resize': 'fit'}, 'medium': {'w': 1200, 'h': 675, 'resize': 'fit'}}}]}, 'extended_entities': {'media': [{'id': 1200581705201070081, 'id_str': '1200581705201070081', 'indices': [60, 83], 'media_url': 'http://pbs.twimg.com/media/EKlSsy5XsAEaKeh.jpg', 'media_url_https': 'https://pbs.twimg.com/media/EKlSsy5XsAEaKeh.jpg', 'url': 'https://t.co/5lrMm7BbKm', 'display_url': 'pic.twitter.com/5lrMm7BbKm', 'expanded_url': 'https://twitter.com/HectorKoolen/status/1200581731780313089/photo/1', 'type': 'photo', 'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'large': {'w': 1280, 'h': 720, 'resize': 'fit'}, 'small': {'w': 680, 'h': 383, 'resize': 'fit'}, 'medium': {'w': 1200, 'h': 675, 'resize': 'fit'}}}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1156682378791792640, 'id_str': '1156682378791792640', 'name': 'Hector Koolen', 'screen_name': 'HectorKoolen', 'location': '', 'description': 'Adjunct Professor of Organic Chemistry at @UEAmazonas - Brazil working with #naturalproducts of microbes and plants from the #Amazon with #massspectrometry', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 152, 'friends_count': 388, 'listed_count': 1, 'created_at': 'Wed Jul 31 21:45:44 +0000 2019', 'favourites_count': 1834, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 103, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1156683623766081536/clnI6max_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1156683623766081536/clnI6max_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 4, 'favorite_count': 11, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}\n",
      "\n",
      "\n",
      "{'created_at': 'Sat Nov 30 06:08:12 +0000 2019', 'id': 1200657763992580096, 'id_str': '1200657763992580096', 'text': 'RT @dataMares: En 1975 se establece una veda permanente a la pesca de #totoaba. ¿Qué información existe sobre los orígenes y la evolución d…', 'truncated': False, 'entities': {'hashtags': [{'text': 'totoaba', 'indices': [70, 78]}], 'symbols': [], 'user_mentions': [{'screen_name': 'dataMares', 'name': 'dataMares', 'id': 509827749, 'id_str': '509827749', 'indices': [3, 13]}], 'urls': []}, 'metadata': {'iso_language_code': 'es', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 890454867571027968, 'id_str': '890454867571027968', 'name': 'Thelma', 'screen_name': 'Thelma89300038', 'location': 'México', 'description': 'Ciencia, arte, tecnología, historia...', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 18, 'friends_count': 56, 'listed_count': 0, 'created_at': 'Thu Jul 27 06:12:39 +0000 2017', 'favourites_count': 21, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 4528, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png', 'profile_image_url_https': 'https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/890454867571027968/1507756243', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': True, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'retweeted_status': {'created_at': 'Fri Nov 29 17:30:18 +0000 2019', 'id': 1200467032707338241, 'id_str': '1200467032707338241', 'text': 'En 1975 se establece una veda permanente a la pesca de #totoaba. ¿Qué información existe sobre los orígenes y la ev… https://t.co/3YdVwTwpLp', 'truncated': True, 'entities': {'hashtags': [{'text': 'totoaba', 'indices': [55, 63]}], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/3YdVwTwpLp', 'expanded_url': 'https://twitter.com/i/web/status/1200467032707338241', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'es', 'result_type': 'recent'}, 'source': '<a href=\"https://www.hootsuite.com\" rel=\"nofollow\">Hootsuite Inc.</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 509827749, 'id_str': '509827749', 'name': 'dataMares', 'screen_name': 'dataMares', 'location': '', 'description': 'Share & consult #data on marine ecosystems. | Comparte y consulta #datos sobre ecosistemas marinos. #dataMares #infoposterdm #transparencia', 'url': 'https://t.co/TETVCkqpes', 'entities': {'url': {'urls': [{'url': 'https://t.co/TETVCkqpes', 'expanded_url': 'http://www.datamares.org', 'display_url': 'datamares.org', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 1031, 'friends_count': 625, 'listed_count': 75, 'created_at': 'Thu Mar 01 06:54:05 +0000 2012', 'favourites_count': 910, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1986, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/560544164022992897/2lCUAPCm_normal.jpeg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/560544164022992897/2lCUAPCm_normal.jpeg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/509827749/1574717530', 'profile_link_color': '1B95E0', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 2, 'favorite_count': 2, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'es'}, 'is_quote_status': False, 'retweet_count': 2, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'es'}\n",
      "\n",
      "\n",
      "{'created_at': 'Sat Nov 30 05:55:02 +0000 2019', 'id': 1200654450936795138, 'id_str': '1200654450936795138', 'text': '@briancheeek @MikeSigner @PeteButtigieg Yeah bro, your toothless plan that leaves millions in debt not beneficial a… https://t.co/zFt16tAqEu', 'truncated': True, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'briancheeek', 'name': 'Brian Cheek', 'id': 550005004, 'id_str': '550005004', 'indices': [0, 12]}, {'screen_name': 'MikeSigner', 'name': 'Mike Signer', 'id': 19500844, 'id_str': '19500844', 'indices': [13, 24]}, {'screen_name': 'PeteButtigieg', 'name': 'Pete Buttigieg', 'id': 226222147, 'id_str': '226222147', 'indices': [25, 39]}], 'urls': [{'url': 'https://t.co/zFt16tAqEu', 'expanded_url': 'https://twitter.com/i/web/status/1200654450936795138', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', 'in_reply_to_status_id': 1200652775316742149, 'in_reply_to_status_id_str': '1200652775316742149', 'in_reply_to_user_id': 550005004, 'in_reply_to_user_id_str': '550005004', 'in_reply_to_screen_name': 'briancheeek', 'user': {'id': 1135771273563262976, 'id_str': '1135771273563262976', 'name': 'Chris H', 'screen_name': 'Chris_H_Politic', 'location': '', 'description': 'Political activist. LGBT. Progressive voter. He / him. U of Chicago MBA. Ready to vote Trump out (or lock  him up) 🏳️\\u200d🌈🌹 #MedicareForAll #BernieOrWarren2020', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 2581, 'friends_count': 3891, 'listed_count': 4, 'created_at': 'Tue Jun 04 04:52:28 +0000 2019', 'favourites_count': 55303, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 4447, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1141212752502853632/xk04iZxC_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1141212752502853632/xk04iZxC_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1135771273563262976/1564557221', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 1, 'favorited': False, 'retweeted': False, 'lang': 'en'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example on how the search query is organized. I just printed this for testing when I was trying to implement it. \n",
    "#https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "#but I then switched to use a stream track query instread\n",
    "\n",
    "search_query = 'UCSD'\n",
    "url = \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)\n",
    "\n",
    "test = [json.loads(i.decode('utf-8')) for i in response]\n",
    "    \n",
    "# test.keys()\n",
    "#print first 5\n",
    "for i in test[0]['statuses'][:5]:\n",
    "    print(i)\n",
    "    print('\\n')\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Divide the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitterreq(url, key, top_lines):\n",
    "    \n",
    "    #get twitter's response\n",
    "    parameters = []\n",
    "    response = getTwitterStream(url, \"GET\", parameters)\n",
    "    \n",
    "    #save top lines\n",
    "    response_top = []\n",
    "    i = 0\n",
    "    for line in response:\n",
    "        \n",
    "        try:\n",
    "            #convert bytest to dictionary using json library\n",
    "            line_tweet_text = json.loads(line.decode('utf-8'))['text']\n",
    "            response_top.append(line_tweet_text) \n",
    "\n",
    "        except KeyError:\n",
    "            response_top.append(np.nan)\n",
    "        \n",
    "        #break when top_lines value are found\n",
    "        i+=1\n",
    "        if i == top_lines: break\n",
    "    \n",
    "    #save tweets as text\n",
    "    tweets_file = open(\"twitterStream.txt\",\"w\")  \n",
    "    for tweet_element in response_top:\n",
    "        tweets_file.write(tweet_element)\n",
    "        tweets_file.write('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "    tweets_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulmartinez/Documents/DSE/2019-rgm001/DSE200/day_5_mining_the_Social_web/exercises\r\n"
     ]
    }
   ],
   "source": [
    "#workind directory where file is saved\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "\n",
    "search_query = 'Syria'\n",
    "twitterreq(\"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_query, key = 'text', top_lines=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_No</th>\n",
       "      <th>word</th>\n",
       "      <th>word count</th>\n",
       "      <th>word total</th>\n",
       "      <th>term frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>@kurdistannews24</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>rojava</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>riseup4rojava</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>we</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>womendefendrojava</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_No               word  word count  word total  term frequency\n",
       "0  tweet_1   @kurdistannews24           1          15        0.066667\n",
       "1  tweet_1             rojava           3          15        0.200000\n",
       "2  tweet_1      riseup4rojava           1          15        0.066667\n",
       "3  tweet_1                 we           1          15        0.066667\n",
       "4  tweet_1  womendefendrojava           1          15        0.066667"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "\n",
    "#load saved file\n",
    "tweets_file = open(\"twitterStream.txt\",\"r\") \n",
    "inData = tweets_file.readlines()\n",
    "tweets_file.close() \n",
    "\n",
    "#count occurences for each term in each tweet\n",
    "inData_tweet = ''.join(inData).split('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "inData_tweet = inData_tweet[:len(inData_tweet)-1]\n",
    "frequency = []\n",
    "tweet_count = 0\n",
    "for tweet_ in inData_tweet:\n",
    "    \n",
    "    #remove certain characters next to words\n",
    "    list_ = ['(',')','{','}','[',']',',',':','.','?','!','\"','-','_','\\n','#','/','|']\n",
    "    for i in list_:\n",
    "        tweet_ = tweet_.replace(i,'')\n",
    "\n",
    "    #remove non-ascii characters\n",
    "    tweet_ = tweet_.encode('ascii',errors='ignore').decode()\n",
    "    \n",
    "    #convert every letter to lower-case\n",
    "    tweet_ = tweet_.lower()\n",
    "    \n",
    "    #save all words from tweet as a set, remove empty entries on word list\n",
    "    word_list = [i for i in list(set(tweet_.split(' '))) if i != '']\n",
    "    \n",
    "    #save word and counts as tuples (tweet#,word,word count,word total,term frequency)\n",
    "    tweet_count+=1  \n",
    "    for word in word_list:\n",
    "        frequency.append(('tweet_'+str(tweet_count),word,tweet_.count(word),len(word_list),tweet_.count(word)/len(word_list)))\n",
    "\n",
    "#convert to dataframe and print head\n",
    "import pandas as pd\n",
    "df_frequency = pd.DataFrame(frequency,columns=['tweet_No','word','word count','word total','term frequency'])\n",
    "df_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['$195', '&amp;', \"'meat\", '1', '1%', '100', '100%', '11', '12', '120', '120km', '14%', '17', '1st', '2', '2010', \"2018'deki\", '25', '3', '30', '30k', '32', '336', '40', '5', '547', '8', '85', '87', '98%', '@02indian11', '@21wire', '@aaronjmate', '@abarnardnyt', '@afshinrattansi', '@ahvalen', '@ajenglish', '@ambjohnbolton', '@arktinentuuli', '@arnoldisugly', '@astroehlein', '@azadirojava', '@bbcworld', '@bhl', '@borisjohnson', '@canadanato', '@chandershekharj', '@clarkemicah', \"@clarkemicah's\", '@contesadesange', '@cornishdamo', '@csojourner', '@davidwohl', '@dearauntcrabby', '@desha7', '@dgisserious', '@dknato', '@dlme', '@dublencoigor', '@ekaraduman1070', '@elizabethrajya4', '@emmanuelmacron', '@eucommission', '@euparlement', '@euparliament1', '@eupublicaffairs', '@evanchill', '@evrenwiltse', '@farnazfassihi', '@ferdiknd', '@franceotan', '@freemediahub', '@garzillaman', '@germanydiplo', '@germanynato', '@globeandmail', '@gmanfan45', '@gppi', '@greens', '@gulnuray', '@hevallo', '@hostagenotes', '@hrw', '@hudamalaysia', '@hylchil', '@ianfletcher05', '@icrc', '@ifindkarma', '@ifla', '@iglv05', '@impulse', '@islamicat', '@italyatnato', '@janetrice', '@jennrollins1002', '@jensstoltenberg', '@jeremycorbyn', '@jimmydore', '@jnhanvey', '@joeyayoub', '@josiensor', '@kabirtaneja', '@kthalps', '@kurdistannews24', '@libyaliberty', '@lindseygrahamsc', '@mail', '@malachybrowne', '@martinglasenapp', '@maxblumenthal', '@mazjobrani', '@mihirssharma', '@minhtngo', '@mohmad70872216', '@mollyjongfast', '@mooncult', '@mountainrizz', '@mtaibbi', '@muhammadnajem20', '@muserebel', '@mutludc', '@myworldmysun', '@naderalihashemi', '@naimbaburoglu', '@nashvilleskies', '@nato', '@natseclisa', '@nfrechen', '@nlatnato', '@noogman', '@norwaynato', '@nytimes', '@opcw', '@pauliddon', '@politicususa', '@progressoccupy', '@pvoneulenburg', '@rachaelswindon', '@ralee85', '@ranterred', '@rashadzali1', '@realdonaldtr', '@realdonaldtrump', '@rebeloutlaw5', '@redsteeze', '@repthomasmassie', '@retweetprayers', '@rusembsyria', '@sahl', '@sahouraxo', '@sakrruth', '@samanthajpower', '@samar11', '@sarahboxer', '@shelaco', '@sheydemann', '@simonawood', '@skayyali1', '@snarwani', '@speakerpelosi', '@starrcongress', '@stefsimanowitz', '@suegrant54321', '@syriandeveloper', '@syrianofficial', '@syrtelevision', '@tellyoursonthis', '@tercorec', '@thearabsource', '@theclearcider', '@thenewsminute', '@thesmirker', '@thotcrime', '@towersight', '@trbrtc', '@tulsigabbard', '@turkialowerde', '@uknato', '@un', '@unescogccyemen', '@vincentbrowne', '@vp', '@walid970721', '@wbrown10', '@words4cat', 'a', 'aamiin', 'abd', 'about', 'abroad', 'abusing', 'accidentally', 'accompanied', 'accounting', 'actionthis', 'actively', 'activist', 'activity', 'actually', 'admitting', 'afghanistan', 'africa', 'after', 'aftermath', 'again', 'against', 'agree', 'agrees', 'aiding', 'ainissa', 'air', 'airstrike', 'airstrikes', 'aleppo', 'aliyah', 'aljazeera', 'all', 'allah', 'allegations', 'alliance', 'allies', 'alm', 'alqaeda', 'alrahma', 'also', 'america', 'amid', 'amik', 'an', 'anakanak', 'analogies', 'and', 'angry', 'annex', 'another', 'antiquities', 'antisaudi', 'any', 'apologists', 'approval', 'arab', 'are', 'areas', 'aren', 'army', 'around', 'arrahman', 'arrested', 'article', 'as', 'assad', \"assad's\", 'assure', 'at', 'atanki', 'attack', 'attempt', 'audio', 'auskennen', \"australia's\", 'authorlink', 'awards', 'awardscongratulat', 'awardsseason', 'b', 'baby', 'ban', 'baru', 'bashing', 'basically', 'be', 'because', 'been', 'before', 'begins', 'behind', 'being', 'believing', 'benefits', 'berebut', 'best', 'beyond', 'big', 'bir', 'bit', 'blackfriday', 'blocking', 'blue', 'blumenthal', 'bodies', 'bombing', 'brain', 'braveheart', 'bravery', 'brilliant', 'british', 'broadcaster', 'brother', 'brothers', 'bu', 'bulldoze', 'buried', 'busy', 'but', 'by', 'called', 'cam', 'campaign', 'camps', 'can', 'candy', 'cannot', 'captive', 'cause', 'ch', 'change', 'chant', 'check', 'chemical', 'children', 'chile', 'civil', 'civilians', 'claimed', 'claims', 'climate', 'clues', 'co', 'cobalt', 'cockpit', 'column', 'come', 'coming', 'commanders', 'comparative', 'concerns', 'conducted', 'confirmed', 'conflict', 'control', 'corbyn', 'could', 'counteroffensive', 'countries', 'country', 'countryoh', 'cracks', 'crimes', 'cul', 'culpability', 'cuts', 'dad', 'daily', 'dalam', 'damaged', 'dapatkan', 'daraasyria', 'das', 'day', 'dayso', 'deadliest', 'decades', 'december', 'declare', 'dedicated', 'defeat', 'defend', 'deilmicarlson', 'delegate', 'deliberate', 'democratic', 'denials', 'deployment', 'describe', 'designated', 'di', 'did', \"didn't\", 'didnt', 'die', 'director', 'dis', 'disana', 'disaster', 'discussion', 'dislike', 'displaced', 'disseminating', 'do', 'documentary', 'done', 'door', 'douma', 'dunia', 'each', 'east', 'eastern', 'egypt', 'eight', 'eine', 'einen', 'einer', 'el', 'en', 'entire', 'equivalent', 'era', 'erdogan', 'et', 'etc', 'ethniccleansingoperation', 'european', 'evangeline', 'even', 'every', 'evil', 'exchange', 'execution', 'experiences', 'expert', 'explosionsyria', 'exposes', 'extension', 'extent', 'fact', 'falls', 'false', 'falsehood', 'families', 'famous', 'fantastic', 'fares', 'farmers', 'favorite', 'fed', 'feels', 'fictional', 'film', 'final', 'find', 'first', 'fled', 'flight', 'footsies', 'for', 'force', 'forces', 'forgotten', 'forsama', 'found', 'france', 'from', 'fucking', 'fueling', 'fuelled', 'full', 'gambar', 'gas', 'gdp', 'genocide', 'germany', 'get', 'gigantic', 'girls', 'given', 'god', 'good', 'granary', 'great', \"grinder'\", 'group', 'groups', 'growing', 'guard', 'guess', 'guilty', 'gunners', 'h', 'happening', 'hara', 'harder', 'has', 'hatred', 'have', \"haven't\", 'he', 'hea', 'heavy', 'held', 'helicopters', 'helmets', 'help', 'her', 'hi', 'his', 'hoa', 'home', 'homes', 'horrors', 'hospitals', 'hostility', 'hous', 'house', 'how', 'https', 'httpstco', 'httpstco1dyw9dgott', 'httpstco3jajfhgxiy', 'httpstco4to7qxqfy3', 'httpstco5u3hp1tyql', 'httpstco6wtj6dgjpy', 'httpstco7f37bhkb7h', 'httpstco88mp0ueang', 'httpstco9gltkhjqit', 'httpstcoakeramfy', 'httpstcobtqh4wn1uf', 'httpstcocgxpvr3pj1', 'httpstcocqygzohnf8', 'httpstcodqdxpmpslc', 'httpstcoegcduhc7uv', 'httpstcof8xv2iypea', 'httpstcogxtmut3vk7', 'httpstcoienwhgu04z', 'httpstcoiltrcvvjfx', 'httpstcoixhbyxtujv', 'httpstcojayhz9x7pm', 'httpstcojcpajilzat', 'httpstcojevyoh02cs', 'httpstcojgmtbutzgy', 'httpstcojrx0sarhie', 'httpstcojt9c1xlt8c', 'httpstcojxl1b2xhum', 'httpstcokfsz2rloeq', 'httpstcokr7vztz2yd', 'httpstcol9oo7fppvo', 'httpstcolf5gp7', 'httpstcomsb0kfospb', 'httpstcomthqadazhb', 'httpstcon8hlzvdzy8', 'httpstconkeo42if6b', 'httpstconkr9zlzbs5da', 'httpstconm2yzyfvx0', 'httpstconnqin1vwsn', 'httpstconzhlyvptpm', 'httpstcoo0dj8rdebh', 'httpstcoqii2watgax', 'httpstcotivwnuon9v', 'httpstcotj46cbnv4p', 'httpstcovgiwayybue', 'httpstcowt3ae1many', 'httpstcoy9esvdr3mc', 'httpstcoy9fpw52aceteltamr', 'httpstcoykfpc5fzzw', 'httpstcoyvpb6odyzs', 'httpstcoz83mmhult8', 'httpstcoz9qz8dahtb', 'httpstcozvbd98d7b7', 'i', 'idlib', 'idlibwatch', 'if', 'ignore', 'im', 'in', 'inc', 'including', 'independent', 'inevitability', 'inevitable', 'inside', 'instances', 'interventions', 'interview', 'interviews', 'into', 'invasion', 'investig', 'investigate', 'investigation', 'invited', 'involvement', 'iran', 'iranprotestshier', 'iraq', 'is', 'isil', 'isis', 'isisthe', 'isistrump', 'islamic', 'israel', 'ist', 'it', \"it's\", 'its', 'itself', 'james', 'japanese', 'jeremy', 'jihadi', 'jihadist', 'jordan', 'journalism', 'journalist', 'july', 'just', 'justify', 'kami', 'keep', 'keeps', 'keine', 'kicks', 'killed', 'killing', 'kimyasal', 'km', 'koreksiiceland', 'kurdish', 'kurdistan', 'kurds', 'kurdsled', 'lack', 'lady', 'lagi', 'largest', 'last', 'later', 'launched', 'launching', 'le', 'leaders', 'leak', 'lebanon', 'left', 'levant', 'liberal', 'life', 'lifeall', 'lihatlah', 'like', 'lips', 'lisa', 'little', 'lkeye', 'location', 'logs', 'lolhttpstcot7wi8owtle', 'los', 'lost', 'love', 'm4', 'machi', 'mad', 'made', 'maduro', 'magic', 'mainlykurdish', 'major', 'makanan', 'makes', 'mall', 'management', 'material', 'matterif', 'mauer', 'max', 'me', 'media', 'meet', 'meeting', 'meets', 'memories', 'mercenaries', 'mesurier', 'mi8', 'middle', 'migration', 'mil', 'mili', 'militants', 'military', 'militia', 'militias', 'million', 'millions', 'misleading', 'mit', 'mob', 'moderate', 'modern', 'monday', 'money', 'months', 'mooching', 'more', 'mosque', 'mounted', 'movements', 'muslims', 'must', 'my', 'n', 'nacotics', 'name', 'narrative', 'national', 'nations', 'nato', 'natosummit', 'neoliberal', 'net', 'never', 'new', 'no', 'noch', 'nordsyrien', 'normally', 'north', 'northeast', 'northern', 'northsyria', 'northwest', 'not', 'notice', 'now', 'nytimes', 'o', 'obama', 'obscure', 'of', 'off', 'office', 'ofpalestinesyriakashmirburmagazaalgeriairaqsomaliaegyptlibyaafghanistan', 'often', 'oh', 'oilemosi', 'oilsyria', 'okkupation', 'old', 'on', 'one', 'only', 'operating', 'other', 'our', 'out', 'over', 'pale', 'palestine', 'parro', 'pastime', 'patches', 'patriarchal', 'patrol', 'people', 'pers', 'peter', 'phenomenal', 'photo', 'photographer', 'photos', 'physical', 'pillage', 'pilot', 'plan', 'played', 'playing', 'please', 'poison', 'police', 'policies', 'pose', 'possible', 'pots', 'ppl', 'previously', 'private', 'prodemocracy', 'propaganda', 'protesters', 'protests', 'proved', 'provided', 'province', 'psychological', 'purpose', 'put', 'qaryat', 'qatar', 'quagmire', 'r', 'radio', 'raed', 'railtrack', 'rally', 'rape', 'raped', 'rather', 're', 'read', 'reading', 'rebranded', 'refugees', 'regime', 'regimerevolution', 'relatio', 'relying', 'reminds', 'renee', 'report', 'reporting', 'represents', 'rescue', 'responsibility', 'result', 'retakes', 'returning', 'revolution', 'right', 'rights', 'rise', 'riseup4rojava', 'risk', 'rojava', 'role', 'roles', 'root', 'round', 'route', 'rt', 'russia', 'russian', 's', 'safe', 'saldrda', 'saldryor', 'sale', 'sama', 'same', 'sanctions', 'sanddo', 'santiago', 'sat', 'say', 'saya', 'scholar', 'scratching', 'sdf', 'secondswatching', 'security', 'see', 'seek', 'seem', 'sekali', 'selfhating', 'send', 'sends', 'sent', 'separation', 'seriously', 'session', 'sharing', 'she', 'shiite', 'shopping', 'should', 'shows', 'sich', 'sicherheitszone', 'simplification', 'single', 'skeptical', 'sloshed', 'smartest', 'smith', 'snake', 'so', 'solidarity', 'soll', 'some', 'sondern', 'sorumlu', 'southeast', 'southern', 'spain', 'spells', 'spending', 'spent', 'spetsnaz', 'spn', 'spotter', 'start', 'started', 'starting', 'state', 'stated', 'step', 'still', 'stole', 'stop', 'stopped', 'strategic', 'street', 'strike', 'strikes', 'stunning', 'stupid', 'such', 'summit', 'supplier', 'support', 'supported', 'suriye', \"suriye'de\", 'surprising', 'surprisingly', 'suspected', 'sweet', 'sy', 'syr', 'syria', 'syriairaq', 'syrian', 'ta', 'taahir', 'table', 'taken', 'talks', 'tapes', 'tasmania', 'tax', 'teltamr', 'tensions', 'territory', 'terror', 'terrorism', 'terrorist', 'tested', 'than', 'thanks', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'these', 'they', 'theyd', 'thing', 'thinking', 'this', 'thoroughly', 'those', 'threat', 'three', 'through', 'tigrm', 'time', 'to', 'today', 'together', 'took', 'top', 'tough', 'tourists', 'towns', 'tracker', 'training', 'traumatized', 'treated', 'trennt', 'triple', 'trivial', 'trkei', 'troops', 'trum', 'trump', 'trusted', 'truth', 'tuesday', 'turkey', \"turkey's\", 'turkeys', 'turkishbacked', 'turkishcontrolled', 'turkishwarcrimes', 'tutulmutuoysa', 'tweet', 'twitter', 'two', 'uae', 'understand', 'unif', 'unit', 'unpublished', 'unstable', 'untuk', 'up', 'us', 'usa', 'use', 'used', 'usuk', 'vagner', 'values', 'venezuela', 'very', 'veterans', 'vets', 'vetsforimpeachment', 'vetsresistsquadron', 'victory', 'video', 'videos', 'virtually', 'visited', 'want', 'wants', 'war', 'wars', 'was', 'watch', 'watched', 'watching', \"watchturkey's\", 'watchwhy', 'waves', 'ways', 'we', 'weakness', 'weapon', 'wearing', 'week', 'well', 'were', 'western', 'what', 'wheeler', 'when', 'where', 'whi', 'which', 'while', 'white', 'who', 'why', 'will', 'win', 'wit', 'with', 'withdrew', 'without', 'witness', 'women', 'womendefendrojava', 'world', 'worth', 'x', 'y', 'ya', 'yang', 'yassin', 'yazidi', 'year', \"year's\", 'years', 'yemen', 'yest', 'yle', 'you', 'your', 'youre', 'yr', 'yrs', 'zellweger', 'zombie', 'zone'])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "\n",
    "\"\"\"\n",
    "idf = obtained by dividing the total number of documents by the number of documents containing \n",
    "the term, and then taking the logarithm of that quotient\n",
    "\"\"\"\n",
    "\n",
    "total_number_documents = len(np.unique(df_frequency.tweet_No))\n",
    "\n",
    "idf = dict()\n",
    "for word in np.unique(df_frequency.word):\n",
    "    \n",
    "    document_count = 0\n",
    "    for document in np.unique(df_frequency.tweet_No):\n",
    "        \n",
    "        words_list = list(df_frequency.word[df_frequency.tweet_No==document])\n",
    "        \n",
    "        if word in words_list:\n",
    "            document_count+=1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    idf[word] = document_count    \n",
    "    \n",
    "#print dictionary keys\n",
    "idf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_No</th>\n",
       "      <th>word</th>\n",
       "      <th>word count</th>\n",
       "      <th>word total</th>\n",
       "      <th>term frequency</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>@kurdistannews24</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>rojava</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>riseup4rojava</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>we</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet_1</td>\n",
       "      <td>womendefendrojava</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_No               word  word count  word total  term frequency  \\\n",
       "0  tweet_1   @kurdistannews24           1          15        0.066667   \n",
       "1  tweet_1             rojava           3          15        0.200000   \n",
       "2  tweet_1      riseup4rojava           1          15        0.066667   \n",
       "3  tweet_1                 we           1          15        0.066667   \n",
       "4  tweet_1  womendefendrojava           1          15        0.066667   \n",
       "\n",
       "     tf_idf  \n",
       "0  0.133333  \n",
       "1  0.400000  \n",
       "2  0.066667  \n",
       "3  0.200000  \n",
       "4  0.066667  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Divide the term frequency for each of the term by corresponding inverse document frequency.\n",
    "\n",
    "tf_idf = []\n",
    "for word,freq in zip(df_frequency.word,df_frequency['term frequency']):\n",
    "    tf_idf.append(freq*idf[word])\n",
    "\n",
    "#add tf_idf to frequency dataframe\n",
    "df_frequency['tf_idf'] = tf_idf\n",
    "df_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_No</th>\n",
       "      <th>word</th>\n",
       "      <th>word count</th>\n",
       "      <th>word total</th>\n",
       "      <th>term frequency</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tweet_2</td>\n",
       "      <td>rt</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>106.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tweet_17</td>\n",
       "      <td>rt</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>106.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>tweet_55</td>\n",
       "      <td>rt</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>tweet_113</td>\n",
       "      <td>rt</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>tweet_47</td>\n",
       "      <td>rt</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>tweet_96</td>\n",
       "      <td>any</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>tweet_96</td>\n",
       "      <td>footsies</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>tweet_96</td>\n",
       "      <td>eastern</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>tweet_141</td>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>tweet_96</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2903 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_No      word  word count  word total  term frequency      tf_idf\n",
       "17      tweet_2        rt           2           3        0.666667  106.666667\n",
       "256    tweet_17        rt           2           3        0.666667  106.666667\n",
       "834    tweet_55        rt           2           5        0.400000   64.000000\n",
       "1638  tweet_113        rt           2           5        0.400000   64.000000\n",
       "735    tweet_47        rt           2           5        0.400000   64.000000\n",
       "...         ...       ...         ...         ...             ...         ...\n",
       "1391   tweet_96       any           1          26        0.038462    0.038462\n",
       "1390   tweet_96  footsies           1          26        0.038462    0.038462\n",
       "1385   tweet_96   eastern           1          26        0.038462    0.038462\n",
       "2069  tweet_141      want           1          26        0.038462    0.038462\n",
       "1393   tweet_96        17           1          26        0.038462    0.038462\n",
       "\n",
       "[2903 rows x 6 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "\n",
    "df_frequency = df_frequency.sort_values(by='tf_idf', ascending=False)\n",
    "df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>106.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>syria</td>\n",
       "      <td>32.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>24.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>20.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ya</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of</td>\n",
       "      <td>8.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>syrian</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>help</td>\n",
       "      <td>5.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word      tf_idf\n",
       "0      rt  106.666667\n",
       "1   syria   32.666667\n",
       "2       a   32.000000\n",
       "3     the   24.090909\n",
       "4      in   20.571429\n",
       "5      ya   11.000000\n",
       "6      is    9.900000\n",
       "7      of    8.250000\n",
       "8  syrian    7.333333\n",
       "9    help    5.625000"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6 Print the top 10 terms.\n",
    "\n",
    "#find max value for tf_idf in each word across all tweets (documents)\n",
    "df_max_tf_idf = df_frequency.groupby(['word'],as_index=False)['tf_idf'].max()\n",
    "\n",
    "#sort and display top 10\n",
    "df_max_tf_idf = df_max_tf_idf.sort_values(by='tf_idf',ascending=False) \n",
    "df_max_tf_idf = df_max_tf_idf.reset_index(drop=True)\n",
    "df_max_tf_idf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-abfa8c55ee81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DJANGO_SETTINGS_MODULE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet_labelling.settings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtweetstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweets'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example provided but not used for the code\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"tweet_labelling.settings\")\n",
    "from tweets.models import Tweet\n",
    "from datetime import datetime\n",
    "import tweetstream\n",
    "\n",
    "stream = tweetstream.SampleStream(\"twitter-id\", \"twitter-password\");\n",
    "counter=0;\n",
    "for tweet in stream:\n",
    "    counter=counter+1;\n",
    "   \n",
    "    try:\n",
    "        print(counter,\" TEXT: \",tweet[\"text\"])\n",
    "    except KeyError:\n",
    "        print(\"No text field\")\n",
    "\n",
    "    try:\n",
    "        g=tweet[\"geo\"];\n",
    "        try:\n",
    "            if(not g is None):\n",
    "                print(counter,\" LOCATION: \",g)\n",
    "        except AttributeError:\n",
    "            g=None; #print \"AttributeError, g=|\",g,\"|\";\n",
    "    except KeyError:\n",
    "        g=None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
